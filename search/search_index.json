{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Course information Course material moved here"},{"location":"#why-awk","title":"Why awk?","text":"<p>Awk is useful when the overhead of more sophisticated approaches is not worth the bother.</p> <p>Quote</p> <p>The Enlightened Ones say that...</p> <ul> <li>You should never use C if you can do it with a script;</li> <li>You should never use a script if you can do it with awk;</li> <li>Never use awk if you can do it with sed;</li> <li>Never use sed if you can do it with grep.</li> </ul> <p>*source: (some years ago) http://awk.info/?whygawk</p>"},{"location":"#objectives","title":"Objectives","text":"<ul> <li>The material on this site is not a complete guide or awk manual.</li> <li>The purpose of this site is to give an overview of the capabilities of the awk language and to underline some particular strengths or disadvantages .</li> <li>The page aims to promote this tool for use in every-days research work and urges you to find solutions yourself rather than expecting ready-made ones.</li> <li>The material assumes that you are somewhat familiar with grep, sed and have some basic programming experience.</li> </ul>"},{"location":"#what-is-awk","title":"What is awk?","text":"<p>AWK is an interpreted programming language designed for text processing and typically used as a data extraction and reporting tool. </p> <p>The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data - either run directly on files or used as part of a pipeline - for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions.</p> <p>AWK has a limited intended application domain, and was especially designed to support one-liner programs.</p> <p>It is a standard feature of most Unix-like operating systems.</p> <p>source: Wikipedia</p>"},{"location":"1.Simple_example/","title":"1.Simple examples","text":""},{"location":"1.Simple_example/#lets-begin-simple","title":"Let's begin simple","text":"<p>It is quite easy to use Awk from the command line to perform simple operations on text files. Suppose we have a file named \"coins.txt\" that describes a coin collection. Each line in the file contains the following information:</p> <p>metal weight in ounces date minted country of origin description</p> <p>The file has the contents: coins.txt<pre><code>gold     1    1986  USA                 American Eagle\ngold     1    1908  Austria-Hungary     Franz Josef 100 Korona\nsilver  10    1981  USA                 ingot\ngold     1    1984  Switzerland         ingot\ngold     1    1979  RSA                 Krugerrand\ngold     0.5  1981  RSA                 Krugerrand\ngold     0.1  1986  PRC                 Panda - silver lined\nsilver   1    1986  USA                 Liberty dollar\ngold     0.25 1986  USA                 Liberty 5-dollar piece\nsilver   0.5  1986  USA                 Liberty 50-cent piece\nsilver   1    1987  USA                 Constitution dollar\ngold     0.25 1987  USA                 Constitution 5-dollar piece\ngold     1    1988  Canada              Maple Leaf\n</code></pre></p> <p>The command bellow will search through the file for lines of text that contain the string \"gold\", and print them out.</p> <pre><code>$ awk '/gold/' coins.txt\ngold     1    1986  USA                 American Eagle\ngold     1    1908  Austria-Hungary     Franz Josef 100 Korona\ngold     1    1984  Switzerland         ingot\ngold     1    1979  RSA                 Krugerrand\ngold     0.5  1981  RSA                 Krugerrand\ngold     0.1  1986  PRC                 Panda\ngold     0.25 1986  USA                 Liberty 5-dollar piece\ngold     0.25 1987  USA                 Constitution 5-dollar piece\ngold     1    1988  Canada              Maple Leaf\n</code></pre> <p><code>/gold/</code> defines a matching criteria that will be used on every line of the file. If no command is specified, the default action is to print the matched lines.</p> Exercise <ul> <li>Try to run the above example for \"silver\". What is different? How can one fix it?</li> </ul> <p>This mimics the use of grep or sed, so why would one possibly need awk?</p> How does this mimic grep or sed? <p>In <code>grep</code>, the command that does exactly the same is:</p> <pre><code>grep silver coins.txt\n</code></pre> <p>In <code>sed</code>, the command that does exactly the same is:</p> <pre><code>sed -n \"/silver/p\" coins.txt\n</code></pre> <p><code>awk</code> starts to shine when the thing you want to do is more complex then detecting lines with a text.</p> <p>Now, suppose we want to list all the coins that were minted before 1980. See Copilot's solution.</p> <p>We invoke Awk as follows:</p> <p><pre><code>$ awk '$3 &lt; 1980 {print $3, \"    \",$5,$6,$7,$8}' coins.txt\n1908      Franz Josef 100 Korona\n1979      Krugerrand\n</code></pre> In the example above, we defined a matching criteria <code>$3 &lt; 1980</code>, so the  commands enclosed in the <code>{print $3, \" \",$5,$6,$7,$8}</code> block will be executed only when the criteria is met - i.e. awk will print the values of columns 3,\"    \",5,6,7, and 8. The columns are separated (defined) by white space (one or more consecutive blanks) or tabulator and addressed by the <code>$</code> sign i.e. <code>$1</code> is the value of the first column, <code>$2</code> - second etc. <code>$0</code> contains the original (unparsed) line including the separators.</p> Discussion and exercises <ul> <li>Can you find all \"silver\" coins older than 1986? One can use grep to filter the silver coins and pipe the result to awk or do it all together in awk.</li> <li>Unfortunately, awk does not have a way to print/address all fields after or before a selected one. How can one print all remaining fields?</li> <li>A <code>TAB</code> separated version 'coins.tab' is more appropriate in such cases and rather common, for the same reason, in many bioinformatics file formats <code>gff|bed|sam|vcf</code>.</li> </ul>"},{"location":"1.Simple_example/#what-about-some-math-can-i-manipulate-or-analyze-the-data","title":"What about some math? Can I manipulate or analyze the data?","text":"<p>Let's use the following simple file that contains 3 lines with numbers and some text, just to make our life more difficult (or maybe not?)</p> <p>123.txt<pre><code>1 2 3\n5 4 6\n7 8 9 10 text\n</code></pre> Let's try to make some calculations with the data in this file - summation and multiplication in this case.</p> <p><pre><code>$ awk '       {print $1+$2*$3}' 123.txt\n7\n29\n79\n</code></pre> Again, we did not provide any matching criteria, so the commands will be executed on every line. Try to run the command. What do you think? Does awk provide the right answer? I am serious! ;-) Let's do the math only if the value in the first column is greater than 4.</p> <p><pre><code>$ awk '$1 &gt; 4 {print $1+$2*$3}' 123.txt\n29\n79\n</code></pre> Now, <code>$1 &gt; 4</code> is our criteria on when to execute the command block <code>{print $1+$2*$3}</code>. Note that the matching criteria is outside the <code>{}</code> block.</p> Exercises <ul> <li>print the third column for each line where the value in the second column is smaller that the value in the first column.</li> <li>print the original content on each line followed by the result of <code>$1+$2*$3</code>.</li> <li>print the the result of <code>$1+$2*$3</code> as 4<sup>th</sup> column, discarding the unnecessary data from the original file.</li> </ul>"},{"location":"1.Simple_example/#awk-command-line-syntax","title":"Awk command-line syntax:","text":"<pre><code>$ awk ' /pattern/ {action} ' file1 file2 ... fileN\n</code></pre> <ul> <li>action is performed on every line that matches pattern.</li> <li>If pattern is not provided, action is performed on every line.</li> <li>If action is not provided, then all matching lines are simply sent to standard output.</li> <li>Since patterns and actions are optional, actions must be enclosed in braces to distinguish them from pattern.</li> <li>The statements in an awk program may be indented and formatted using spaces, tabs, and new lines.</li> <li>Two special patterns: <code>BEGIN</code> (execute an action before first input line) and <code>END</code> ( ... after all lines are read.)</li> </ul>"},{"location":"1.Simple_example/#simple-output-examples","title":"Simple output examples","text":"<p><code>{ print }</code> - will print the whole line to standard out</p> <p><code>{ print $0 }</code> - will do the same thing</p> <p><code>{ print $1, $3 }</code> - expressions separated by a comma are, by default, separated by a single space when output</p> <p><code>{ print NF, $1, $NF }</code> - will print the number of fields, the first field, and the last field in the current record</p> <p><code>{ print $(NF-2) }</code> prints the third to last field</p> Exercises <ul> <li>run the last two examples on <code>123.txt</code> file. What is the difference between <code>NF</code> and <code>$NF</code>?</li> <li>is there a difference between <code>$(NF-2)</code> and <code>$NF-2</code>?</li> </ul>"},{"location":"1.Simple_example/#computing-and-printing","title":"Computing and Printing","text":"<p><code>{ print $1, $2 * $3 }</code> You can also do computations on the field values and include the results in your output</p> <p><code>{ print NR, $0 }</code> - will print each line prefixed with its line number</p> Exercises <ul> <li>what happens if you provide the <code>123.txt</code> file twice for the second example?</li> <li>compare the output of <code>NR</code> and <code>FNR</code> when you run the above test.</li> </ul>"},{"location":"1.Simple_example/#putting-text-in-the-output","title":"Putting Text in the Output","text":"<p><code>{ print \"total pay for\", $1, \"is\", $2 * $3 }</code> you can also add other text to the output besides what is in the current record. Note that the inserted text needs to be surrounded by double quotes.</p> <p><code>{ printf(\"total pay for %s is $%.2f\\n\", $1, $2 * $3) }</code> when using <code>printf</code>, formatting is under your control, so no automatic spaces or newlines are provided by awk. You have to insert them yourself.</p> <p><code>{ printf(\"%-8s %6.2f\\n\", $1, $2 * $3 ) }</code> - well, this escalated too fast...</p> Exercises pay.txt<pre><code>David   3       6\nAna     5       7\nOlla    4       4\n</code></pre> <ul> <li>run the examples above with the content of the <code>pay.txt</code> file. </li> <li>remove the minus sign in the <code>%-8s</code> formatting to see the effect.</li> <li>more string manipulations exercises</li> </ul> <p>More on format modifiers: gawk documentation</p> <p>Files</p> <ul> <li>coins.txt</li> </ul>"},{"location":"2.Teasing_with_grep/","title":"2.Teasing with grep","text":"<p>The most common use is using regular expressions. The construction below will search for \"pattern\" on each line.</p> <pre><code>$ awk ' /pattern/ {action} ' file1 file2 ... fileN\n</code></pre> <p>What about patter matching only particular field? Here is an example of such matching criterium that targets only the first field/column. </p> <pre><code>$ awk ' $1 ~ /pattern/ {action} ' file1 file2 ... fileN\n</code></pre> <p>The <code>!</code> will negate the result.</p> <pre><code>$ awk ' ! /pattern/ {action} ' file1 file2 ... fileN\n</code></pre> <p>One can match exact text of a field. Matching is case sensitive.</p> <pre><code>$ awk ' $1 == \"text\" {action} ' file1 file2 ... fileN\n</code></pre> <p>Here is an example of of arithmetic comparison for different fields. In this example the script will match lines with value in the first field larger than 10 AND the second smaller that 7. <code>&amp;&amp;</code> is the logical AND, <code>||</code> is for OR.</p> <pre><code>$ awk ' $1 &gt; 10  &amp;&amp;  $2 &lt; 7 {action} ' file1 file2 ... fileN\n</code></pre> <p>You can compare between different fields as well...</p> <pre><code>$ awk ' ($1+$2 &gt; 10)  &amp;&amp;  ($3 &lt; $4) {action} ' file1 file2 ... fileN\n</code></pre> <p>There are two reserved keywords: <code>BEGIN</code> and <code>END</code>. Here is how it works.</p> <pre><code>$ awk ' BEGIN {action_B}  /pattern/ {action}   END {action_E}' file\n</code></pre> <p><code>BEGIN</code> block gives you the opportunity to do something before you even start reading the file {action_B}. This is perfect for variable initialization or gathering data from another files that is needed for the current script.</p> <p>Then, awk will run {action} for each line that matches /pattern/.</p> <p>At the <code>END</code> awk will run {action_E}. Perfect to print the collected data. </p> <p>Here is a summary of the awk workflow logic.</p> <pre><code>  $ awk ' BEGIN {action_B}     \n         /pattern1/ {action1} \n         /pattern2/ {action2} \n         END {action_E}       \n   ' file1 file2 ... fileN\n</code></pre> Exercises <ul> <li>Can you add a header <code># metal | weight in ounces | date minted | country of origin | description</code> for the output of the coins older than 1986? Use this shorter <code># header</code> in the beginning, until you get it working.</li> <li>What will happen if you do not provide file as input to the above exercise?</li> </ul> <p>And here is the teaser ;-).  Perhaps I am not very familiar with grep but here is a simple example that is rather tricky to do with grep.</p> <pre><code>1 2 4 3 5 6\n5 3 5 6 8 2\n4 3 1 5 7 8\n2 5 6 1 9 0\n1 4 5 6 7 8\n</code></pre> <p>Remove all lines that contain \"2\" and \"3\" in any order. These lines are 1 and 2.</p> <p>Possible way to do it:</p> <pre><code>$ grep -v \"2.*3\\|3.*2\" numbers.dat\n</code></pre> <p>which will grep for the two possible combinations when \"2\" is before \"3\" and the opposite case. This works but already presents us with the problem. What happens if we search for more than two matches? One neds to permutate all the possible configurations...</p> <p>Here is how can be done with awk.</p> <pre><code>$ awk '!(/2/ &amp;&amp; /3/) {print $0}' numbers.dat \n</code></pre> <p><code>!(/2/ &amp;&amp; /3/)</code> let's decrypt the matching rule. <code>&amp;&amp;</code> stands for \"and\"  <code>!</code> negates the logical result</p> <p><code>(/2/ &amp;&amp; /3/)</code> matches any line that contains 2 and 3, then <code>!</code> negates the result- line that does not contain them - if so, print them.</p> <p>Awk has a bit more elaborate matching capabilities, not mentioning the ability of arithmetical operations in the matching criteria expressions.</p>"},{"location":"3.Shell_we_awk/","title":"3.Shell we awk!","text":""},{"location":"3.Shell_we_awk/#q-ok-i-see-this-and-that-but-really-why-should-i-bother-with-awk","title":"Q: OK, I see this and that but really, why should I bother with awk?","text":"<p>Indeed, why? Lets get back to our very first example with the coins.</p> coins.txt<pre><code>gold     1    1986  USA                 American Eagle\ngold     1    1908  Austria-Hungary     Franz Josef 100 Korona\nsilver  10    1981  USA                 ingot\ngold     1    1984  Switzerland         ingot\ngold     1    1979  RSA                 Krugerrand\ngold     0.5  1981  RSA                 Krugerrand\ngold     0.1  1986  PRC                 Panda - silver lined\nsilver   1    1986  USA                 Liberty dollar\ngold     0.25 1986  USA                 Liberty 5-dollar piece\nsilver   0.5  1986  USA                 Liberty 50-cent piece\nsilver   1    1987  USA                 Constitution dollar\ngold     0.25 1987  USA                 Constitution 5-dollar piece\ngold     1    1988  Canada              Maple Leaf\n</code></pre> <p>Let's, just for a moment, assume that the file contains thousands of lines and your task is to summarize how many coins from each country you have in the file.  You can do it in some spreadsheet software, you can write a program in C, C++, Fortran, Python, Perl - I know. You can use a helicopter to travel pretty much everywhere, but are you going to use it for a visit to the nearby grocery store? </p> <p>So, perhaps grep + wc is a good start. </p> <pre><code>$ grep USA coins.txt | wc -l\n</code></pre> <p>This will give you the number of coins (lines) that have USA in the description. Then you go over all possible countries that are in the file but how many? Do you create a list of all countries and then run a loop over it? Why not give it to awk and use some associative arrays for this purpose?</p> count.awk<pre><code>#!/usr/bin/awk -f\n\n{ data[$4]= data[$4] + 1 }\n\nEND {\n  for (i in data) print \"Country: \" i \" count: \" data[i]\n}\n</code></pre> <p>This is written in a script file, but if you insist here is the one line equivalent.</p> <pre><code>$ awk '{data[$4]++} END {for (i in data) print \"Country: \" i \" count: \" data[i]}' coins.txt\n</code></pre> <p>Note that we neither defined the names of the countries nor their number in advance... The only restriction is that the name of the country should be a single word.</p> <p>So, on every line <code>{data[$4]= data[$4]+1}</code> will address the element with the value of the 4<sup>th</sup> column in the array <code>data</code> and will be increased by one. No need to declare the array in advance, no need to pre-allocate the size or to dynamically allocate the array... Everything is done behind the scene for your convenience... and the price to pay is... speed. In this particular case (with the thousands line file) the speed is not a problem.</p> Exercises <ul> <li>Can you change the program so that it will count how many gold and silver coins there are? ...how many coins for each year...</li> <li>This was a small file. Let's try something large. Look at the content of the <code>dgrp2.vcf</code> file you have downloaded (look in the provided instructions on how to download it). Here is a description of the VCF format. Find out how many different chromosomes contains the file. The relevant information is in the first column. What other information you could easily extract? - data entries per chromosome, fractions...</li> </ul>"},{"location":"3.Shell_we_awk/#well-coins-countries-anything-else","title":"Well, coins, countries - anything else?","text":"<p>Hm, if you work regularly with unformatted text files, like output from some programs, where the numbers are mixed with a complimentary text - then keep reading...        Let's use the output from the Gaussian code as an example (something from my research).</p> <p>gaussian.out<pre><code>  . . . ( more than 1 000 lines ) . . .\n  ---------------------------------------------------------------------\n                      Distance matrix (angstroms):\n                    1          2          3          4          5\n     1  O    0.000000\n     2  H    0.963063   0.000000\n     3  H    3.055408   2.200353   0.000000\n     4  H    3.599651   2.649467   1.360343   0.000000\n     5  O    2.921061   1.978989   0.745120   0.995689   0.000000\n     6  H    1.853252   1.916060   3.229967   3.940127   3.023829\n     7  H    3.373977   3.518394   4.584225   5.275136   4.356355\n     8  O    2.872140   2.799938   3.649823   4.343776   3.416579\n     9  H    3.899854   3.574193   2.560758   3.901261   3.205196\n  . . . ( more than 1 000 lines ) . . .\n Hexadecapole moment (field-independent basis, Debye-Ang**3):\n XXXX=          -2970.5127 YYYY=          -1148.8478 ZZZZ=          -1274.6533 XXXY=           -394.6318\n XXXZ=            676.4844 YYYX=           -298.0473 YYYZ=             48.9948 ZZZX=            302.4536\n ZZZY=            111.2988 XXYY=           -793.1298 XXZZ=           -808.1397 YYZZ=           -454.2615\n XXYZ=            -73.3495 YYXZ=             82.5166 ZZXY=           -127.7147\n N-N= 4.131583513295D+02 E-N=-2.289173820990D+03  KE= 6.050114478748D+02\n Counterpoise: corrected energy =    -693.356401084648\n Counterpoise: BSSE energy =       0.003357515766\n\n Test job not archived.\n 1\\1\\NSC-NEOLITH\\SP\\RB3LYP\\6-31++G(d,p)\\H17O9(1-)\\X_PAVMI\\23-Mar-2012\\0\n \\\\#B3LYP/6-31++G(d,p) Counterpoise=2 Charge SCF=Tight NoSymm\\\\OH- in H\n 2O,std. LJ params. for H2O, MC/QM\\\\-1,1\\O,0,0.,0.,0.\\H,0,-0.836605,-0.\n  . . . ( more lines ) . . .\n</code></pre> The numbers that I am interested in are in bold. There are 56 such pairs in the whole file. I need them tabulated in simple, two-column file that is easy to read, analyze and plot. Here I will not discuss other solutions. Instead, here is a possible awk solution:</p> extract-gaussian.awk<pre><code>#!/usr/bin/awk -f\nBEGIN { AU2eV= 27.211383 }\n/Distance matrix/ {getline; getline; getline; getline; getline; getline; rOH= $5}\n/Counterpoise: corrected energy/ { printf \"%.12f %.6f\\n\", rOH, $5*AU2eV }\n</code></pre> <p>Here is the result: <pre><code>./extract-gaussian.awk gaussian.out\n\n0.745120000000 -18867.186585\n0.760120000000 -18867.562399\n0.775121000000 -18867.891626\n0.790119000000 -18868.178646\n0.805120000000 -18868.427531\n0.820119000000 -18868.641859\n0.835121000000 -18868.824992\n0.850121000000 -18868.979881\n0.865120000000 -18869.109227\n0.880121000000 -18869.215504\n0.895120000000 -18869.300920\n0.910121000000 -18869.367515\n0.925120000000 -18869.417119\n0.940121000000 -18869.451414\n0.955119000000 -18869.471908\n0.970120000000 -18869.479979\n0.985120000000 -18869.476868\n1.000120000000 -18869.463711\n1.015120000000 -18869.441546\n1.030121000000 -18869.411316\n. . .\n</code></pre></p> <p><code>BEGIN{ AU2eV= 27.211383 }</code> simply defines a conversion factor from atomic units to eV.</p> <p><code>/Distance matrix/{getline; getline; getline; getline; getline; getline; rOH= $5}</code> will match lines with the specified criteria, skip 6 lines, then accumulate the 5<sup>th</sup> column in variable rOH - the distance between an O and an H in the molecule of interest.</p> <p><code>/Counterpoise: corrected energy/ { printf \"%.12f %.6f\\n\", rOH, $5*AU2eV }</code> will match the other interesting line, then prints the collected earlier rOH and corresponding energy converted in eV.</p> <p>Reading the resulting file is trivial in any programming language or plotting program.</p> <p>Look at this alternative solution that uses a smart trick to advance 6 lines after the <code>/Distance matrix/</code> line </p> credits to Pavol Bauer, workshop 2017.01.13 <pre><code>#!/usr/bin/awk -f\nBEGIN { AU2eV= 27.211383 }\n/Distance matrix/ {myNR=NR}\nNR==myNR+6        {rOH=$5}\n/Counterpoise: corrected energy/ { printf \"%.12f %.6f\\n\", rOH, $5*AU2eV }\n</code></pre> <p>Other case studies that will be gradually collected for further inspiration.</p> Exercises <ul> <li>Can you modify the script to print a third column with values from the line <code>Counterpoise: BSSE energy =       0.003357515766</code> also converted in eV?</li> </ul> gnuplot tips <p>Here is an example how you can quickly visualize the results with <code>gnuplot</code>, directly from the output of the script. $ gnuplot<pre><code>plot \"&lt; ./extract-gaussian.awk gaussian.out\" with lines\n</code></pre> </p> <p>Another example that visualizes the country distribution from the coins.txt data. Try to sort the output as earlier and plot on the fly. $ gnuplot<pre><code>set boxwidth 0.9; plot [:][0:] \"&lt; ./count.awk coins.txt\" using 0:4:xticlabels(2) with boxes\n</code></pre> </p> <p>Or visualizing the chromosome counting/distribution exercise... <code>histogram-discrete.awk</code> script is discussed here. $ gnuplot<pre><code>set boxwidth 0.9; plot [:][0:] \"&lt;grep -v ^# dgrp2.vcf | histogram-discrete.awk\" using 0:2:xticlabels(1) with boxes\n</code></pre> </p>"},{"location":"4.Brief_commands/","title":"4.Brief commands overview","text":"<p>The language looks a little like C but automatically handles input, field splitting, initialization, and memory management.</p> <ul> <li>Built-in string and number data types</li> <li>No variable type declarations</li> </ul> <pre><code>data[1]= 330                   \ndata[3]= \"text\"     # yes, the elements could be of different type\ndata[7]= 7.5                   \ndata[\"Monday\"]= 1               \n</code></pre> <p>awk is a great prototyping language</p> <ul> <li>start with a few lines and keep adding until it does what you want </li> </ul> <p>awk gets its input from</p> <ul> <li>files                        </li> <li>redirection and pipes        </li> <li>directly from standard input</li> </ul> Exercise <ul> <li>Let's start with something simple Exercises/01.Simple arithmetic.</li> </ul> <p>Common commands and constructions - examples</p> <p>Loop</p> <pre><code>for (i=1; i&lt;=10; i++) {\n  print i\n}\n</code></pre> Exercise <ul> <li>Let's try to use loops for some simple tasks Exercises/05.Easy_tricks</li> </ul> <p>If else statement</p> <pre><code>if ($1 &gt; 2) {\n  print $2 \n} else if ($1 &lt; 2) {\n  print $3 \n} else {\n  print $3 \n}\n</code></pre> Exercises <ul> <li>time to add a bit more, so we can get some meaningful calculations on our data Exercises/Warming up.</li> </ul> <p>While statement</p> <pre><code>while ($1 &lt; $3) {\n  getline; print\n}\n</code></pre> <p>Other</p> <pre><code>print \"Text\"$1\"more text\"\n\nprintf (\"Text %g more text\", $1)\n\nnext    #  (skips the remaining patterns on the current line of input)\n\nexit    #  (skips the rest of the current line)\n</code></pre> Exercises <ul> <li>Let's see how can we format better the output of an awk script Exercises/String manipulation</li> <li>At this point, we should be abe to decipher Multiline Fasta To Single Line Fasta.</li> </ul>"},{"location":"4.Brief_commands/#predefined-variables","title":"Predefined variables","text":"<ul> <li>NR - Number of records processed</li> <li>FNR - Number of record processed in the current file while <code>NR</code> refers to the total record number. For the first file <code>FNR==NR</code>, but for the second <code>FNR</code> will restart from 1 while <code>NR</code> will continue to increment.</li> <li>NF - Number of fields in current record</li> <li>FILENAME - name of current input file</li> <li>FS - Field separator, space or TAB by default</li> <li>OFS - Output field separator, space by default</li> <li>ARGC/ARGV - Argument Count, Argument Value array - get arguments from the command line</li> </ul> <p>$1 - first field value, $2 - second etc. </p> <p>$0 - contains the entire line</p>"},{"location":"4.Brief_commands/#built-in-functions","title":"Built-in functions","text":""},{"location":"4.Brief_commands/#arithmetic","title":"Arithmetic","text":"<p>sin(), cos(), atan(), exp(), int(), log(), rand(), sqrt()</p>"},{"location":"4.Brief_commands/#string-manipulation-functions","title":"String manipulation functions","text":"<p>length(), substitution, find substrings, split strings</p>"},{"location":"4.Brief_commands/#output","title":"Output","text":"<p>print, printf(), print and printf to file</p>"},{"location":"4.Brief_commands/#special","title":"Special","text":"<p>system() - executes a Unix command e.g., system(\"date\") to execute \"date\" command. Note double quotes around the Unix command</p> <p>If you have reached this point, perhaps you are already asking where you can read something to begin with. This site is perhaps good for beginners: https://en.wikibooks.org/wiki/AWK</p>"},{"location":"5.String_manipulation/","title":"5.String manipulation","text":"<p>This is not exactly Awk specific, but it is helpful to know how one can manipulate string fields.</p> <p>Quite often, you have files with such names, that are alphabetically ordered and it is a bit tricky to put them in order again. Lets assume we have the following data in a file. The lines contain filenames that have common pattern and number that we are interested. Let's \"extract\" the alternating number i.e 1, 10, 123, ... </p> names.dat<pre><code>H2O-1_1.res\nH2O-1_10.res\nH2O-1_123.res\nH2O-1_21.res\nH2O-1_44.res\nH2O-1_5.res\nH2O-1_7.res\n</code></pre> <p>One possible way, is by removing the pattern that remains unchanged. (BASH can do it as well, way easier with the file names than the lines in a file) <pre><code>$ awk '{ gsub( \"H2O-1_|.res\", \"\" , $1); print $1 }' names.dat\n</code></pre></p> <p>This is my preferred way, since it is rather easy to understand and modify.</p> <p>Another way, could be by using backreferences the way sed will do it. <pre><code>$ awk '{ print gensub(/H2O-1_(.*).res/,\"\\\\1\",\"g\")}' names.dat\n</code></pre></p> <p>How about we fix the annoyance with the alphabetic sort by replacing 1 with 001, 5 with 005, 10 -&gt; 010, etc. <pre><code>$ awk '{ gsub(\"H2O-1_|.res\",\"\",$1); printf(\"H2O-1_%03g.res\\n\", $1) }' names.dat\n</code></pre></p> <pre><code>H2O-1_001.res\nH2O-1_010.res\nH2O-1_123.res\nH2O-1_021.res\nH2O-1_044.res\nH2O-1_005.res\nH2O-1_007.res\n</code></pre> <p>Now one can pipe to <code>sort</code> and get them sorted. You can sort them in awk as well but the OVERHEAD does not worth the effort.</p> <p>Have a look on other string manipulation functions in awk. <code>printf</code> function is pretty much the same as in many other languages.</p>"},{"location":"6.One_line_programs/","title":"6.One line programs","text":"<p>Many useful awk programs are as short as just a line or two. Here is a collection of useful, short programs to get you started. Some of these programs contain constructs that haven't been covered yet. The description of the program will give you a good idea of what is going on. </p> <p>Here I realized that it is better to post some links and just mention some of my favorites, perhaps.</p> <ul> <li>The best AWK one-liners</li> <li>awk one-liners by *nix shell</li> <li>Handy One-line Scripts for AWK compiled by Eric Pement</li> </ul>"},{"location":"About/","title":"About","text":""},{"location":"About/#a-few-lines-about-me","title":"A few lines about me:","text":"<p>My name is Pavlin Mitev and I work as an Application Expert (Software Engineer) at Uppsala Multidisciplinary Center for Advanced Computational Science (UPPMAX), Uppsala University. I use commercial and open source software in my work, on a daily basis. Quite often I need to program, but a large number of the problems I need to solve include preparation of inputs (in some cases 1100 different inputs), collection of results from different programs, analysis or modification of the data before further use etc..</p> <p>Most of the time my data is buried in text readable outputs containing many different parameters, which makes the direct access to the numbers a bit problematic. All programs, more or less, use different formats for their inputs and in some cases there are available suites that are very helpful for this transitions. Unfortunately, quite often they just can't do what you want or it is not yet implemented, or ...</p> <p> Here is how I found myself using awk. Initially, as a middle-ware between programs, then gradually using more of the features that come with the language. Then, I found it so addictive that most of my small tools are written in awk, despite the fact that sometimes there are better alternative solutions. I even use awk to write Python code, only because I find it cumbersome to use Python to collect my data (ex: Python vs. awk). </p> <p>The purpose of the page is to illustrate some particularly strong sides of the language for text parsing, simple data collection, analysis, and much more. It is inspired by multiple simple solutions, often applied in mine and my colleagues' research work.</p> <p>This site also serves as an auxiliary material to an awk course/seminar given by UPPMAX, Uppsala University.</p> <p>Seminars are organized on a demand-base and currently follows the schedule for the \"Introductory Linux course\" at UPPMAX. Please, visit the UPPMAX course/seminar page for detailed information - venue, schedule, registration etc.</p> pavlin.mitev @ uppmax.uu.se Feedback from past workshops <ul> <li>2024.08 | 2024.01</li> <li>2023.09 | 2023.01 *</li> <li>2022.09 | 2022.01</li> <li>2021.09 | 2021.01</li> <li>2020.08 | 2020.01</li> <li>2019.08 | 2019.01</li> <li>2018.08 | 2018.01</li> <li>2017.01 | 2017.08</li> <li>2016.08 | 2016.01</li> <li>2015.10</li> </ul> <p>* can not be anonymised or violates GDPR</p> <p>Acknowledgment</p> <p>This page is supported by</p> <ul> <li>NAISS (2024 - )</li> <li>UPPMAX (2012 - )</li> <li>SNIC (2012 - 2022)</li> </ul>"},{"location":"Links/","title":"Links","text":""},{"location":"Links/#useful-places-to-start-learning-awk","title":"Useful places to start learning awk","text":"<ul> <li>30 Examples For Awk Command In Text Processing</li> <li>https://en.wikibooks.org/wiki/AWK</li> <li>http://www.grymoire.com/Unix/Awk.html</li> <li>Basic UNIX Commands - awk on YouTube</li> <li>awk Programming in Unix on YouTube</li> <li>Steve's Awk Academy - awk crash course</li> <li>Awk by example - IBM developerWorks</li> </ul>"},{"location":"Links/#awk-for-bioinformatics","title":"Awk for bioinformatics","text":"<ul> <li>Essential AWK Commands for Next Generation Sequence Analysis</li> <li>Linux and Bioinformatics</li> <li>AWK GTF! How to Analyze a Transcriptome Like a Pro</li> </ul>"},{"location":"Links/#other-links","title":"Other links","text":"<ul> <li>Execute awk online</li> </ul>"},{"location":"Python_vs_awk/","title":"Python vs. awk","text":"<p>Well, the title is misleading. I am not going to argue which language is better. Obviously, Python is more powerful and has vastly more features. Instead, here I have selected a particular case where (I think) the use of Python is not a good choice at all.</p> <p>Some time ago, I needed to extract frequency data from a Gaussian calculation. As many of us do, I searched the Net for already existing solutions, after all I do not want to rediscover the wheel...  Luckily, the very first hit was a very well written Python script that does the job. Here is the link to the original page with the solution. I have to say that ~50 lines of code were a bit too much for me and for this particular task. Here I will paste the code just to support my case.</p> <pre><code>#!/usr/bin/python\n# Compatible with python 2.7 \n# Reads frequency output from a g09 (gaussian) calculation\n# Usage ex.: g09freq g09.log ir.dat\nimport sys \n\ndef ints2float(integerlist):\n    for n in range(0,len(integerlist)):\n        integerlist[n]=float(integerlist[n])\n    return integerlist\n\ndef parse_in(infile):\n    g09output=open(infile,'r')\n    captured=[]\n    for line in g09output:\n        if ('Frequencies' in line) or ('Frc consts' in line) or ('IR Inten' in line):\n            captured+=[line.strip('\\n')]\n    g09output.close()\n    return captured\n\ndef format_captured(captured):\n    vibmatrix=[]\n    steps=len(captured)\n    for n in range(0,steps,3):\n        freqs=ints2float(filter(None,captured[n].split(' '))[2:5])\n        forces=ints2float(filter(None,captured[n+1].split(' '))[3:6])\n        intensities=ints2float(filter(None,captured[n+2].split(' '))[3:6])\n        for m in range(0,3):\n            vibmatrix+=[[freqs[m],forces[m],intensities[m]]]\n    return vibmatrix\n\ndef write_matrix(vibmatrix,outfile):\n    f=open(outfile,'w')\n    for n in range(0,len(vibmatrix)):\n        item=vibmatrix[n]\n        f.write(str(item[0])+'\\t'+str(item[1])+'\\t'+str(item[2])+'\\n')\n    f.close()\n    return 0\n\nif __name__ == \"__main__\":\n    infile=sys.argv[1]\n    outfile=sys.argv[2]\n\n    captured=parse_in(infile)\n\n    if len(captured)%3==0:\n        vibmatrix=format_captured(captured)\n    else:\n        print 'Number of elements not divisible by 3 (freq+force+intens=3)'\n        exit()\n    success=write_matrix(vibmatrix,outfile)\n    if success==0:\n        print 'Read %s, parsed it, and wrote %s'%(infile,outfile)\n</code></pre> <p>A large amount of the code deals with declarations of functions, reading and finding the data.</p> <p>Here is how this problem could be solved with awk: extract-freq.awk<pre><code>#!/usr/bin/awk -f\n\n/Frequencies/ { for (i=3;i&lt;=NF; i++) { im++; freq[im ]=$i } }\n/Frc consts/  { for (i=NF;i&gt;=4; i--)     fc[im-(NF-i)]=$i   }\n/IR Inten/    { for (i=NF;i&gt;=4; i--)     ir[im-(NF-i)]=$i   }\n\nEND { for (i=1;i&lt;=im;i++) print freq[i],fc[i],ir[i] }\n</code></pre> Somehow, I think this is much more readable and easier to modify...</p> python challenge <p>If you think that this could be done significantly easier in <code>python</code> than the python solution above, here is a link to a smaller file on which you can try your code. I will be glad to share the solution on this page.  <pre><code>./extract-freq.awk H2O.log\n\n23.1750 0.0019 1.4225\n34.8104 0.0044 0.0422\n42.9143 0.0060 0.0702\n46.5066 0.0071 2.7913\n48.1367 0.0081 7.5572\n79.9492 0.0282 0.0113\n87.5406 0.0309 2.7116\n93.0389 0.0417 2.5642\n121.2068 0.0585 1.9881\n145.7597 0.0776 2.0812\n. . . \n3048.5567 5.8718 1459.3611\n3180.5241 6.3493 796.1409\n3217.1453 6.5022 2372.1535\n3242.9776 6.5949 309.5433\n3247.6888 6.6231 2019.7043\n3260.1368 6.6708 59.4172\n3634.5856 8.2915 533.1068\n3636.1663 8.3042 226.5813\n3641.2286 8.3247 598.4021\n3644.1577 8.3436 313.5530\n3830.3485 9.2159 2.4124\n3854.9395 9.3284 12.6945\n3855.2245 9.3301 50.4032\n3858.7550 9.3463 39.0654\n3858.9545 9.3474 21.1611\n</code></pre></p> <ul> <li> <p>2023.03.23 - Solution by ryan-duve <pre><code>data = {\"Frequencies\":[], \"Frc consts\":[], \"IR Inten\":[]}\n\nwith open(\"H2O.log\") as f:\n    for line in f.readlines():\n        for label in data:\n            if label in line:\n                data[label].extend(line.split()[-3:])\n\nfor line in zip(*data.values()):\n    print(\" \".join(line))    \n</code></pre></p> </li> <li> <p>2023.03.24 - Solution by Neil Fitzgerald <pre><code>from sys import stdin\n\nfor line in stdin:\n    if 'Frequencies' in line:\n        mat = []\n        for i in range(3):\n            mat.append(line.split()[-3:])\n            line = next(stdin)\n            if i == 0: line = next(stdin)\n        for row in zip(*mat):\n            print('\\t'.join(row))\n</code></pre></p> </li> </ul>"},{"location":"arrays/","title":"Awk arrays","text":"<p>Here is a simple array definition and a way to scan its elements.</p> <pre><code>#!/usr/bin/awk -f\nBEGIN { \n  D[\"a\"]=\"A\"\n  D[\"b\"]=\"B\"\n  D[\"c\"]=\"C\"\n\n  for (i in D){        # loop over the  index\n    print i\" : \"D[i]\n  }\n}\n</code></pre> <p>Output: <pre><code>a : A\nb : B\nc : C\n</code></pre></p> <p>Note: by default, when a for loop traverses an array, the order is undefined, meaning that the awk implementation determines the order in which the array is traversed. This order is usually based on the internal implementation of arrays and will vary from one version of awk to the next.. Here is how to sort them Predefined Array Scanning order</p>"},{"location":"arrays/#multidimensional-arrays-docs","title":"Multidimensional arrays - docs","text":"<p>A multidimensional array is an array in which an element is identified by a sequence of indices instead of a single index. For example, a two-dimensional array requires two indices. The usual way (in many languages, including awk) to refer to an element of a two-dimensional array named grid is with <code>grid[x,y]</code>.</p> <p><pre><code>#!/usr/bin/awk -f      \nBEGIN {                \n  D[\"a\",\"A\"]=\"aA\"     \n  D[\"a\",\"B\"]=\"aB\"     \n  D[\"a\",\"C\"]=\"aC\"     \n\n  D[\"b\",\"A\"]=\"bA\"     \n\n  for (i in D){        # loop over the first index\n    print i\" : \"D[i]\n    print \"--------\"   \n  }                    \n}\n</code></pre> Output: <pre><code>aA : aA\n--------\naB : aB\n--------\naC : aC\n--------\nbA : bA\n--------\n</code></pre></p> <p>If you look carefully, <code>i</code> iterates over an index that is a string concatenated of both indexes. In other words, the combined string is used as a single index into an ordinary, one-dimensional array. This makes it somewhat dificult to iterate ovet the second index... but could be used in some specific solutions like Manipulating the output from a genome analysis - vcf and gff.</p>"},{"location":"arrays/#array-of-arrays-docs","title":"Array of arrays - docs","text":"<p>The so called \"Array of Arrays\" implementation is easier for scanning (iterating) than the above  multidimensional array implementation.</p> <p><pre><code>#!/usr/bin/awk -f\nBEGIN { \n  D[\"a\"][\"A\"]=\"aA\"\n  D[\"a\"][\"B\"]=\"aB\"\n  D[\"a\"][\"C\"]=\"aC\"\n\n  D[\"b\"][\"A\"]=\"bA\"\n\n  for (i in D){                    # loop over the first  index\n    for (j in D[i])                # loop over the second index\n      print i\",\"j\" : \"D[i][j]\n    print \"--------\"\n  }\n}\n</code></pre> Output <pre><code>a,A : aA\na,B : aB\na,C : aC\n--------\nb,A : bA\n--------\n</code></pre></p>"},{"location":"arrays/#more","title":"More","text":"<p>If you want to learn or just check what other \"tricks\" one could do with arrays in Awk, here a suggested tutorial on the topic - look for \"AWK tips and tricks\" section on the page.</p> <p>Quote</p> <p>AWK arrays appear only rarely online in tutorials, and I found myself  guilty of using arrays on my data blog (https://www.datafix.com.au/BASHing/) as though I was assuming that every reader was comfortable with them.</p> <p>This is not true, of course, so I wrote a 4-part series on the blog explaining how they work and how you can use them (see the \"AWK tips and tricks\" section of the Bashing data index page). One reader wrote me to say the articles had clarified for him how arrays work and he now felt more confident - always good feedback!</p> <p>Please feel free to link to those articles on your AWK site, or use them as models for your own tutorial work.</p> <p>Dr Robert Mesibov West Ulverstone, Tasmania, Australia 7315</p>"},{"location":"awk_bash/","title":"Awk or bash","text":"<p>The short answer - keep using bash but have a look at of some awk neat features that might come in handy.</p> <p><code>cmd= \"ls -lrt\"</code> - lets have the command stored for convenience</p> <p><code>system(cmd)</code> - will make a system call and execute the command, while sending the output to standard output</p> <p><code>cmd | getline</code> - will do the same but you will read the first line from the output.</p> <pre><code># then let's read all lines and do something\nwhile (cmd | getline) {    \n   if ($5 &gt; 200) system(\"cp \"$9\" target_folder/\")\n}\n</code></pre> <p>Essentially, one gets an extremely easy way to communicate between programs and execute system calls in addition to convenient arithmetic functions (for example, bash does not like fractional numbers) and you add a lot of flexibility to your decision making script...</p> <p>Have a look at the script that I wrote some 100 years ago that prepares a Gnuplot input to fit Birch\u2013Murnaghan equation of state, then gets the results from the fit (without using temporary files) and prettifies a little bit the final plot. The latest Gnuplot can actually do this internally, but have a look at what one can do with awk.</p>"},{"location":"awk_bash/#awk-and-multiple-files","title":"Awk and multiple files","text":"<p>Something that awk is really good at is handling of multiple files, as bash does, but combined with all the tools that comes with the language. Look at this Case study Multiple files - first approach to get an idea why awk could be a better choice in such situation.</p>"},{"location":"awk_bash/#awk-inside-bash-script","title":"Awk inside bash script","text":"<p>Perhaps, a better idea is to add transparently an awk code into your bash script. Here are simple examples that illustrates two different ways to do it.</p> <p><pre><code>#!/bin/bash\n\nawk '\n  BEGIN {print \"Hi 3\"}\n  {print}\n' /etc/issue\n\necho \"===================================\"\n</code></pre> Note that with the <code>&lt;&lt; EOF ... EOF</code> implementation needs the <code>$</code> sign needs to be escaped, otherwise bash will replace it with the content of the shell variable it refers to. This allows you to mix bash and awk in somewhat dangerous way...</p> <pre><code>#!/bin/bash\n\nawk -f - &lt;&lt; EOF /etc/os-release\n  BEGIN {print \"Hi\"}\n  {print \\$0}\nEOF\n\necho \"===================================\"\n</code></pre>"},{"location":"handy/","title":"Handy to have","text":"<p>Awk reference card (.pdf)</p> <p>Here are some functions that are missing in awk and might come in handy to have...  The functions have been collected from different sources or written by myself, so always make sure that they work as expected. Please, do not assume that they are always correct!</p> <p>For more complete and up to date list with various functions, look here: http://rosettacode.org/wiki/Category:AWK</p> <pre><code>#==============================================\nBEGIN {\n  pi=3.14159265358979;\n  rad2deg=180./pi\n  Q = \"\\\"\";\n   _ = SUBSEP;\n  Inf    = 1.99999*(2**(127));\n  NegInf = -1*2**(126);\n  White=\"^[ \\t\\n\\r]*$\";\n  Number=\"^[+-]?([0-9]+[.]?[0-9]*|[.][0-9]+)([eE][+-]?[0-9]+)?$\";\n}\n#==============================================\n\nfunction asin(a) { return atan2(a,sqrt(1-a*a)) }\nfunction acos(a) { return pi/2-asin(a) }\nfunction abs(x)  { return (x &gt;= 0) ? x : -x }\nfunction sgn(x)  { return (x == 0) ? 0 : ( (x &gt; 0) ? 1 : -1  ) }\nfunction coth(x) {  exp2x=exp(2.0*x);  return (exp2x + 1.0)/(exp2x -1.0) }\n\n#vectors\nfunction norm(x) {return (sqrt(x[1]*x[1]+x[2]*x[2]+x[3]*x[3]));}\nfunction dotprod (x,y) {return ( x[1]*y[1] + x[2]*y[2] + x[3]*y[3] );}\nfunction angle (v1,v2) {\n  myacos = dotprod(v1,v2)/norm(v1)/norm(v2);\n  if (myacos&gt; 1.0) myacos =  1.0;\n  if (myacos&lt;-1.0) myacos = -1.0;\n  return(acos(myacos)*180.0/pi);\n}\n\n\n\n# Printing\n\nfunction barph(str) {print str&gt;\"/dev/tty\"}\nfunction die(str)   {barph(str); exit 1}\n\n# Arrays\n# These array store the size of the array in position C&lt;Array[0]&gt;.\n\nfunction top(a) {return a[a[0]]}\nfunction push(a,x,  i) {i=++a[0]; a[i]=x; return i}\nfunction push2(a,x,y,  i) {i=++a[x,0]; a[x,i]=y; return i}\nfunction pop(a,   x,i) {\n i=a[0]--;\n if (!i) {return \"\"} else {x=a[i]; delete a[i]; return x}\n}\n\n\n# Arrays to strings\n\nfunction saya(s,a) {print s; print a2s(a)}\nfunction a2s(a,  n,pre,i,str) {\n  for(i in a) str= str pre \"[\" i \"]=[\" a[i] \"]\\n\";\n  return str;\n}\n\n# Strings\n\nfunction number(x)    { return x ~  Number  }\nfunction symbol(x)    { return ! number(x)  }\nfunction blank(x)     { return x~/^[ \\t]*$/ }\nfunction trimLeft(x)  { sub(/^[ \\t]*/,\"\",x); return x}\nfunction trimRight(x) { sub(/[ \\t]*$/,\"\",x); return x}\nfunction trim(x, y)   { return trimRight(trimLeft(x))}\nfunction str2keys(str,keys,sep,   n,i,tmp) {\n  n=split(str,tmp,sep);\n  for(i in tmp) keys[tmp[i]];\n  keys[0]=n;\n}\nfunction pairs2nums(str,pairs,sep, n,i,tmp) {\n  n=split(str,tmp,sep);\n  for(i=1;i&lt;=n;i=i+2) {\n    paris[tmp[i]]=tmp[i+1]+0;\n    pairs[0]++;\n  }\n}\n\n# Numbers\n\nfunction odd(x)     {return x % 2}\nfunction even(x)    {return ! odd(x)}\nfunction most(x,y)  { if (x&gt;y) {return x} else {return y}}\nfunction least(x,y) { if (x&lt;y) {return x} else {return y}}\nfunction round(x)   { if (x&lt;0) {return int(x-0.5)} else {return int(x+0.5)}}\nfunction between(min,max) {\n   if (max==min) {return min}\n   else {return min+ ((max-min)*rand())}}\nfunction mean(sumX,n) {return sumX/n}\nfunction sd(sumSq,sumX,n) {return sqrt((sumSq-((sumX*sumX)/n))/(n-1))}\n\n\n#File exists\n\nfunction exists(file, dummy, ret) {\n  ret=0;\n  if ( (getline dummy &lt; file) &gt;=0 ) {ret = 1; close(file)};\n  return ret;\n}\n\n# rewind.awk --- rewind the current file and start over\nfunction rewind(i) {\n    # shift remaining arguments up\n    for (i = ARGC; i &gt; ARGIND; i--)\n        ARGV[i] = ARGV[i-1]\n\n    # make sure gawk knows to keep going\n    ARGC++\n\n    # make current file next to get done\n    ARGV[ARGIND+1] = FILENAME\n\n    # do it\n    nextfile\n}\n</code></pre>"},{"location":"multiple_files/","title":"Multiple files ...","text":"<p>... or how to apply different rules for different input files.</p>"},{"location":"multiple_files/#common-practice-nr-and-fnr","title":"Common practice - <code>NR</code> and <code>FNR</code>","text":"<p>Perhaps one of the most common approach is utilizing the internally predefined variables <code>NR</code> and <code>FNR</code>.</p> <p>To illustrate this, we can create two files with some distinct data. You can run <code>seq -f \"File_1 - Line_%g\" 1 3</code> and <code>seq -f \"File_2 - Line_%g\" 1 3</code> and redirect the output to two separate files.</p> <pre><code>$ seq -f \"File_1 - Line_%g\" 1 3 \nFile_1 - Line_1\nFile_1 - Line_2\nFile_1 - Line_3\n\n$ seq -f \"File_2 - Line_%g\" 1 3 \nFile_2 - Line_1\nFile_2 - Line_2\nFile_2 - Line_3\n</code></pre> <p>Or use anonymous pipeline to run directly this example: <pre><code>awk '{print \"FNR: \"FNR\" NR: \"NR\"  =&gt; \"$0}' &lt;(seq -f \"File_1 - Line_%g\" 1 3) &lt;(seq -f \"File_2 - Line_%g\" 1 3)\nFNR: 1 NR: 1  =&gt; File_1 - Line_1\nFNR: 2 NR: 2  =&gt; File_1 - Line_2\nFNR: 3 NR: 3  =&gt; File_1 - Line_3\nFNR: 1 NR: 4  =&gt; File_2 - Line_1\nFNR: 2 NR: 5  =&gt; File_2 - Line_2\nFNR: 3 NR: 6  =&gt; File_2 - Line_3\n</code></pre></p> <p>Note that <code>FNR</code> keeps the line number in the file, while <code>NR</code> keeps incrementing - i.e. referring to the line number irrelevant of the file.</p> <p>It is rather common (in the awk community) to treat differently the first file which usually is realized by adding check for <code>NR==FNR</code> that is true only for the first file.</p> <pre><code>...\nNR==FNR { commands } # runs only on the first file\nNR!=FNR { commands } # runs on any but the first file\n        { commands } # runs on all files\n...\n</code></pre>"},{"location":"multiple_files/#filename-argind","title":"<code>FILENAME</code>, <code>ARGIND</code>","text":"<ul> <li><code>FILENAME</code> - The name of the current input file. When no data files are listed on the command line, awk reads from the standard input and <code>FILENAME</code> is set to \"-\". <code>FILENAME</code> changes each time a new file is read.</li> <li><code>ARGIND</code> - Every time gawk opens a new data file for processing, it sets <code>ARGIND</code> to the index in ARGV of the file name. When gawk is processing the input files, <code>\u0091FILENAME == ARGV[ARGIND]</code>\u0092 is always true.</li> </ul> <p>Warning</p> <p><code>ARGIND</code> is not working under OS X - you need gawk for this.</p>"},{"location":"tinyutils/","title":"Tiny utilities by Douglas Scofield","text":"<p>https://github.com/douglasgscofield/tinyutils</p> <p>Tiny scripts that work on a single column of data. Some of these transform a single column of their input while passing everything through, some produce summary tables, and some produce a single summary value. All (so far) are in awk, and all have the common options <code>header=0</code> which specifies the number of header lines on input to skip, <code>skip_comment=1</code> which specifies whether to skip comment lines on input that begin with <code>#</code>, and <code>col=1</code>, which specifies which column of the input stream should be examined. Since they are awk scripts, they also have the standard variables for specifying the input field separator <code>FS=\"\\t\"</code> and the output field separator <code>OFS=\"\\t\"</code>. Default output column separator is<code>\"\\t\"</code>.</p> <p>Set any of these variables by using <code>key=value</code> on the command line. For example to find the median of the third column of numbers, when the first 10 lines of input are header:</p> <pre><code>$ median col=3 header=10 your.dat\n</code></pre> <p>Stick these in a pipeline that ends with spark for quick visual summaries. If <code>indels.vcf.gz</code> is a compressed VCF file containing indel calls, then this will print a sparkline of indel sizes in the range of \u00b110bp:</p> <pre><code>$ zcat indels.vcf.gz \\\n| stripfilt \\\n| awk '{print length($5)-length($4)}' \\\n| inrange abs=10 \\\n| hist \\\n| cut -f2 \\\n| spark\n\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2588\u2581\u2587\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\n</code></pre> <p>We get the second column of hist output because that's the counts. This clearly shows the overabundance of single-base indels, and a slight overrepresentation of single-base deletions over insertions.</p> <p>...</p> <p>Please, visit the Git repository for complete documentation and up to date sources: https://github.com/douglasgscofield/tinyutils</p>"},{"location":"Bio/NCBI-taxonomy/","title":"Substitute scientific with common species names in a phylogenetic tree file","text":"<p>Problem formulated an presented at the workshop by Voichita Marinescu, Department of Medical Biochemistry and Microbiology, Comparative genetics and functional genomics</p>"},{"location":"Bio/NCBI-taxonomy/#step-1-generate-a-table-with-the-scientific-common-name-correspondence","title":"Step 1 - Generate a table with the scientific-common name correspondence","text":"<p>We need the correspondence between the scientific and common species names as described in the NCBI Taxonomy Database.</p> <p>We want to do this for any number of species automatically, so we download the entire archive taxdump.tar.gz from the NCBI taxonomy database dump.</p> <p>This archive contains the <code>names.dmp</code> file with the format: <pre><code>10090   |   house mouse |       |   genbank common name |\n10090   |   LK3 transgenic mice |       |   includes    |\n10090   |   mouse   |   mouse &lt;Mus musculus&gt;    |   common name |\n10090   |   Mus musculus Linnaeus, 1758 |       |   authority   |\n10090   |   Mus musculus    |       |   scientific name |\n10090   |   Mus sp. 129SV   |       |   includes    |\n10090   |   nude mice   |       |   includes    |\n10090   |   transgenic mice |       |   includes    |\n</code></pre></p> <p>Make new folder for this exercise and <code>cd</code> into it. Download the file and extract the <code>names.dmp</code></p> <pre><code>$ wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz\n$ tar -xvf taxdump.tar.gz names.dmp\n</code></pre> <pre><code>taxdump.tar.gz    51MB\nnames.dmp        189MB \n</code></pre> <ul> <li>We want to convert this table into a tab delimited file with this format: <pre><code>taxonID    scientific_name    common_name    genbank_common_name\n10090      Mus musculus       mouse          house mouse\n</code></pre></li> <li>We want to add an underscore between the genus (e.g. Mus) and the specific name of the species (e.g. musculus), so the scientific name is listed as in the tree file (e.g Mus_musculus).</li> <li>We want to capitalize the first word in the common name (if not already capitalized).</li> </ul>"},{"location":"Bio/NCBI-taxonomy/#step-2-edit-the-phylogenetic-tree-file","title":"Step 2 - Edit the phylogenetic tree file","text":"<p>The phylogenetic tree file used for the 100way alignment is hg38.100way.scientificNames.nh.  It can be downloaded from here and details could be found here.</p> <p>Download the file (4.1KB). <pre><code>$ wget http://hgdownload.cse.ucsc.edu/goldenpath/hg38/multiz100way/hg38.100way.scientificNames.nh\n</code></pre></p> <p>The format of the tree file is <pre><code>    ((((((((((((((((((Homo_sapiens:0.00655,\n                 Pan_troglodytes:0.00684):0.00422,\n                Gorilla_gorilla_gorilla:0.008964):0.009693,\n               Pongo_pygmaeus_abelii:0.01894):0.003471,\n              Nomascus_leucogenys:0.02227):0.01204,\n             (((Macaca_mulatta:0.004991,\n               Macaca_fascicularis:0.004991):0.003,\n              Papio_anubis:0.008042):0.01061,\n             Chlorocebus_sabaeus:0.027):0.025):0.02183,\n            (Callithrix_jacchus:0.03,\n            Saimiri_boliviensis:0.01035):0.01965):0.07261,\n           Otolemur_garnettii:0.13992):0.013494,\n          Tupaia_chinensis:0.174937):0.002,\n         (((Spermophilus_tridecemlineatus:0.125468,\n</code></pre></p> <p>See a description of the Newick tree format here.</p> <p>The phylogenetic tree could be visualized online at https://itol.embl.de/ (notice that this application takes care of removing the _ from the scientific name).</p> <p></p> <p>Before</p> <p></p> <p>After</p> <p></p>"},{"location":"Bio/NCBI-taxonomy/#step-1","title":"Step 1","text":"<p> !!! WARNING !!! WARNING !!! WARNING !!! </p> <p>The files are in Windows/DOS <code>ASCII text, with CRLF line terminators</code> format which makes awk to misbehave. Check your files and convert them to UNIX format if necessary.</p> <pre><code># check the format\n$ file filename\nfilename: ASCII text, with CRLF line terminators\n\n# Convert to unix format\n$ dos2unix filename\ndos2unix: converting file filename to Unix format ...\n\n# check again\n$ file filename\nfilename: ASCII text\n</code></pre> <p>Let's first tabulate the NCBI Taxonomy Database in more convenient format for us - getting the relevant information on single line, replace some spaces with underscore symbol <code>_</code>, remove the extra blanks in front and after the names, etc.</p> <p>names.tab</p> <pre><code>9605|Homo|Humans|\n9606|Homo_sapiens|Man|\n9608|Canidae|Dog, coyote, wolf, fox|\n9611|Canis||\n9612|Canis_lupus|Grey wolf|\n9614|Canis_latrans|Coyote|\n9615|Canis_lupus_familiaris|Dogs|\n9616|Canis_sp.||\n9619|Dusicyon||\n9620|Cerdocyon_thous|Crab-eating fox|\n9621|Lycaon||\n9622|Lycaon_pictus|Painted hunting dog|\n9623|Otocyon||\n9624|Otocyon_megalotis|Bat-eared fox|\n9625|Vulpes||\n9626|Vulpes_chama|Cape fox|\n9627|Vulpes_vulpes|Silver fox|\n9629|Vulpes_corsac|Corsac fox|\n9630|Vulpes_macrotis|Kit fox|\n9631|Vulpes_velox|Swift fox|\n</code></pre> <p>Might not be the best solution but it is easy to read and modify, for now. Note, we do not need to sort but it will look better if we have the final result in order.</p> <pre><code>$ ./01.tabulate-names.awk names.dmp | sort -g -k 1 &gt; names.tab\n\n# Or with bzip2 compression \"on the fly\"\n$ ./01.tabulate-names.awk &lt;(bzcat names.dmp.bz2) | sort -g -k 1 | bzip2 -c  &gt; names.tab.bz2\n</code></pre> 01.tabulate-names.awk <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  FS=\"|\"\n#  print \"#taxonID scientific_name        common_name     genbank_common_name\"\n}\n\n$4 ~ \"scientific name\"     { sciname[$1*1]=  unds(Clean($2)); next}\n\n# Order is important, since the second case will match lines that match the first case. \n$4 ~ \"genbank common name\" { genbank[$1*1]=  unds(Clean($2)); next}\n$4 ~ \"common name\"         { com_name[$1*1]= Cap(Clean($2));  next}\n\nEND{\n  for(i in sciname) print i\"|\"sciname[i]\"|\"com_name[i]\"|\"genbank[i]\n}\n\n# Function declarations ==============================\n\nfunction Clean (string){\n  sub(/^[ \\t]+/, \"\",string)\n  sub(/[ \\t]+$/, \"\",string)\n  return string\n}\n\nfunction unds(string) { gsub(\" \",\"_\",string); return string}\n\nfunction Cap (string) { return toupper(substr(string,0,1))substr(string,2) }\n</code></pre> <p>Note that this script will keep the last values for any match of the same ID. It appears that the database have repeated lines that does not contain complete information and the tabulated data get destroyed. To prevent this, we need to take care that any subsequent match will be ignored.</p> <pre><code>$ ./01.tabulate-names-first.awk names.dmp | sort -g -k 1 &gt; names-first.tab\n</code></pre> 01.tabulate-names-first.awk <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  FS=\"|\"\n#  print \"#taxonID scientific_name        common_name     genbank_common_name\"\n}\n\n$4 ~ \"scientific name\"     { if (! sciname[$1*1] ) sciname[$1*1]=  unds(Clean($2)); next}\n\n# Order is important, since the second case will match lines that match the first case. \n$4 ~ \"genbank common name\" { if (! genbank[$1*1] ) genbank[$1*1]=  unds(Clean($2)); next}\n$4 ~ \"common name\"         { if (! com_name[$1*1]) com_name[$1*1]= Cap(Clean($2));  next}\n\n\nEND{\n  for(i in sciname) print i\"|\"sciname[i]\"|\"com_name[i]\"|\"genbank[i]\n}\n\nfunction Clean (string){\n  sub(/^[ \\t]+/, \"\",string)\n  sub(/[ \\t]+$/, \"\",string)\n  return string\n}\n\nfunction unds(string) { gsub(\" \",\"_\",string); return string}\n\nfunction Cap (string) { return toupper(substr(string,0,1))substr(string,2) }\n</code></pre>"},{"location":"Bio/NCBI-taxonomy/#step-2","title":"Step 2","text":"<p>Now we can use the tabulated data in <code>names.tab</code> and perform the replacement in <code>hg38.100way.scientificNames.nh</code> by matching the scientific names in <code>$2</code> with the common names in <code>$3</code> - we use <code>FS=\"|\"</code></p> <pre><code>((((((((((((((((((Man:0.00655,\n                 Chimpanzee:0.00684):0.00422,\n                Western lowland gorilla:0.008964):0.009693,\n               Pongo_pygmaeus_abelii:0.01894):0.003471,\n              White-cheeked Gibbon:0.02227):0.01204,\n             (((Rhesus monkeys:0.004991,\n               Long-tailed macaque:0.004991):0.003,\n              Olive baboon:0.008042):0.01061,\n             Green monkey:0.027):0.025):0.02183,\n            (White-tufted-ear marmoset:0.03,\n            Bolivian squirrel monkey:0.01035):0.01965):0.07261,\n           Small-eared galago:0.13992):0.013494,\n</code></pre> <p>Again, this might not be the best way but it works. The suggested solutions could be easily merged into a single script. I would prefer to have them in steps, so I can make sure that the first step has completed successfully (it takes some time) before I continue. Also I can filter the unnecessary data in the newly tabulated file and use only relevant data or alter further if I need. </p> <pre><code>$ ./02.substitute.awk  names-first.tab  hg38.100way.scientificNames.nh &gt; NEW.g38.100way.scientificNames.nh\n</code></pre> 02.substitute.awk <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  FS=\"|\"\n#  print \"#taxonID scientific_name        common_name     genbank_common_name\"\n}\n\nNR== FNR{ map[$2]= $3; next}\n\n{\n  line= $0\n  gsub(\"[0-9.,;:)( ]\",\"\")\n  if ( $0 in map) sub($0,map[$0],line)\n  print line\n}\n</code></pre>"},{"location":"Bio/Stat-large-files/","title":"Statistics on very large columns of values","text":"<p>Problem formulated an presented at the workshop by Voichita Marinescu, Department of Medical Biochemistry and Microbiology, Comparative genetics and functional genomics </p> <p>When analyzing variables with large numbers of values, one needs to generate descriptive statistics (e.g. mean, median, std, quartiles, etc.) in order to set thresholds for further analyses.  This could easily be done in R if the vector or values could be loaded. But sometimes the number of values is prohibitively large for R, and even <code>pandas</code> in Python may fail.</p> <p>One such example is provided by the conservation scores for each nucleotide position of the MultiZ alignment of 99 vertebrate genomes against the human genome (UCSC 100way alignment). You visualized the phylogenetic tree for the species in this alignment in the previous exercise.  Using the program phyloP (phylogenetic P-values) from the PHAST package, a conservation score is computed for each position in the human genome resulting in 3 billion values. To identify the most conserved positions (the ones with the highest phyloP scores) one would need to generate descriptive statistics for the score distribution and set thresholds accordingly.</p> <p>We want to output the total number of values (count) and the mean, median, std, min, max, 10%, 25%, 75%, 90% of the score values, and also to plot the histogram.</p>"},{"location":"Bio/Stat-large-files/#input","title":"Input","text":"<p>The complete file in bigwig format is 5.5GB in size.</p> <ul> <li>Do not download the file <code>hg38.phastCons100way.bw</code> from http://hgdownload.soe.ucsc.edu/goldenPath/hg38/phastCons100way/</li> <li>The bigwig format can be converted to wig format using bigWigToWig            https://www.encodeproject.org/software/bigwigtowig/</li> <li>The wig format can be converted to bed format using wig2bed       https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/wig2bed.html</li> </ul> <p>For a short presentation of the main bioinformatics file formats see the UCSC Data File Formats page.</p> <p>In this exercise we will not work with complete file, but with smaller files downloaded using the UCSC Table Browser.</p> <p>We will use a 400kb interval on chr17:7,400,001-7,800,000 </p> <p>The Table Browser allows downloads for up to 100kb.</p> <ul> <li>chr17_7.400.001-7.500.000.data</li> <li>chr17_7.500.001-7.600.000.data</li> <li>chr17_7.600.001-7.700.000.data</li> <li>chr17_7.700.001-7.800.000.data</li> </ul> <p> !!! WARNING !!! WARNING !!! WARNING !!! </p> <p>The files are in Windows/DOS <code>ASCII text, with CRLF line terminators</code> format which makes awk to misbehave. Check your files and convert them to UNIX format if necessary.</p> <pre><code># check the format\n$ file filename\nfilename: ASCII text, with CRLF line terminators\n\n# Convert to unix format\n$ dos2unix filename\ndos2unix: converting file filename to Unix format ...\n\n# check again\n$ file filename\nfilename: ASCII text\n</code></pre> <p>The phyloP scores are in the second column.</p>"},{"location":"Bio/Stat-large-files/#compute-statistics","title":"Compute statistics","text":"<p>The memory problem for large data could be solved in Python with Dask. Since the analysis uses approximate methods for the quartile and other percentiles, you might need to resort to <code>percentiles_method=\"tdigest\"</code> which improves the final results.</p> <p>Here we will rely on the specifics of the data to implement an easy trick in awk (you can do it in any another language as well). We will calculate a discrete histogram over the values in the second column in the files. This generates too many elements, but looking into the data we can see that resolution is up to the 3 decimal points... So we will reduce the numbers (which in this case is even unnecessary but might be become a potential problem).</p>"},{"location":"Bio/Stat-large-files/#step-1","title":"Step 1","text":"<pre><code>$ ./01.histogram.awk *.data.txt | sort -g -k 1 &gt; hist.dat\n</code></pre> <p><code>01.histogram.awk</code> <pre><code>#!/usr/bin/awk -f\n#BEGIN{CONVFMT=\"%.3f\"}  # Uncomment if necessary\n\nNF==2 { counts[$2*1]++ }\n\nEND { for (v in counts) print v, counts[v] }\n</code></pre></p>"},{"location":"Bio/Stat-large-files/#step-2","title":"Step 2","text":"<p>Let's see what we got. <pre><code>plot \"hist.dat\" with line\n</code></pre></p> <p></p> <p>You can use other programs to make the histograms in intervals - here we can use gnuplot as example:</p> <pre><code>#!/usr/bin/gnuplot -persist\nset nokey\nset noytics\n\n# Find the mean\nmean= system(\"awk '{sum+=$1*$2; tot+=$2} END{print sum/tot}' hist.dat\")\n\nset arrow 1 from mean,0 to mean, graph 1 nohead ls 1 lc rgb \"blue\"\nset label 1 sprintf(\" Mean: %s\", mean) at mean, screen 0.1\n\n# Histogram\nbinwidth=0.1\nbin(x,width)=width*floor(x/width)\nplot 'hist.dat' using (bin($1,binwidth)):(1.0) smooth freq with lines\n</code></pre> <p></p>"},{"location":"Bio/Stat-large-files/#step-3","title":"Step 3","text":"<p>Let's find some relevant numbers from <code>hist.dat</code>.</p> <pre><code>$ head -n 1 hist.dat | awk '{print \"min:\\t\"$1}'\nmin:    -14.247\n\n$ tail -n 1 hist.dat | awk '{print \"max:\\t\"$1}'\nmax:    10.003\n</code></pre> <p>This was easy. How about the real numbers we are after? We need ianother program - again you can do it in other languages as well.</p> <pre><code>$ ./stats.awk hist.dat\ncount: 398516\nmean: 0.361477\n10% :-0.973866\n25% :-0.326732\n50% :0.127701\n75% :0.53989\n90% :1.72925\n</code></pre> <p><code>stats.awk</code> <pre><code>#!/usr/bin/awk -f\nBEGIN {\n  # Here sorting is very important, so we can count \n  # all elemnts in ascending numerical order for the indexes - first column\n  PROCINFO[\"sorted_in\"] = \"@ind_num_asc\" \n}\n\n{\n  counts[$1]+= $2\n  sum+= $1*$2\n  total+= $2\n}\n\nEND {\n  quantiles[1]=10;\n  quantiles[2]=25;\n  quantiles[3]=50;\n  quantiles[4]=75;\n  quantiles[5]=90;\n  q=1; nq=5\n  quantile=quantiles[q]\n\n  print \"count: \"total\n  print \"mean: \" sum/total\n\n  for (v in counts) {\n    comul+= counts[v]\n    if (comul &gt;= total*quantile/100) {\n      print quantiles[q]\"% :\" v\n      if (q &lt; nq) quantile= quantiles[++q]\n      else exit\n    }\n  }\n}\n</code></pre></p>"},{"location":"Bio/bioawk/","title":"Bioawk","text":"<p>Bioawk is an awk extension for biological formats written by Dr. Heng Li. See the documentation here .</p>"},{"location":"Bio/bioawk/#install-on-macos-with-the-homebrew-package-manager","title":"Install on MacOS with the Homebrew package manager","text":"<pre><code>$ brew install bioawk\n</code></pre>"},{"location":"Bio/bioawk/#conda","title":"Conda","text":"<pre><code>conda install -c bioconda bioawk\n</code></pre>"},{"location":"Bio/bioawk/#install-from-source","title":"Install from source","text":"<pre><code>$ git clone git://github.com/lh3/bioawk.git\n$ cd bioawk &amp;&amp; make \n\n# Make sure that bioawk is in your $PATH or use the full path to call the executable.\n</code></pre> <p> Several tutorial on the net suggest installing it with <code>sudo</code> in you system path - no need for that. This is another way to say - avoid unnecessary (it is bad practice) installation of tools as root. You will not be able to do it on any computer center anyway.  </p> <p>If you get error about <pre><code>...\nmake: yacc: Command not found\n</code></pre></p> <p>then you need install few packages (root privileges required)</p> <pre><code>$ sudo apt-get install bison byacc\n</code></pre>"},{"location":"Bio/bioawk/#examples","title":"Examples","text":""},{"location":"Bio/bioawk/#bioawk-supported-formats","title":"bioawk supported formats","text":"<pre><code>$ bioawk -c help\n\nbed:\n    1:chrom 2:start 3:end 4:name 5:score 6:strand 7:thickstart 8:thickend 9:rgb 10:blockcount 11:blocksizes 12:blockstarts \nsam:\n    1:qname 2:flag 3:rname 4:pos 5:mapq 6:cigar 7:rnext 8:pnext 9:tlen 10:seq 11:qual \nvcf:\n    1:chrom 2:pos 3:id 4:ref 5:alt 6:qual 7:filter 8:info \ngff:\n    1:seqname 2:source 3:feature 4:start 5:end 6:score 7:filter 8:strand 9:group 10:attribute \nfastx:\n    1:name 2:seq 3:qual 4:comment \n</code></pre>"},{"location":"Bio/bioawk/#we-will-use-gtf-and-fasta-files-for-the-chr177400001-7800000-region-downloaded-using-the-ucsc-table-browser","title":"We will use GTF and FASTA files for the chr17:7400001-7800000 region, downloaded using the UCSC Table Browser.","text":"<ul> <li><code>chr17_fragm.gtf</code></li> <li><code>chr17_fragm.fasta</code></li> </ul> <p>use the file <code>chr17_fragm.gtf</code></p>"},{"location":"Bio/bioawk/#print-the-length-of-all-the-exons-end-position-start-position","title":"Print the length of all the exons (end position - start position):","text":"<pre><code>$ bioawk -c gff  '$3 ~ /exon/ {print $seqname, $feature, $end-$start}' chr17_fragm.gtf | sort -nk3 \n</code></pre> <p>use the file <code>chr17_fragm.fasta</code></p>"},{"location":"Bio/bioawk/#count-the-number-of-fasta-entries","title":"Count the number of FASTA entries","text":"<pre><code>$ bioawk -c fastx 'END{print NR}' chr17_fragm.fasta\n</code></pre>"},{"location":"Bio/bioawk/#reverse-complement-the-sequences","title":"Reverse complement the sequences","text":"<pre><code>$ bioawk -c fastx '{print \"&gt;\"$name\"\\n\"revcomp($seq)}' chr17_fragm.fasta &gt; chr17_fragm.revcomp.fasta | head -n 2\n\n$ head -n 2 chr17_fragm.fasta\n$ head -n 2 chr17_fragm.revcomp.fasta\n</code></pre>"},{"location":"Bio/bioawk/#create-a-table-with-the-sequence-length-in-the-fasta-file","title":"Create a table with the sequence length in the FASTA file.","text":"<pre><code>$ bioawk -c fastx '{print $name,length($seq)}' chr17_fragm.fasta &gt; chr17_fragm.fasta.seqlen | head -n 10 chr17_fragm.fasta.seqlen\n$ head -n 10 chr17_fragm.fasta.seqlen\n</code></pre>"},{"location":"Case_studies/CHGCAR_diff/","title":"Multiple files - VASP CHGCAR difference","text":"<p>From time to time, I need to calculate electron charge difference that is tabulated on a regular 3D grid. Common examples in my field will be Gaussian .cube files or VASP CHGCAR files. One can find some scripts, tools and programs that can do this in one way or another. In my case, again, I need something slightly different and... Well, doing it with awk is so simple that I never use other tools but the one I will mention here. With small changes I am able to subtract 6 files at the same time, and since the files tend to be large, I keep them compressed.</p> <p>Some advantages of the script are:</p> <ul> <li>extremely small memory footprint (the calculation is done line by line)</li> <li>it does not need to know anything about the format, it only expects that all files have the same number of grid points (in each direction)</li> <li>the number of atoms in the CHGCAR could be different</li> </ul> <p>VASP-CHGCAR-diff.awk</p> <pre><code>#!/usr/bin/awk -f\n\nBEGIN {\n  cmd1= \"bzcat  \"ARGV[1];\n  cmd2= \"bzcat  \"ARGV[2];\n  cmd3= \"bzcat  \"ARGV[3];\n\n  NF=1;  while(NF&gt;0){ cmd2 |&amp; getline;       } cmd2 |&amp; getline;\n  NF=1;  while(NF&gt;0){ cmd3 |&amp; getline;       } cmd3 |&amp; getline;\n  NF=1;  while(NF&gt;0){ cmd1 |&amp; getline; print } cmd1 |&amp; getline; print;\n\n  do {\n    cmd1 |&amp; getline; split($0,d1)\n    cmd2 |&amp; getline; split($0,d2)\n    cmd3 |&amp; getline; split($0,d3)\n    for(i=1;i&lt;=NF;i++) printf \"%18.11e \", d1[i]-d2[i]-d3[i]\n    print\"\"\n  } while ($1*1==$1)\n\n}\n</code></pre> <p><code>cmd1</code>, <code>cmd2</code> and <code>cmd3</code> are the commands that will read the compressed files.</p> <p><code>NF=1; while(NF&gt;0){ cmd2 |&amp; getline; } cmd2 |&amp; getline;</code>   will fast forward to the grid data. The lines in the first file will be used as first lines in the new file. The rest is just reading line by line and calculating the difference column by column until it reaches the end of the file.</p> <p>And here are some charge difference plots produced with Python </p> <p></p> <p>Files</p> <ul> <li>VASP_CHGCAR_diff.awk</li> <li>plot-plane-v01.py</li> </ul>"},{"location":"Case_studies/Dipole_moment/","title":"Dipole moment example","text":"<p>Here is a simple script that will calculate the dipole moment of a molecule<sup>1</sup>. In this particular case I have used the result for a water molecule from a Wannier function localization calculation. To keep the script simple, I have added the charges of the species as 5<sup>th</sup> column in the file. You can perhaps change it, so it recognizes O with charge of 6 valence electrons, H with \\(1e\\), and X - Wannier center with charge of \\(-2e\\). The coordinates are in Angstroms.</p> <p>Here is the code <pre><code>#!/usr/bin/awk -f\n# x,y,z in Angstroms\nNR &gt; 2 {\n  mx= mx + $2*$5; my= my + $3*$5; mz= mz + $4*$5;\n}\n\nEND {\n  norm=sqrt(mx**2 + my**2 + mz**2);\n  # Convert to Debye\n  toD= 4.80320425;\n  mx= mx*toD; my= my*toD; mz= mz*toD;\n  norm= norm*toD;\n  printf(\"Dipole Moment\\t  x\\t\\t  y\\t\\t  z\\t\\t| D.Moment |\\n\");\n  printf(\" [ Debye ]   \\t%f\\t%f\\t%f\\t%f\\n\", mx, my, mz, norm);\n}\n</code></pre></p> <p>and here is the .xyz file with the charges added <pre><code>     7\n Wannier centres, written by Wannier90 on 6Aug2013 at 14:23:10\nX          6.50000026       6.18706578       6.50000000  -2\nX          6.50000000       6.71337672       6.50000002  -2\nX          6.50000008       6.95339960       6.49999998  -2\nX          6.49999971       6.54636884       6.50000000  -2\nH          6.50000000       7.09480000       7.26880000   1\nH          6.50000000       7.09480000       5.73120000   1\nO          6.50000000       6.50000000       6.50000000   6\n</code></pre></p> <p>Here is the output of the program <pre><code>$ ./dipole_moment.awk waterX.xyz\nDipole Moment     x               y               z             | D.Moment |\n [ Debye ]      -0.000000       1.869302        0.000000        1.869302\n</code></pre></p> <p>This example should be easy to alter, so you can calculate (if you want) the center of the mass of the molecule, the geometrical center, radius of gyration...</p> <p>Files</p> <ul> <li>dipole_moment.awk</li> <li>waterX.xyz</li> </ul> <ol> <li> <p>Dipole moment of an array of charges \u21a9</p> </li> </ol>"},{"location":"Case_studies/Discrete_histogram/","title":"Discrete histogram","text":"<p>This is an improved version of the script that was mentioned in the introduction of the task to count the countries in the coin.txt file. The usual histogram is a graphical representation of the distribution of numerical data. That also implies that you have regular intervals for the range of the data.  What if you want statistics over Months, days or any text - yes it is possible - remember the coins?</p> <p>It uses an undocumented feature that is rather useful. Here is the code, always check with the attached file which should contain the latest version.</p> <pre><code>#!/usr/bin/awk -f\nBEGIN {\n  if (!col) col=1\n}\n\n{\n  counts[$(col)]++;\n  total++;\n}\n\nEND {\n  for (v in counts) {\n    printf \"%s %.0f %f \\n\", v, counts[v], counts[v]/0.01/total ;\n  }\n}\n</code></pre> <p>Simply running the script over the file with the data (coins.txt) will calculate the distribution over the first column.</p> <pre><code>$ ./histogram-discrete.awk coins.txt\nsilver 4 30.769231\ngold   9 69.230769\n</code></pre> <p>This finds that there are 4 silver coins and 9 gold in the file. The last column is the percentage.</p> <p>Now, for the trick (special thanks to my colleague Douglas Scofield for introducing me to this trick). If you add col=4 before the name of the input file it will be interpreted as variable assignment (providing that you do not have a file with this name ;-)). Here is the result - nothing is changed in the script.</p> <pre><code>$ ./histogram-discrete.awk col=4 coins.txt\nSwitzerland     1  7.692308\nCanada          1  7.692308\nAustria-Hungary 1  7.692308\nPRC             1  7.692308\nRSA             2 15.384615\nUSA             7 53.846154\n</code></pre> <p>The official documentation says that you should write <code>-v col=4</code> or <code>--assign=col=4</code> so, keep this in mind. Notice that the output has no particular order. In the latest awk versions there is a way to force a particular order to the output but this is something I leave to you. You can always pipe the output via sort.</p> <p>Files</p> <ul> <li>coins.txt</li> <li>histogram-discrete.awk</li> </ul>"},{"location":"Case_studies/Fasta_tips/","title":"Fasta file format tips","text":"<p>With multi-fasta files, it is often required to extract few fasta sequences which contain the keyword/s of interest. One fast way to do this, is by awk.</p> <p>data.fa</p> <pre><code>&gt;Chr1\nATCTGCTGCTCGGGCTGCTCTAT...\n&gt;Chr2\nGTACGTCGTAGGACATGCATCG...\n&gt;MT1\nTACGATCGATCAGCTCAGCATC...\n&gt;MT2\nCGCCATGGATCAGCTACATGTA...\n</code></pre> <pre><code>$ awk 'BEGIN {RS=\"&gt;\"} /Chr2/ {print \"&gt;\"$0}' data.fa\n</code></pre> <p>Note that in the <code>BEGIN</code> section of the script, we have redefined the internal variable for the record separator <code>RS=\"&gt;\"</code> which by default is \"new line\". This way, awk will treat the whole fasta (multi-line) record as one record. </p> <p>output: <pre><code>&gt;Chr2\nGTACGTCGTAGGACATGCATCG...\n</code></pre></p> <p>example taken from: link</p>"},{"location":"Case_studies/Gaussian-extract-geometry/","title":"Gaussian - extract geometry from output .log file","text":"<p>It is rather common problem that one wants to extract the final geometry from a Gaussian calculation. It is possible to do it with GaussView, molden... But when you have hundreds of geometries...</p> <p>Here is a solution using Open Babel <pre><code>$ obabel -ig09 gaussian_output.log -oxyz\n8\ngaussian_output.log        Energy: -190290.8876477\nO          1.48397        0.74575       -0.00894\nH          0.54487        0.98777        0.10858\nO          1.36176       -0.69934       -0.11156\nH          1.80921       -0.98067        0.69811\nO         -1.48420       -0.74573        0.00817\nH         -0.54547       -0.98830       -0.11124\nO         -1.36140        0.69926        0.11245\nH         -1.80957        0.98168       -0.69645\n1 molecule converted\n</code></pre></p> <p>Which will find the last geometry (not sure what happens if the point is not stationary) and will write out the standard orientation of the molecule.</p> <p>Here is a solution in awk, which gives you the option to select which orientation you want: Input or Standard orientation</p> <p>By default the script will print the last geometry in the output.log file in \"Standard orientation\".</p> <pre><code>$ GAUSSIAN-log2xyz.awk  gaussian_output.log\n8\n  XYZ  -Stationary point- [Standard orientation]  extracted from: gaussian_output.log\n   O      1.48396700      0.74574700     -0.00893700\n   H      0.54487000      0.98777200      0.10857600\n   O      1.36176000     -0.69934000     -0.11155800\n   H      1.80920900     -0.98067200      0.69811200\n   O     -1.48420500     -0.74573000      0.00817100\n   H     -0.54547200     -0.98829500     -0.11124500\n   O     -1.36140100      0.69926200      0.11245000\n   H     -1.80957500      0.98167900     -0.69645200\n</code></pre> <p>To print in the Input orientation add outf=0 before the output.log i.e.</p> <pre><code>$ GAUSSIAN-log2xyz.awk outf=0 gaussian_output.log\n8\n  XYZ  -Stationary point- [Input orientation]  extracted from: gaussian_output.log\n   O     -0.70140300     -0.20578800      0.09097700\n   H     -1.11738400      0.67789200      0.07209400\n   O      0.69843900      0.17126700     -0.01863500\n   H      0.91917500     -0.18830400     -0.88864000\n   O      0.23362800      2.98135600      0.03930000\n   H      0.65037800      2.09806100      0.06000100\n   O     -1.16628300      2.60379100      0.14722400\n   H     -1.38832100      2.96390800      1.01668100\n</code></pre> <p>GAUSSIAN-log2xyz.awk</p> <pre><code>#!/bin/awk -f\nBEGIN{\n ss=\"H,He,Li,Be,B,C,N,O,F,Ne,Na,Mg,Al,Si,P,S,Cl,Ar,K,Ca,Sc,Ti,V,Cr,Mn,Fe,Co,Ni,Cu,Zn,Ga,Ge,As,Se,Br,Kr,Rb,Sr,Y,Zr,Nb,Mo,Tc,Ru,Rh,Pd,Ag,Cd,In,Sn,Sb,Te,I,Xe,Cs,Ba,La,Ce,Pr,Nd,Pm,Sm,Eu,Gd,Tb,Dy,Ho,Er,Tm,Yb,Lu,Hf,Ta,W,Re,Os,Ir,Pt,Au,Hg,Tl,Pb,Bi,Po,At,Rn,Fr,Ra,Ac,Th,Pa,U,Np,Pu,Am,Cm,Bk,Cf,Es,Fm,Md,No,Lr,Rf,Ha,D\"\n  split(ss,atsym,\",\")\n  outf=1 # Default output format\n  orient[0]= \"Input orientation\"\n  orient[1]= \"Standard orientation\"\n}\n\n/Stationary point found/{\n  SP= 1; # found stationary point\n  SPtxt= \" -Stationary point- \" # add info to the second line\n}\n\n$0 ~ orient[outf] &amp;&amp; (SP &lt; 2) {\n  if (SP==1) SP= 2 # stop looking for geometry output\n\n  getline; getline; getline; getline; getline;\n\n  iat= 0\n  do {\n    iat++\n    atn[iat]=$2; x[iat]=$4; y[iat]=$5; z[iat]=$6\n    getline;\n  } while (NF !=1)\n\n}\n\nEND{\n    print iat\"\\n  XYZ \"SPtxt\"[\"orient[outf]\"]  extracted from: \"FILENAME\n    for (i=1;i&lt;=iat;i++) printf (\"%4s  %14.8f  %14.8f  %14.8f\\n\",atsym[atn[i]],x[i],y[i],z[i])\n}\n</code></pre> <p>Files</p> <ul> <li>GAUSSIAN-log2xyz.awk</li> </ul>"},{"location":"Case_studies/Gaussian_smearing/","title":"Gaussian smearing","text":"<p>Sometimes it is useful or necessary to apply Gaussian smearing on your discrete values. For example if you want to add temperature broadening on theoretically calculated spectra (from Lattice Dynamics, normal mode analysis etc.).</p> <p>Here is a code that shows how it could be done in awk (illustrating the use of functions as well).</p> <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  FWHM=30; # Default smearing if none is provided on the command line\n  FWHM= FWHM/2.35482\n  # trick to read parameters from the command line\n  if (ARGC==3) FWHM=ARGV[2];  ARGC=2; \n}\n\n!/#/ {\n  f++;\n  if (f==1) {fmin=$1; fmax=$1}\n\n  freq[f]=$1;\n  if (fmin &gt; $1) fmin=$1;\n  if (fmax &lt; $1) fmax=$1;\n  nfreq=f;\n}\n\nEND {\n  print \"# freq   intensity | nfreq: \"nfreq\" fmin: \"fmin\" fmax: \"fmax\n  for (i=int(fmin -5*FWHM); i&lt;=int(fmax +5*FWHM);i++){\n    for (f=1;f&lt;=nfreq;f++){\n      data[i]= data[i] + gauss(freq[f],i,FWHM);\n    }\n    print i,data[i]\n  }  \n}\n\nfunction gauss(x0,x,c){\n  area=1;\n  if ((x-x0)**2 &lt; 10000) { return  area*exp(-(((x-x0))**2)/(2.*c**2))}\n  else {return 0.0}\n}\n</code></pre> <p>Here are a couple of interesting (or not) points in the code.</p> <p>In the <code>#awk BEGIN</code> section, we define default <code>FWHM</code> value in case none is provided on the command line. There are minimum checks that facilitate the command line input. When the script is executed, the argument values are stored in <code>ARGV</code> array and the number of arguments in <code>ARGC</code>. The zeroth element is the script's name itself. 1<sup>st</sup> will have the first parameter and so on. So, if <code>ARGC==3</code> (0, 1 and 2 will count as 3) then we set the second argument as <code>FWHM</code> in our script and decrease the <code>ARGC</code> back to 2 to trick the script that we have only 2 parameters on the command line. Otherwise, awk will try to read our 3<sup>rd</sup> parameter as a regular file.</p> <p>Then, for each line that is not a comment, the script reads the value in the first column and store it locally, while trying to keep track of the smallest and greatest value. </p> <p>The <code>END</code> section, essentially runs a loop over a range slightly larger than <code>fmax-fmin</code> and in turns <code>for (f=1;f&lt;=nfreq;f++)</code> calculates Gaussian contribution from each peak.</p> <p>The last section in the script is a user defined function for the Gaussian function with small precautions to avoid errors when the numbers become unreasonably small.</p> <p>To run the script: <pre><code>./Gauss-smearing.awk freq.dat 10\n</code></pre></p> <p>Here is the result from a Gnuplot script that calls the awk script directly, that avoids unnecessary creation of temporary files. </p> <p>All scripts and data files are attached bellow.</p> <p>Files</p> <ul> <li>gauss-smear-data.awk</li> <li>freq.dat</li> <li>ps-plot-v01.gnu - Gnuplot script</li> </ul> <p>Comment</p> <p>The script requires only small changes to handle intensities as well, can you do it yourself?</p>"},{"location":"Case_studies/Linear_interpolation/","title":"Linear interpolation","text":"<p>Here is a problem that occurs often. One have a set of data files that contain 2 columns (x and y) for some calculated property. Here I have selected only two such sets.</p> <p></p> <p>I want to be able to add them, subtract them or make an average. So, what is the problem?  The problem is, that the data is sampled in the same region but on different grids i.e. the number of points is different.</p> 1.dat <pre><code>   0.00    0.151576740\n   6.51    0.157566880\n  13.03    0.175107240\n  19.54    0.202843140\n...\n3967.59   -0.000073200\n3974.10   -0.000082352\n3980.62   -0.000095120\n3987.13   -0.000107504\n</code></pre> 2.dat <pre><code>   0.00    0.060664982\n   7.41    0.063726254\n  14.83    0.072136290\n  22.24    0.083837971\n...\n3965.71   -0.000059534\n3973.12   -0.000055689\n3980.53   -0.000048746\n3987.94   -0.000041886\n</code></pre> <p>One needs to have the values on the same grid to make some simple calculations. I am interested in the region x in [1500:4000] and with a small awk script I can do on-the-fly linear interpolation (which is good enough in this particular case). Here is how the interpolated data compares to the original.</p> 1_new.dat <pre><code>1500 0.00488383\n1501 0.0050328\n1502 0.00518177\n1503 0.00533075\n...\n</code></pre> 2_new.dat <pre><code>1500 0.00410784\n1501 0.00411207\n1502 0.0041163\n1503 0.00412054\n...\n</code></pre> <p>The script is running over integer numbers for easier understanding, but could be easily modified to handle real numbers for the grid values.</p> <p>interpolate-on-regular-grid.awk</p> <pre><code>#!/usr/bin/awk -f\nBEGIN {\n  minx= 1500;  # Range: lower limit\n  maxx= 4000;  #        upper limit\n  dx= 1;       #        increment\n}\n\n{\n  # check if the lower limit has been reached\n  if (($1 &gt; minx ) &amp;&amp; ($1 &lt; maxx)){\n    x2=$1; y2=$2;\n\n    # for the defined range\n    for (xi= minx; xi &lt;= maxx; xi= xi+dx){\n      # step forward if the current grid point is outside the [x1:x2] region\n      if (xi &gt; x2)  {\n        while (($1&lt;xi) &amp;&amp; (getline!=0) ) {x1=x2; y1=y2; x2=$1; y2=$2}\n      }\n      yi= (y2-y1)/(x2-x1)*(xi-x1) + y1; # linear interpolation\n      print xi, yi\n    }\n  }\n  x1= $1; y1=$2\n}\n</code></pre> <p>Here is the result: </p> <p>Warning</p> <p>I have written the script for my purposes and tested it against my data. It works for all of my cases, but could fail in some unforeseen situations so, please, always make sure that your results are reasonable. Do not blindly use the scripts provided on this site.</p> <p>Files</p> <ul> <li>interpolate-on-regular-grid.awk</li> <li>1.dat</li> <li>2.dat</li> <li>1_new.dat</li> <li>2_new.dat</li> </ul>"},{"location":"Case_studies/List/","title":"Case studies","text":"<p>Here is a collection of mine and contributed awk scripts.  </p>"},{"location":"Case_studies/List/#general-topic","title":"General topic","text":"<ul> <li>Awk and Jmol awk is using input data to write java script code to plot vectors in Jmol</li> <li>Multiple input files - first approach awk collects and assembles data from multiple files in memory</li> <li>Multiple input files - second approach awk collects data from multiple files but picks only the necessary data to save on memory</li> <li>Multiple output files MUST KNOW feature - covered during the workshop</li> <li>Color output with custom keywords use simple awk script to highlight keywords in your output</li> </ul>"},{"location":"Case_studies/List/#bioinformatics-oriented","title":"Bioinformatics oriented","text":"<ul> <li>bioawk Bioawk is an extension to Brian Kernighan's awk, adding the support of several common biological data formats, including optionally gzip'ed BED, GFF, SAM, VCF, FASTA/Q and TAB-delimited formats with column names.</li> <li>Fasta file format tips worth to know if working often with files in multi-fasta format</li> <li>Multiline fasta to single line fasta single cryptic-looking line that will decyphered during the workshop</li> <li>Sequence clustering with awk application of the multiple files approach - contribution by Mart\u00edn Gonz\u00e1lez Buitr\u00f3n</li> <li>Substitute scientific with common species names in a phylogenetic tree file</li> <li>Statistics on very large columns of values</li> <li>Manipulating and getting statistics for .vcf and .gff files</li> </ul>"},{"location":"Case_studies/List/#math-oriented","title":"Math oriented","text":"<ul> <li>Discrete histogram very handy discrete histogram awk code</li> <li>Gaussian smearing trivial task done with awk - example how to use functions</li> <li>Linear interpolation use linear interpolation to resample your data on different grid</li> </ul>"},{"location":"Case_studies/List/#physics-oriented","title":"Physics oriented","text":"<ul> <li>Dipole moment example simple calculations should not be difficult to code - here is an example</li> <li>Multiple files - VASP CHGCAR difference an simplified example on how to read multiple files (bzip-ed) line-by-line simultaneously to save memory </li> <li>POSCAR: reorder atom types simple task creates programming nightmare</li> </ul>"},{"location":"Case_studies/List/#primarily-used-as-reference","title":"Primarily used as reference","text":"<ul> <li>Awk and Gnuplot outdated problem but shows hot to send data to another program and read back the results</li> <li>Awk writes Python collecting your data might be so tedious to program, that you might wamt to use awk to write python instead</li> <li>Gaussian - extract geometry from output .log file example</li> <li>ProLiant Status check simple check on some values, collected and mailed</li> <li>Awk and Networking awk have some advance network protocol capabilities...</li> </ul>"},{"location":"Case_studies/Multi2single_fasta/","title":"Multiline Fasta To Single Line Fasta","text":"<p>I got the question, solved it, then I realized it is rather common problem.  So, I googled for it and the solution, I have to admit, was better than mine.  Here is the solution and link to the original source.</p> <p>I have a fasta file with following format</p> <p>file.fa<pre><code>&gt;gi|321257144|ref|XP_003193485.1| flap endonuclease [Cryptococcus gattii WM276]\nMGIKGLTGLLSENAPKCMKDHEMKTLFGRKVAIDASMSIYQFLIAVRQQDGQMLMNESGDVTSHLMGFFY\nRTIRMVDHGIKPCYIFDGKPPELKGSVLAKRFARREEAKEGEEEAKETGTAEDVDKLARRQVRVTREHNE\nECKKLLSLMGIPVVTAPGEAEAQCAELARAGKVYAAGSEDMDTLTFHSPILLRHLTFSEAKKMPISEIHL\nDVALRDLEMSMDQFIELCILLGCDYLEPCKGIGPKTALKLMREHGTLGKVVEHIRGKMAEKAEEIKAAAD\nEEAEAEAEAEKYDSDPENEEGGETMINSDGEEVPAPSKPKSPKKKAPAKKKKIASSGMQIPEFWPWEEAK\nQLFLKPDVVNGDDLVLEWKQPDTEGLVEFLCRDKGFNEDRVRAGAAKLSKMLAAKQQGRLDGFFTVKPKE\nPAAKDAGKGKGKDTKGEKRKAEEKGAAKKKTKK\n&gt;gi|321473340|gb|EFX84308.1| hypothetical protein DAPPUDRAFT_47502 [Daphnia pulex]\nMGIKGLTQVIGDTAPTAIKENEIKNYFGRKVAIDASMSIYQFLIAVRSEGAMLTSADGETTSHLMGIFYR\nTIRMVDNGIKPVYVFDGKPPDMKGGELTKRAEKREEASKQLVLATDAGDAVEMEKMNKRLVKVNKGHTDE\nCKQLLTLMGIPYVEAPCEAEAQCAALVKAGKVYATATEDMDSLTFGSNVLLRYLTYSEAKKMPIKEFHLD\nKILDGLSYTMDEFIDLCIMLGCDYCDTIKGIGAKRAKELIDKHRCIEKVIENLDTKKYTVPENWPYQEAR\nRLFKTPDVADAETLDLKWTQPDEEGLVKFMCGDKNFNEERIRSGAKKLCKAKTGQTQGRLDSFFKVLPSS\nKPSTPSTPASKRKVGCIIYLFLYF\n</code></pre> but I wanna to look sequence in a single line, not in many line as they are. any quick method??</p> <p>using awk: <pre><code>awk '/^&gt;/ {printf(\"\\n%s\\n\",$0);next; } { printf(\"%s\",$0);}  END {printf(\"\\n\");}' file.fa\n\n&gt;gi|321257144|ref|XP_003193485.1| flap endonuclease [Cryptococcus gattii WM276]\nMGIKGLTGLLSENAPKCMKDHEMKTLFGRKVAIDASMSIYQFLIAVRQQDGQMLMNESGDVTSHLMGFFYRTIRMVDHGIKPCYIFDGKPPELKGSVLAKRFARREEAKEGEEEAKETGTAEDVDKLARRQVRVTREHNEECKKLLSLMGIPVVTAPGEAEAQCAELARAGKVYAAGSEDMDTLTFHSPILLRHLTFSEAKKMPISEIHLDVALRDLEMSMDQFIELCILLGCDYLEPCKGIGPKTALKLMREHGTLGKVVEHIRGKMAEKAEEIKAAADEEAEAEAEAEKYDSDPENEEGGETMINSDGEEVPAPSKPKSPKKKAPAKKKKIASSGMQIPEFWPWEEAKQLFLKPDVVNGDDLVLEWKQPDTEGLVEFLCRDKGFNEDRVRAGAAKLSKMLAAKQQGRLDGFFTVKPKEPAAKDAGKGKGKDTKGEKRKAEEKGAAKKKTKK\n&gt;gi|321473340|gb|EFX84308.1| hypothetical protein DAPPUDRAFT_47502 [Daphnia pulex]\nMGIKGLTQVIGDTAPTAIKENEIKNYFGRKVAIDASMSIYQFLIAVRSEGAMLTSADGETTSHLMGIFYRTIRMVDNGIKPVYVFDGKPPDMKGGELTKRAEKREEASKQLVLATDAGDAVEMEKMNKRLVKVNKGHTDECKQLLTLMGIPYVEAPCEAEAQCAALVKAGKVYATATEDMDSLTFGSNVLLRYLTYSEAKKMPIKEFHLDKILDGLSYTMDEFIDLCIMLGCDYCDTIKGIGAKRAKELIDKHRCIEKVIENLDTKKYTVPENWPYQEARRLFKTPDVADAETLDLKWTQPDEEGLVKFMCGDKNFNEERIRSGAKKLCKAKTGQTQGRLDSFFKVLPSSKPSTPSTPASKRKVGCIIYLFLYF\n</code></pre></p> <p>Note</p> <p>Note that there is a blank line at the beginning of the output - this is further discusses in the original forum post. This example case will be discussed during the workshop</p>"},{"location":"Case_studies/Multiple_output_files/","title":"Multiple output files","text":"<p>Here is a type of problem particularly suited for awk. </p> <p>Let's use the example with the coins again  and assume that the file is much larger and you want to print the data in separate files, named after the country and the year, for example \"USA.1986.txt\".</p> <p>This will require to write into multiple files in different order. Most languages will require to open files (in append mode), assign file handles and closing it. If you want to avoid multiple opening/closing you need to handle the list of open files yourself. This puts some extra overhead that could be easily avoided in awk (and all shells [bash, csh, tcsh] as long as I know). It is as easy as writing:</p> <pre><code>print \"whatever you want\" &gt;  country\".\"year\".txt\"\nprint \"whatever you want\" &gt;&gt; country\".\"year\".txt\"\n</code></pre> <p>Here is an one line variant of it... <pre><code>$ awk '{print $0 &gt; $4\".\"$3\".txt\"}' coins.txt\n$ wc -l *.txt\n   1 Austria-Hungary.1908.txt\n   1 Canada.1988.txt\n  13 coins.txt\n   1 PRC.1986.txt\n   1 RSA.1979.txt\n   1 RSA.1981.txt\n   1 Switzerland.1984.txt\n   1 USA.1981.txt\n   4 USA.1986.txt\n   2 USA.1987.txt\n  26 total\n</code></pre></p> <p>Awk can keep up to 1024 open files at the same time.</p> <p>Warning</p> <p>The example above is not working under OS X . A simple fix is to store the file name in a variable before printing into it i.e.</p> <p>OS X version. It will work with Gnu Awk as well</p> <p><pre><code>$ awk '{f=$4\".\"$3\".txt\"; print $0 &gt; f }' coins.txt\n</code></pre> Credits to Ding He for the tip original source: http://stackoverflow.com/questions/7980325/splitting-a-file-using-awk-on-mac-os-x</p>"},{"location":"Case_studies/POSCAR_reorder/","title":"POSCAR: reorder atom types","text":"<p>This study case is designed to illustrate a combination of awk features while solving an easy to describe problem but painfully annoying to program. It contains only the basic functionality and does not check for error,  does not handle some exceptions in the File format etc.</p> <p>POSCAR is an text format structure input file for the VASP computational code. It defines the lattice geometry and the ionic positions. A limitation of the format is that the positions of the ions cannot be provided in random order, which makes the task of reordering them particularly unpleasant. Nowadays one can find tools to do this, but here we pick the case for the purpose of the workshop.</p> <p>POSCAR</p> <pre><code>MgOH2                                   \n    1.000000000000000     \n     3.18083395988771     0.00000000000000      0.00000000000000  \n    -1.59041697994386     2.75468301448513      0.00000000000000  \n     0.00000000000000     0.00000000000000      4.76789525772412\n  Mg O H\n  1 2 2\nDirect\n  0.0000000000000000  0.0000000000000000  0.000000000000\n  0.3333333333300033  0.6666666666599994  0.219129582305 \n  0.6666666666599994  0.3333333333300033 -0.219129582305 \n  0.3333333333300033  0.6666666666599994  0.423195382917 \n  0.6666666666599994  0.3333333333300033 -0.423195382917\n</code></pre> <p>The lines below <code>Direct</code> are the positions of the ions in order specified on line 6 - <code>Mg O H</code> . Yet, this is not enough, since on line 7 one gets the number of each ion type... The example above has 3 types and 5 ions in total and it is easier to manually fix the desired order. When this file contains more than 100 ions and several types, this simple routine becomes extremely dangerous, since any error is very difficult to spot.</p> <p>Here we have a code that we can easily build interactively as exercise, but below each block is briefly described.</p> <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  argc= ARGC;\n  ARGC=2 # just read the first file\n}\n\nNR&lt;=5 {print} # print the part that does not change\n\nNR==6 {\n  Ntypes= NF;                           # Keep the number of different types\n  for(i=1; i&lt;=NF; i++){element[i]=$i}   # Make list with the elements\n} \n\nNR==7 {for(i=1; i&lt;=NF; i++){elementN[element[i]]=$i} } # count the number of elements\n\n/Direct/ {\n  for(i=1; i&lt;=Ntypes;i++){                   # loop over the different types\n    for(j=1; j&lt;=elementN[element[i]]; j++){  # loop over the number of atoms for each type\n      getline                                # get one line\n      line[element[i],j]= $0                 # keep the whole line,[atom name, N]\n    }\n  }\n}\n\nEND{\n  for(i=1; i&lt;=argc-2; i++){\n    printf(\"%s \",ARGV[i+1])                  # print the new order\n  }\n  print \"\"                                   # print new line\n  for(i=1; i&lt;=argc-2; i++){\n    printf(\"%s \",elementN[ARGV[i+1]])        # print the type numbers in the order\n  }\n  print \"\"                                   # print new line\n\n  print \"Direct\"\n  for(i=1; i&lt;=argc-2; i++){\n    atom= ARGV[i+1]\n    for(j=1; j&lt;=elementN[atom]; j++){\n      print line[atom,j]\n    }\n  }\n}\n</code></pre> <p>We will put a code that will read the provided file as first parameter and use the remaining to specify the new order of ions.</p> <p><code>BEGIN{...}</code> we need the original number of arguments, that is why we first <code>argc= ARGC;</code> Then we change the number of parameters to trick awk to read only the first argument <code>ARGC=2</code> .</p> <p><code>NR&lt;=5 {print}</code> lines 1 to 5 does not need to be changed - so we print them.</p> <p><code>NR==6 {...}</code>  We need to know the number of different types Ntypes= NF and then make a index list with the labels of the elements.   </p> <p><code>NR==7 {...)</code> We match the number of elements for each type, so it becomes <code>elementN[\"Mg\"]=1; elementN[\"O\"]=2; elementN[\"H\"]=2;</code></p> <p><code>/Direct/ {...}</code> We have all the information to read the following lines with a double loop over each type and corresponding number. Done. We have each line indexed in a way that we can access them in a random order. The first line will look like <code>line[\"Mg\",1]= 0.0000000000000000 0.0000000000000000 0.000000000000</code> . Note, we do not need to do anything but reorder the lines, so we keep them intact.</p> <p><code>END{...}</code> Time to fetch the new order from the command line and print in loops by calling the collected data.</p> <p>Output <pre><code>./POSCAR-reorder.awk POSCAR H O Mg\nMgOH2\n    1.000000000000000\n     3.18083395988771     0.00000000000000      0.00000000000000\n    -1.59041697994386     2.75468301448513      0.00000000000000\n     0.00000000000000     0.00000000000000      4.76789525772412\nH O Mg\n2 2 1\nDirect\n  0.3333333333300033  0.6666666666599994  0.423195382917\n  0.6666666666599994  0.3333333333300033 -0.423195382917\n  0.3333333333300033  0.6666666666599994  0.219129582305\n  0.6666666666599994  0.3333333333300033 -0.219129582305\n  0.0000000000000000  0.0000000000000000  0.000000000000\n</code></pre></p> <p>Easy or not?</p> <p>This might look complicated but it mostly contains for-loops that collect the data. What could be an advantage then?  If you look, each block matches and operates only on the targeted lines - makes it a bit easier to split the problem to separate tasks.</p>"},{"location":"Case_studies/ProLiant_status_check/","title":"ProLiant Status check","text":"<p>Here is another example of a common problem solved by awk script.</p> <p>If you are running an HP ProLiant server, you might want to receive mail with the system health every day. In general that is easy, but what I want is, to check the report before delivery and make the subject reflect the general result i.e. \"\" [ProLiant Status]: OK - if everything is fine or [ProLiant Status]: !!!WARNING!!! followed by the problematic line/s</p> <p>So, awk makes system calls and checks for \"hot\" keywords (FAIL, nok) indicating failures and checks the reported temperature values against the thresholds.</p> <p>ProLiant_Status.awk</p> <pre><code>#!/usr/bin/awk -f\n# This script will execute the commands bellow, collect the output\n# then check for failures and mail the status\n# The subject of the mail will be:\n# [ProLiant Status]: OK - if everything is fine or\n# [ProLiant Status]: !!!WARNING!!! followed by the problematic line/s\nBEGIN {\n\n  cmd=\"/usr/sbin/hpacucli controller all show status\";\n  line[l++]=\"&gt; \"cmd;  get_output(cmd);\n\n  cmd=\"/usr/sbin/hpacucli controller slot=0 logicaldrive all show status\";\n  line[l++]=\"&gt; \"cmd; get_output(cmd);\n\n  cmd=\"/usr/sbin/hpacucli controller slot=0 physicaldrive all show status\";\n  line[l++]=\"&gt; \"cmd; get_output(cmd);\n\n  cmd=\"/sbin/hpasmcli -s \\\"SHOW ASR\\\"\";\n  line[l++]=\"&gt; \"cmd; get_output(cmd);\n\n  cmd=\"/sbin/hpasmcli -s \\\"SHOW FANS\\\"\";\n  line[l++]=\"&gt; \"cmd; get_output(cmd);\n\n  cmd=\"/sbin/hpasmcli -s \\\"SHOW TEMP\\\"\";\n  line[l++]=\"&gt; \"cmd; get_output(cmd);\n\n# Compose a mail containing the output\n  if (! SUBJ) SUBJ=\"[ProLiant Status - allium]: OK\";\n  else SUBJ=\"[ProLiant Status - allium]: !!!WARNING!!! \"SUBJ\n\n  cmd=\"mail -s \\\"\"SUBJ\"\\\" root\"\n#  cmd=\"cat\" # print on screen instead mailing\n  for (i=0;i&lt;=l;i++) {\n    print line[i] | cmd\n  }\n  close(cmd);\n}\n\n# Collects the output and checks for failures\nfunction get_output(command) {\n\n  while (cmd | getline) {\n    line[l++]=$0;\n    # check the line for NOK or FAIL\n    if (tolower($0) ~ \"nok|fail\")  SUBJ=SUBJ\" \"$0\n\n    # if the line contains temperature info - check the threshold\n    if ($0 ~ \"C.*F.*C.*F\") {\n      gsub(\"C/\",\" \",$0); # trick to get easily only the numbers\n      if ($3 &gt;= $5) SUBJ=SUBJ\" \"$0\n    }\n  }\n  close(cmd)\n}\n</code></pre> <p>Here is how the report looks like: <pre><code>Subject: [ProLiant Status - aberlour]: OK\n\n&gt; /usr/sbin/hpacucli controller all show status\n\nSmart Array P212 in Slot 1\n   Controller Status: OK\n   Cache Status: OK\n   Battery/Capacitor Status: OK\n\n\n&gt; /usr/sbin/hpacucli controller slot=1 logicaldrive all show status\n\n   logicaldrive 1 (40.0 GB, 6): OK\n   logicaldrive 2 (16.3 TB, 6): OK\n\n&gt; /usr/sbin/hpacucli controller slot=1 physicaldrive all show status\n\n   physicaldrive 1I:1:1 (port 1I:box 1:bay 1, 3 TB): OK\n   physicaldrive 1I:1:2 (port 1I:box 1:bay 2, 3 TB): OK\n   physicaldrive 1I:1:3 (port 1I:box 1:bay 3, 3 TB): OK\n   physicaldrive 1I:1:4 (port 1I:box 1:bay 4, 3 TB): OK\n   physicaldrive 1I:1:5 (port 1I:box 1:bay 5, 3 TB): OK\n   physicaldrive 1I:1:6 (port 1I:box 1:bay 6, 3 TB): OK\n   physicaldrive 1I:1:7 (port 1I:box 1:bay 7, 3 TB): OK\n   physicaldrive 1I:1:8 (port 1I:box 1:bay 8, 3 TB): OK\n\n&gt; /sbin/hpasmcli -s \"SHOW ASR\"\n\nASR timeout is 10 minutes.\nASR is currently enabled.\n\n&gt; /sbin/hpasmcli -s \"SHOW FANS\"\n\nFan  Location        Present Speed  of max  Redundant  Partner  Hot-pluggable\n---  --------        ------- -----  ------  ---------  -------  -------------\n#1   SYSTEM          Yes     NORMAL  55%     N/A        N/A      No            \n#2   SYSTEM          Yes     NORMAL  55%     N/A        N/A      No            \n#3   SYSTEM          Yes     NORMAL  59%     N/A        N/A      No            \n#4   SYSTEM          Yes     NORMAL  53%     N/A        N/A      No            \n\n\n&gt; /sbin/hpasmcli -s \"SHOW TEMP\"\n\nSensor   Location              Temp       Threshold\n------   --------              ----       ---------\n#1        MEMORY_BD            33C/91F    87C/188F \n#2        MEMORY_BD             -         87C/188F \n#3        MEMORY_BD            33C/91F    87C/188F \n#4        MEMORY_BD             -         87C/188F \n#5        MEMORY_BD             -         87C/188F \n#6        MEMORY_BD             -         87C/188F \n#7        MEMORY_BD            40C/104F   95C/203F \n#8        MEMORY_BD             -         87C/188F \n#9        MEMORY_BD             -         87C/188F \n#10       MEMORY_BD             -         87C/188F \n#11       MEMORY_BD             -         87C/188F \n#12       MEMORY_BD             -         87C/188F \n#13       MEMORY_BD             -         87C/188F \n#14       MEMORY_BD             -         95C/203F \n#15       SYSTEM_BD             -         60C/140F \n#16       SYSTEM_BD             -         60C/140F \n#17       AMBIENT              30C/86F    60C/140F \n#18       AMBIENT              38C/100F   70C/158F \n#19       SYSTEM_BD            17C/62F    112C/233F\n#20       SYSTEM_BD            41C/105F   79C/174F \n#21       SYSTEM_BD            31C/87F    60C/140F\n</code></pre></p> <p>Files</p> <ul> <li>ProLiant_Status.awk</li> </ul>"},{"location":"Case_studies/Sequence_clustering/","title":"Sequence clustering with awk","text":"<p>Contribution by Mart\u00edn Gonz\u00e1lez Buitr\u00f3n, National University of Quilmes, Argentina, exchange PhD student - Stockholm University 2020.04</p> <p>CDHit is a software that allows users to perform sequence clustering. An important thing about clustering is to evaluate clustering conditions, i.e: identity, coverage, etc. One main problem about different conditions is to know what happen with each sequence. Lucky us, CDHit standard output has few patterns that anyone could exploit using AWK. These are, how clusters are written and how sequence lines belong in one cluster begins.</p> <p>Suppose someone has a non-redundant dataset of hundred or thousands of sequences and performed two clustering using CDHit. Moreover, suppose that earlier dataset is diverse. A problem appears, which configuration is better for your clustering goal?</p> <p>Knowing that, the following script process both standard CDHit clustering files (ended in *.clstr) and checks in which cluster it contains each sequence. If the above is done by hand, it could not be so good for the health. So, the script output has these fields in CSV format: <pre><code>query_code, query_length, cluster_number_in_file_1, cluster_size_in_file_1, target_code, target_length, cluster_number_in_file_2, cluster_size_in_file_2, cluster_size_diff, same_cluster\n</code></pre></p> <p>Where <code>same_cluster</code> is a boolean column and has information about the changing location of that sequence, i.e:  </p> <p>same_cluster = Yes  ------&gt; that sequence didn't change of cluster same_cluster = No  ------&gt; that sequence change to other cluster (new or already existed)</p> <p>Where <code>cluster_size_diff</code> has information about the difference in size from each cluster (cluster_size_in_file_2 - cluster_size_in_file_1):  </p> <p>cluster_size_diff = 0  ------&gt; that sequence is not in a bigger or smaller cluster cluster_size_diff &gt; 0  ------&gt; that sequence is in a bigger cluster (new or already existed) cluster_size_diff &lt; 0  ------&gt; that sequence is in a smaller cluster (new or already existed)</p> <p>Warnings!</p> <ol> <li>Could be that <code>cluster_size_diff</code> is different than 0 but the cluster continue being the same (e.g: look sequences in Cluster_0). That happen due how CDHit works and of course, the values in clustering conditions what were used to compare. For a deeper and accurate way to understand what is happening, you should then check these three columns, same_cluster and both \"cluster_size\" for any sequence, easily you can check column $4, $8 and $10.</li> <li>Cloud be that <code>same_cluster</code> is \"No\" but the cluster continue being pretty the same (e.g: look sequences in Cluster_1 to Cluster_4). That happen due how CDHit works and of course, the values in clustering conditions what were used to compare. For a deeper and accurate way to understand what is happening in this hard case, you should then check sequence neighbours and/or these two \"<code>cluster_size</code>\" columns for any sequence, easily you can check column $4, $8. For example, for bigger clusters in File 1 is more probable that those clusters leak/add some sequence in other CDHit condition but not changing to much in size (do size percentage study if necessary). </li> <li>There are others possible results that you may need to study on detail, like:<ul> <li>Creation of new clusters (\"border sequences\" in different clusters form this new cluster or just sequences that were alone are now together)</li> <li>Complete destruction of clusters (this happen frequently in small clusters that have \"border sequences\")</li> <li>Funny destruction of clusters (they split on half creating two \"new ones\" or something like that)</li> </ul> </li> </ol> <p>To run the program <pre><code>$ ./get_seq_jumping_description_from_CDHIT_clstr_info.awk 2020-03-13_cdhit_ide_1.00_cov_0.98.clstr 2020-03-13_cdhit_ide_0.98_cov_0.90.clstr\n</code></pre></p> <p>Example output from the program <pre><code>File1,,,,File2,,,,\nquery_code, query_length, cluster_number_in_file_1, cluster_size_in_file_1, target_code,target_length, cluster_number_in_file_2, cluster_size_in_file_2, cluster_size_diff, same_cluster\n1FJG:A,1522,0,271,1FJG:A,1522,0,378,107,Yes\n1HNW:A,1522,0,271,1HNW:A,1522,0,378,107,Yes\n1HNX:A,1522,0,271,1HNX:A,1522,0,378,107,Yes\n1HNZ:A,1522,0,271,1HNZ:A,1522,0,378,107,Yes\n1HR0:A,1522,0,271,1HR0:A,1522,0,378,107,Yes\n1I94:A,1514,0,271,1I94:A,1514,0,378,107,Yes\n1IBK:A,1522,0,271,1IBK:A,1522,0,378,107,Yes\n1IBL:A,1522,0,271,1IBL:A,1522,0,378,107,Yes\n...\n6UCQ:2a,1521,0,271,6UCQ:2a,1521,0,378,107,Yes\n6UO1:1a,1521,0,271,6UO1:1a,1521,0,378,107,Yes\n6UO1:2a,1521,0,271,6UO1:2a,1521,0,378,107,Yes\n1VY4:AW,76,1,209,1VY4:AW,76,4,213,4,No\n1VY4:AY,76,1,209,1VY4:AY,76,4,213,4,No\n1VY4:CW,76,1,209,1VY4:CW,76,4,213,4,No\n1VY4:CY,76,1,209,1VY4:CY,76,4,213,4,No\n...\n</code></pre></p> <p>For details, look into the code available for download at the bottom of this page. The role of each line is commented for easier understanding.</p> <p>Example files</p> <ul> <li>get_seq_jumping_description_from_CDHIT_clstr_info.awk </li> <li>2020-03-13_cdhit_ide_0.98_cov_0.90.clstr </li> <li>2020-03-13_cdhit_ide_1.00_cov_0.98.clstr</li> </ul>"},{"location":"Case_studies/awk-jmol/","title":"Awk and Jmol","text":"<p>I have a structure that I can easily visualize with Jmol and I want to plot vectors at each atom. Here I am giving an example with Jmol, but the concept is the same for any other program. The command to plot such vectors with Jmol is </p> <pre><code>draw ID vector (atomno=1) {x,y,z}\n</code></pre> <p>For larger molecules this quickly becomes quite a tedious work to type all this commands... so let awk write it for us. The output is printed to the screen and saved in file <code>vectors.spt</code> that will later run in Jmol.</p> <pre><code>$ awk '{i++;printf (\"draw v%i vector (atomno=%i) { %f, %f, %f}\\n\",i,i,$1,$2,$3) }' vectors.dat | tee vectors.spt\n\ndraw v1 vector (atomno=1) {-0.500000,0.700000,0.700000}\ndraw v2 vector (atomno=2) {0.500000,-1.000000,0.900000}\ndraw v3 vector (atomno=3) {0.500000,-0.500000,0.900000}\n</code></pre> <p>where <code>vectors.dat</code> contains our vectors (x,y,z)</p> <p>vectors.dat</p> <pre><code>-0.5   0.7  0.7\n 0.5  -1.0  0.9\n 0.5  -0.5  0.9\n</code></pre> <p>The Jmol script that creates the figure above looks like this (in red is the line that loads the awk generated commands) <pre><code># load the molecule\nload \"1.xyz\"\n\n# save the status, orianation etc.\ncenter {6.285864 6.6071978 6.971991};\n   moveto -1.0 {0 0 1 0} 100.0 0.0 0.0 {6.285864 6.6071978 6.971991} 2.1566827 {0 0 0} 0 0 0 3.0 0.0 0.0;\n  save orientation \"default\";\n  moveto 0.0 { -488 825 -286 58.44} 100.0 0.0 0.0 {6.285864 6.6071978 6.971991} 2.1566827 {0 0 0} 0 0 0 3.0 0.0 0.0;;\n\nwireframe 0.1\nset perspectiveDepth  FALSE\n\n# load the file generated with awk\nscript \"vectors.spt\"\n\nwrite image  200 200 png \"molecule.png\"\n</code></pre></p> <p>Here are few more plots generated the same way.</p> <p> </p> <p>Of course, you can use the same \"trick\" to change colors, sizes, or any other properties by generating instructions with simple awk scripts.</p>"},{"location":"Case_studies/awk_gnuplot/","title":"Awk and Gnuplot","text":""},{"location":"Case_studies/awk_gnuplot/#using-awk-to-fit-birchmurnaghan-equation-of-state-and-show-the-results","title":"Using awk to fit Birch\u2013Murnaghan equation of state and show the results.","text":"<p>I have written this script a long time ago, before Gnuplot had the options to print its own variables on the plot. Nowadays, it is possible to make the fit entirely from Gnuplot, although it will be still tricky to make some decisions if you want to align some labels.</p> <p>Perhaps the most valueable part is the demonstration of simultaneous output/input to external program (Gnuplot in this case) <code>while ((gnu |&amp; getline) &gt; 0)</code> and for future reference.</p> <pre><code>#!/usr/bin/awk -f\n#############################################################################\n#                                                                           #\n# GNU License - Author: Pavlin Mitev                                        #\n# Version 0.3 - Original code from A. Papaconstantopoulos                   #\n# http://cst-www.nrl.navy.mil/bind/static/index.html                        #\n#                                                                           #\n# Fits Birch equation to obtain equilibrion energy, equilibrium lattice     #\n# constant, bulk modulus, and pressure derivative of the bulk modulus.      #\n#                                                                           #\n# REMARK: FCC version; affects the estimation of eq.latt. constant          #\n#                                                                           #\n# Format of the input file:                                                 #\n# Atomic_Volume_in_Angstroms   Energy_per_atom_in_eV                        #\n#                                                                           #\n#############################################################################\nBEGIN {\n  if (ARGC&gt;2) {\n    gnu=\"(gnuplot -persist &gt;&amp; /dev/stdout)\";\n    vfw=ARGV[2];\n    print \"t(vo,v) = (vo/v)**(2./3.) - 1.0;\" |&amp; gnu\n    print \"e(eo,vo,ko,kop,v) = eo + 1.125*ko*vo*t(vo,v)*t(vo,v)* (1.0 + 0.5*(kop-4.0)*t(vo,v));\" |&amp; gnu ;\n    print \"ef=1; vf=\"vfw\"; kf=0.1; kfp=4;\" |&amp; gnu ;\n    print \"fit [\"ARGV[3]\":\"ARGV[4]\"] e(ef,vf,kf,kfp,x) \\\"\"ARGV[1]\"\\\" via ef,vf,kf,kfp;\" |&amp; gnu;\n    print \"print \\\"Results of 3rd order Birch fit:\\\";\" |&amp; gnu ;\n    print \"print \\\"E_0 = \\\",ef,\\\" Ev\\\";\" |&amp; gnu ;\n    print \"print \\\"V_0 = \\\",vf,\\\" Angstrom**3\\\";\" |&amp; gnu ;\n    print \"af = (4.0*vf)**(1./3.);\" |&amp; gnu ;\n    print \"print \\\"a_0 = \\\",af,\\\" Angstrom\\\";\" |&amp; gnu ;\n    print \"kfx = 160.21765*kf;\" |&amp; gnu ;\n    print \"print \\\"B_0 = \\\",kfx,\\\" GPa\\\";\" |&amp; gnu ;\n    print \"print \\\"B_0d pressure derivative= \\\",kfp;\" |&amp; gnu ;\n#    close(gnu,\"to\");\n    while ((gnu |&amp; getline) &gt; 0) {\n      print $0;\n      if ($1==\"E_0\") {\n        print \"set label \\\"E_0= \"$3\" [eV]\\\" at screen  0.5, 0.78 center\" |&amp; gnu;}\n      if ($1==\"V_0\") {\n        vfw= $3;}\n      if ($1==\"a_0\") {\n        print \"set label \\\"a_0= \"$3\" [A]\\\" at screen  0.5, 0.74 center\" |&amp; gnu;}\n      if ($1==\"B_0\") {\n        print \"set label \\\"Bulk Modulus= \"$3\" [GPa]\\\" at screen  0.5, 0.70 center\" |&amp; gnu;}\n      if ($1==\"B_0d\") {\n        print \"set label \\\"B_0 dp= \"$4\"\\\" at screen  0.5, 0.66 center\" |&amp; gnu;\n        print \"set label \\\"\"vfw\"\\\" at  first \"vfw\", ef*0.95 center\" |&amp; gnu;\n print \"set arrow from \"vfw\",ef*0.96 \\\n               to \"vfw\",ef\" |&amp; gnu;\n        print \"plot \\\"\"ARGV[1]\"\\\" title \\\"Original data\\\" w p,\\\n   e(ef,vf,kf,kfp,x) title \\\"Birtch fit\\\"\" |&amp; gnu;\n        close(gnu); exit(0)\n      }\n    }\n  }else {\n    print \"Syntax:\";\n    print \"Bulk  datafile_name  atomic_volume_near_equilibrium [lower_limit [upper_limit]]\"\n    print \"Read the source code for details\";\n    print \"\";\n  }\n}\n</code></pre> <p>Without parameters, it prints the syntax... <pre><code>$ ./Bulk-Modulus-Birch-FCC-Vol-A-eV.awk\nSyntax:\nBulk  datafile_name  atomic_volume_near_equilibrium [lower_limit [upper_limit]]\nRead the source code for details\n</code></pre></p> <p>Here how it works. <pre><code>./Bulk-Modulus-Birch-FCC-Vol-A-eV.awk bulkm.r-e.dat 4\n</code></pre> </p> <p>Files</p> <ul> <li>Bulk-Modulus-Birch-FCC-Vol-A-eV.awk</li> <li>bulkm.r-e.dat</li> </ul>"},{"location":"Case_studies/awk_network/","title":"Awk and networking","text":"<p>Here is just an example that illustrates the awk ability to communicate over common network protocols. Please, read the awk documentation for detailed explanation of the commands.</p> <p>On this Internet address http://130.238.141.28/inet_obs.htm one can find some real-time weather data. The script below extracts the raw values and print them on the screen (or uses the old Linux notification tool to show it on the desktop /commented out in this example/).</p> <p><pre><code>#!/usr/bin/awk -f\nBEGIN{\n  RS=ORS=\"\\r\\n\"\n  http=\"/inet/tcp/0/130.238.141.28/80\"\n  print \"GET http://130.238.141.28/inet_obs.htm\"  |&amp; http\n\n  while ((http |&amp; getline) &gt; 0) {\n    if (match($0,\"Temperatur:\"))    ss=   sprintf(\"         Temp :%6g\u00b0C \\n\",$2)\n    if (match($0,\"Luftfuktighet:\")) ss=ss sprintf(\"Rel. humidity :%6g%  \\n\",$2)\n    if (match($0,\"Vindhastighet:\")) ss=ss sprintf(\"   Wind speed :%6gm/s\\n\",$2)\n    if (match($0,\"Lufttryck:\"))     ss=ss sprintf(\"Atm. pressure :%6ghPa\\n\",$2)\n  }\n\n  close(http)\n  #cmd=\"echo -e '\"ss\"' | \"osd\" &amp;\"; system(cmd); close(cmd);\n  print ss\n}\n</code></pre> <pre><code>         Temp :  11.1\u00b0C\nRel. humidity :    21%\n   Wind speed :   1.3m/s\nAtm. pressure :1013.9hPa\n</code></pre> </p> <p>How about simple web server? http://rosettacode.org/wiki/Hello_world/Web_server#AWK</p>"},{"location":"Case_studies/awk_writes_python/","title":"Awk writes Python","text":"<p>This sounds strange. Why would one use awk to write Python code? Well, the reason is the same as before, \"the overheads of more sophisticated approaches are not worth the bother\". My analysis program runs in Python but collecting my data from the output of another program (GULP in this case) requires some tedious work. The result is a small awk script that extracts the relevant values and writes a Python structure that takes a couple of lines to load. The code looks large and complicated, but if you just look closely, each search section is independent from the rest and addresses a single property of interest. With time, I kept adding more and more patterns to make the script more robust and generic. </p> <p>If you are still not convinced, just look how the GULP output looks like. In the file section, as usual, there is a complete set as an example.</p> <p>GULP-out2python.awk</p> <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  print \"import numpy as np\\n\\nclass GULP:\\n\\tdef __init__(self):\\n\\t\\tself.ver=\\\"2015.04.15\\\"\"\n}\n\n/Dimensionality =/{\n  if ($3==3) print \"GULP.pbc= np.array([ True, True, True], dtype=bool)\"\n  if ($3==2) print \"GULP.pbc= np.array([ True, True, False], dtype=bool)\"\n  if ($3==1) print \"GULP.pbc= np.array([ True, False, False], dtype=bool)\"\n  if ($3==0) print \"GULP.pbc= np.array([ False, False, False], dtype=bool)\"\n}\n\n\n/Cartesian lattice vectors \\(Angstroms\\) :/{\n  getline;\n  getline; printf \"GULP.cell= np.array([\\n\\t[%g,%g,%g],\\n\",$1,$2,$3\n  getline; printf \"\\t[%g,%g,%g],\\n\",$1,$2,$3\n  getline; printf \"\\t[%g,%g,%g]])\\n\",$1,$2,$3\n}\n\n/Fractional coordinates of asymmetric unit/ {\n  for(i=1;i&lt;=6;i++) getline\n  while (NF&gt;1){\n    scoord[$1]=\"[\"$4\",\"$5\",\"$6\"]\";\n    charge[$1]=$7;\n    if ($2 == \"X1\") $2=\"X\"\n    symbol[$1]=$2;\n    natoms=$1;\n    getline;\n  }\n  print  \"GULP.scaled_positions= np.array([\";  for (i=1;i&lt;=natoms-1;i++){ print \"\\t\"scoord[i]\",\" }; print \"\\t\"scoord[i]\"])\"\n  printf \"GULP.charges=   np.array([\";  for (i=1;i&lt;=natoms-1;i++){ print \"\\t\"charge[i]\",\" };print \"\\t\",charge[natoms]\"])\";\n  printf \"GULP.symbols=   np.array([\";  for (i=1;i&lt;=natoms-1;i++){ printf \"'\"symbol[i]\"',\" };print \"'\"symbol[natoms]\"'])\";\n}\n\n/Initial Cartesian coordinates :/{\n  for(i=1;i&lt;=6;i++) getline\n  while (NF&gt;1){\n    coord[$1]=\"[\"$4\",\"$5\",\"$6\"]\"\n    charge[$1]=$7 \n    symbol[$1]=$2\n    natoms=$1\n    getline;\n  }\n  print  \"GULP.positions= np.array([\";  for (i=1;i&lt;=natoms-1;i++){ print \"\\t\"coord[i]\",\" }; print \"\\t\"coord[i]\"])\" \n}\n\n/Electrostatic potential at atomic positions :/{\n  for(i=1;i&lt;=6;i++) getline\n   while (NF&gt;1){\n      gsub(\"-\",\" -\",$0); split($0,data)\n      aPOT[$1]=data[4]+0\n      aEF[$1]=\"[\"data[5]+0\",\"data[6]+0\",\"data[7]+0\"]\"\n      getline;\n   }\n   printf \"GULP.EPOT= np.array([\";  for (i=1;i&lt;=natoms-1;i++){ print \"\\t\"aPOT[i]\",\" }; print \"\\t\"aPOT[natoms]\"])\";\n   print  \"GULP.EPOTgradient= np.array([\";  for (i=1;i&lt;=natoms-1;i++){ print \"\\t\"aEF[i]\",\"  }; print \"\\t\"aEF[i]\"])\"\n}\n</code></pre> <p>The output from the script looks like this \"data.py\" <pre><code>import numpy as np\n\nclass GULP:\n def __init__(self):\n self.ver=\"2014.01.28\"\nGULP.pbc= np.array([ True, True, True], dtype=bool)\nGULP.cell= np.array([\n [8.93076,0,0],\n [0,5.95384,0],\n [0,0,23.42]])\nGULP.scaled_positions= np.array([\n [0.578633,0.302123,0.218589],\n [0.762789,0.553684,0.312622],\n [0.078391,0.553354,0.313708],\n [0.242805,0.801983,0.220219],\n [0.078509,0.053341,0.313727],\n [0.578346,0.802044,0.218543],\n [0.764646,0.052150,0.315978],\n...\n</code></pre></p> <p>and here is the code that loads the data in my Python program</p> <pre><code># Load the data from GULP\n#==========================================\nfrom data import GULP\n\nstructure= Atoms(symbols=GULP.symbols,\n  cell=GULP.cell, \n  #positions= GULP.positions,\n  scaled_positions= GULP.scaled_positions,\n  pbc= GULP.pbc)\nstructure.charges= GULP.charges;\nstructure.EPOTgradient= GULP.EPOTgradient;\n#==========================================\n</code></pre> <p>Files</p> <ul> <li>GULP-out2python.awk</li> <li>data.py</li> <li>EF.gout - GULP program output</li> </ul>"},{"location":"Case_studies/colors/","title":"Color output with custom keywords","text":"<p>There are few tools to highlight different language syntax with colors and they are excellent tools for what they are designed.  Some time I would like to use my own keywords for highlighting and this always turns to be a difficult task.</p> <p>Here is a small script that highlights provided keywords (or simple regular expressions) on the fly. For convenience, I made it possible to choose between some predefined color schemes.</p> freq-dvr.out <pre><code>3300.42   H2O_H-bonded   DVR/snapshot-000100-281-59.xyz_H281-O279.dvr\n3436.45   H2O_H-bonded   DVR/snapshot-000150-280-59.xyz_H280-O279.dvr\n3382.03   H2O_H-bonded   DVR/snapshot-000200-280-59.xyz_H280-O279.dvr\n3360.05   H2O_H-bonded   DVR/snapshot-000350-280-59.xyz_H280-O279.dvr\n3709.4   NOT_H-bonded   DVR/snapshot-002850-181.xyz_H181-O180.dvr\n3647.54   NOT_H-bonded   DVR/snapshot-002900-181.xyz_H181-O180.dvr\n3683.93   NOT_H-bonded   DVR/snapshot-003100-191-181-199.xyz_H181-O180.dvr\n3703.93   NOT_H-bonded   DVR/snapshot-004100-224.xyz_H224-O222.dvr\n3670.12   NOT_H-bonded   DVR/snapshot-004150-224.xyz_H224-O222.dvr\n3654.3   NOT_H-bonded   DVR/snapshot-004200-224.xyz_H224-O222.dvr\n3710.58   NOT_H-bonded   DVR/snapshot-004250-224.xyz_H224-O222.dvr\n3643.06   CO2_H-bonded   DVR/snapshot-004350-134-199.xyz_H134-O132.dvr\n3535.59   H2O_H-bonded   DVR/snapshot-004500-133.xyz_H133-O132.dvr\n3668.44   NOT_H-bonded   DVR/snapshot-004600-134-199.xyz_H134-O132.dvr\n3638.55   NOT_H-bonded   DVR/snapshot-004650-118-134-199.xyz_H134-O132.dvr\n3576.47   H2O_H-bonded   DVR/snapshot-004750-233-133.xyz_H133-O132.dvr\n3735.93   NOT_H-bonded   DVR/snapshot-004750-233-133.xyz_H233-O231.dvr\n3625.43   H2O_H-bonded   DVR/snapshot-004800-233-133-118.xyz_H133-O132.dvr\n3740.8   NOT_H-bonded   DVR/snapshot-004800-233-133-118.xyz_H233-O231.dvr\n3673.06   H2O_H-bonded   DVR/snapshot-004850-233-199.xyz_H233-O231.dvr\n3723.15   NOT_H-bonded   DVR/snapshot-004900-233-199.xyz_H233-O231.dvr\n3703.71   NOT_H-bonded   DVR/snapshot-004950-200-233.xyz_H233-O231.dvr\n3682.4   CO2_H-bonded   DVR/snapshot-005000-200-233.xyz_H233-O231.dvr\n3693.42   NOT_H-bonded   DVR/snapshot-005050-233.xyz_H233-O231.dvr\n3551.17   H2O_H-bonded   DVR/snapshot-005950-233.xyz_H233-O231.dvr\n3704.16   NOT_H-bonded   DVR/snapshot-006350-200-62.xyz_H062-O060.dvr\n3732.31   NOT_H-bonded   DVR/snapshot-006400-200-62.xyz_H062-O060.dvr\n3705.08   NOT_H-bonded   DVR/snapshot-006450-62.xyz_H062-O060.dvr\n3724.14   NOT_H-bonded   DVR/snapshot-006500-62.xyz_H062-O060.dvr\n3658.92   NOT_H-bonded   DVR/snapshot-006550-62.xyz_H062-O060.dvr\n3700.88   H2O_H-bonded   DVR/snapshot-006600-62.xyz_H062-O060.dvr\n3707.23   H2O_H-bonded   DVR/snapshot-006650-62.xyz_H062-O060.dvr\n3645.98   H2O_H-bonded   DVR/snapshot-006700-200-62.xyz_H062-O060.dvr\n3709.66   NOT_H-bonded   DVR/snapshot-006750-200-62.xyz_H062-O060.dvr\n3681.21   NOT_H-bonded   DVR/snapshot-006800-200-62.xyz_H062-O060.dvr\n3682.88   NOT_H-bonded   DVR/snapshot-006850-200-62.xyz_H062-O060.dvr\n3704.9   H2O_H-bonded   DVR/snapshot-006900-200-62.xyz_H062-O060.dvr\n3600.96   NOT_H-bonded   DVR/snapshot-007250-272.xyz_H272-O270.dvr\n3663.54   NOT_H-bonded   DVR/snapshot-007350-62.xyz_H062-O060.dvr\n3710.52   NOT_H-bonded   DVR/snapshot-007400-200-62.xyz_H062-O060.dvr\n3708.11   NOT_H-bonded   DVR/snapshot-007450-62.xyz_H062-O060.dvr\n3717.28   NOT_H-bonded   DVR/snapshot-007500-62.xyz_H062-O060.dvr\n3751   NOT_H-bonded   DVR/snapshot-007550-62.xyz_H062-O060.dvr\n3722   NOT_H-bonded   DVR/snapshot-007900-223.xyz_H223-O222.dvr\n3703.76   CO2_H-bonded   DVR/snapshot-007950-223.xyz_H223-O222.dvr\n3805.29   CO2_H-bonded   DVR/snapshot-008000-223.xyz_H223-O222.dvr\n3651.37   CO2_H-bonded   DVR/snapshot-008050-223.xyz_H223-O222.dvr\n3766.86   CO2_H-bonded   DVR/snapshot-008100-223.xyz_H223-O222.dvr\n3698.48   CO2_H-bonded   DVR/snapshot-008150-223.xyz_H223-O222.dvr\n3692.41   CO2_H-bonded   DVR/snapshot-008200-223.xyz_H223-O222.dvr\n</code></pre> <pre><code>$ cat freq-dvr.out | color.awk 2H2O_H-bonded 1NOT_H-bonded 3CO2_H-bonded\n</code></pre> <p></p> color.awk<pre><code>#!/usr/bin/awk -f\nBEGIN{\n  nARGC= ARGC; ARGC=1 # Trick the command line to ignore the files and use them as options\n\n  c[0]= \"\\033[97;41m\" # white-red\n  c[1]= \"\\033[31;1m\"  # red\n  c[2]= \"\\033[32;1m\"  # green\n  c[3]= \"\\033[33;1m\"  # yellow\n  c[4]= \"\\033[34;1m\"  # blue\n  c[5]= \"\\033[35;1m\"  # magenta\n  c[6]= \"\\033[36;1m\"  # cyan\n  c[7]= \"\\033[93;41m\" # yellow-red\n  cn=   \"\\033[0m\"     # reset\n}\n\n{\n  for (i=1; i&lt; nARGC; i++ ){\n    ci= ARGV[i]+0 # color index\n    m= ARGV[i]    # match string\n    sub(ci,\"\",m)  # remove color index\n    gsub(m, c[ci]\"&amp;\"cn)  # insert color codes\n  }\n  print $0\n}\n</code></pre> <p>The color scheme is selected by number before the matching string, which limits a bit the functionality, but it keeps the script simple...</p> <p>Here is an alternative script that has some general keywords and matching criteria predefined. It is straight forward to add your own patterns or remove the unnecessary ones.</p> <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  c[0]= \"\\033[97;41m\" # white-red\n  c[1]= \"\\033[31;1m\"  # red\n  c[2]= \"\\033[32;1m\"  # green\n  c[3]= \"\\033[33;1m\"  # yellow\n  c[4]= \"\\033[34;1m\"  # blue\n  c[5]= \"\\033[35;1m\"  # magenta\n  c[6]= \"\\033[36;1m\"  # cyan\n  c[7]= \"\\033[93;41m\" # yellow-red\n  cn=   \"\\033[0m\"     # reset\n}\n\n{\n  gsub(/WARNING|Warning|warning/,                         c[2]\"&amp;\"cn)  # Warning\n  gsub(/ERROR|Error|error/,                               c[0]\"&amp;\"cn)  # Error\n  gsub(/FAIL|Fail|fail|FAILED|Failed|failed/,             c[0]\"&amp;\"cn)  # Failed\n  gsub(/([0-9]{1,3}\\.){3}[0-9]{1,3}/,                     c[2]\"&amp;\"cn)  # IP4 address\n  gsub(/([0-9a-fA-F]{2}[:-]){5}[0-9a-fA-F]{2}/,           c[4]\"&amp;\"cn)  # MAC\n  gsub(/[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}/, c[6]\"&amp;\"cn)  # e-mail\n  gsub(/https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}([-a-zA-Z0-9()@:%_\\+.~#?&amp;//=]*)/, c[4]\"&amp;\"cn)  # web address\n\n  $0= gensub(/(^|[[ ]){1}(OK|Ok)([] ]|$){1}/,           \"\\\\1\"c[2]\"\\\\2\"cn\"\\\\3\",  \"g\") # OK - special case\n\n  gsub(/[&amp;@][A-Za-z_]+/,                                  c[3]\"&amp;\"cn)  # @&amp; \n  gsub(/^#SBATCH.*$/,                                     c[6]\"&amp;\"cn)  # SBATCH\n  gsub(/^#.*$/,                                           c[4]\"&amp;\"cn)  # comment\n\n  print $0\n}\n</code></pre>"},{"location":"Case_studies/manipulating_vcf/","title":"Manipulating the output from a genome analysis - vcf and gff","text":"<p>Problem formulated and presented at the workshop by Jonas S\u00f6derberg, Department of Cell and Molecular Biology, Molecular Evolution</p>"},{"location":"Case_studies/manipulating_vcf/#task-at-hand","title":"Task at hand","text":"<p>We have a bunch of files that describes a standard fly, the genes of the fly and also where our new non-standard flies differ from the standard fly (the reference). These differences are called variants and are divided ito three categories; SNPs (Singular Nucleotide Polymorphisms), Insertions (Some DNA is inserted in our variant in comparison to the reference) and Deletions (Some DNA is deleted from our variant in comparison to the reference). We want to find out where and in what way the new flies have variants, if those variants are inside genes or not and finally what the genes (in which those variants are located) actually do.</p>"},{"location":"Case_studies/manipulating_vcf/#getting-the-raw-files","title":"Getting the RAW files","text":"<p>We have a comparison between a number of different new fly cell lines. These are found in a huge vcf file (dgrp2.vcf). We also have the annotations in a gff3 file (Drosophila_melanogaster.BDGP6.28.101.chr.gff3) and the genome itself in a fasta file.</p>"},{"location":"Case_studies/manipulating_vcf/#the-annotation-of-the-fly-genome","title":"The annotation of the fly genome:","text":"<p>Find the GFF3 here.</p>"},{"location":"Case_studies/manipulating_vcf/#the-genome-sequence-used-in-this-project","title":"The genome sequence used in this project:","text":"<p>The genome is located here.</p>"},{"location":"Case_studies/manipulating_vcf/#the-comparison-of-all-the-variants-in-the-cell-lines-in-the-freeze-project","title":"The comparison of all the variants in the cell lines in the freeze project","text":"<p>Here is the VCF file with the variants. (That page seems to be lost, so here is a very similar, although not identical file)</p>"},{"location":"Case_studies/manipulating_vcf/#common-names-and-functions","title":"Common names and functions","text":"<p>Finally, full gene names and functions found in this file.</p>"},{"location":"Case_studies/manipulating_vcf/#tips-before-starting","title":"Tips before starting","text":"<ul> <li>For readability of the vcf file:</li> </ul> <p><code>awk '{print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6\"\\t\"$7\"\\t\"$8\"\\t\"$9}' dgrp2.vcf &gt; dgrp2_trimmed.vcf &amp;</code></p> <ul> <li>If your awk takes too long to run (10h or so) - Make sure you have the latest awk, and also you might want to think about parallelisation.</li> <li>My solutions also work for files with more than one chromosome, so in some cases they are longer than needed for your exercise. I left it so on purpose.</li> <li>SNPs and INDELs are collectively named variants</li> <li> <p>Genes and CoDing Sequences (CDSs) are sometimes just collectively called genes</p> </li> <li> <p>For shorter run times, extract chromosome 4 and look only at that.</p> </li> </ul>"},{"location":"Case_studies/manipulating_vcf/#making-awking-the-data-easier","title":"Making awking the data easier","text":"<p>Start by splitting the task into sub-tasks. This makes it easier to see what happens and you might get interesting intermediary results. </p>"},{"location":"Case_studies/manipulating_vcf/#bonus-result","title":"bonus result","text":"<p>A table with counted and sorted different genomic features in chromosome 4.</p>"},{"location":"Case_studies/manipulating_vcf/#bonus-result_1","title":"bonus result","text":"<p>SNPs sorted by number. Just like the coins on day one.</p>"},{"location":"Case_studies/manipulating_vcf/#the-exercise","title":"The exercise","text":"<p>Identify the steps you need and use awk to do those. Open the hints if you get stuck.</p> <p> </p>"},{"location":"Case_studies/manipulating_vcf/#hints-ordered-by-subject-dont-use-them-unless-necessary-they-are-not-good-code-examples","title":"Hints, ordered by subject. Don't use them unless necessary. They are not good code examples.","text":""},{"location":"Case_studies/manipulating_vcf/#overall-an-example-of-things-to-look-for","title":"Overall, an example of things to look for","text":"Hint Example <p>Let's say we want to find out all genes that contains a variant and all variants that are located within a gene. What do we want to do first? Take a look at the vcf file. That is the one that contains all the variants. Then look at the gff file, which contains the genes and other annotations. Finally, take a look at the DNA sequence. You will need to combine all three to answer the question. </p> Hint Example, what do we need to get? <ul> <li>Positions for SNPs and INDELs</li> <li>Positions for genes and CDSs</li> <li>Separation of variants (SNPs and INDELs) into two groups, inside and outside genes (and CDSs)</li> <li>Separation of genes/CDSs into those with and without variants (and maybe how many there are per gene)</li> </ul>"},{"location":"Case_studies/manipulating_vcf/#chromosome-4","title":"Chromosome 4","text":"Hint What do we need? <p>Extract chr4 from the vcf and the gff and make new files</p> Hint <p>All lines from chromosome 4 start with a 4</p> Solution <p><code>awk '/^4/{print $0}' Drosophila_melanogaster.BDGP6.28.101.gff3 &gt; Drosophila_melanogaster.chr4.gff3</code></p> <p><code>awk '/^4/{print $0}' dgrp2_trimmed.vcf &gt; dgrp2_chr4.vcf</code></p> bonus result example A table with counted and sorted different genomic features in chromosome 4. <pre><code>   1 chromosome\n   1 snoRNA\n   2 pre_miRNA\n   7 pseudogene\n  11 pseudogenic_transcript\n  26 ncRNA_gene\n  31 ncRNA\n  79 gene\n 295 mRNA\n 338 three_prime_UTR\n 571 five_prime_UTR\n2740 CDS\n3155 exon\n</code></pre>"},{"location":"Case_studies/manipulating_vcf/#variants","title":"Variants","text":"Hint Which output do we want? <p>Get distribution of variants and list them in two separate files. For a bonus plot of the lengths of the INDELS, get the length of all INDELS into a third file</p> Hint <p>Remove lines beginning with # (grep)</p> Hint <p>If columns 4 and 5 have different length, it's an INDEL. Otherwise it's a SNP.</p> Hint <p>You want the output to be a file with columns 1, 2, 4 and 5, a classifier (SNP or INDEL) and finally the length of the INDEL (put \"-\" for the SNPs)</p> Solution <p><code>cat dgrp2_chr4.vcf | grep -v \"#\" | awk '{if (length($4)&gt;1||length($5)&gt;1){a=\"INDEL\";b=length($4)-length($5);cnt[b]+=1;} else {a=\"SNP\";b=\"-\";} printf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\", $1, $2, a, b, $4, $5) &gt; \"indels_Drosophila_chr4\";}END{for (x in cnt){print x,cnt[x] &gt; \"distr_Drosophila_chr4\"}}'</code>  or  <code>cat dgrp2_chr4.vcf | grep -v \"#\" | ./indels.awk</code></p> indels.awk <pre><code>#!/usr/bin/awk -f\n{\n    if (length($4)&gt;1||length($5)&gt;1){\n        a=\"INDEL\";\n        b=length($4)-length($5);\n        cnt[b]+=1;\n    }\n    else{\n        a=\"SNP\";\n        b=\"-\";\n    }\nprintf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\", $1, $2, a, b, $4, $5) &gt; \"indels_Drosophila_chr4\";\n}\n\nEND{\n    for (x in cnt){\n        print x,cnt[x] &gt; \"distr_Drosophila_chr4\"\n    }\n}\n</code></pre> Solution proposed by Lo\u00efs Rancilhac - 2022.08.30 <p><code>awk '/_SNP/ {SNP++; print $0 &gt; \"chr4_SNPs.vcf\"} /_DEL/ {DEL++; print $0 &gt; \"chr4_DEL.vcf\"; LENGTH=length($4)-length($5); print LENGTH &gt; \"Deletions_lengths.txt\"} /_INS/ {INS++; print $0 &gt; \"chr4_INS.vcf\"; LENGTH=length($5)-length($4); print LENGTH &gt; \"Insertions_lengths.txt\"} END{print \"SNPs: \"SNP\"\\nInsertions: \"INS\"\\nDeletions: \"DEL}' chr4.vcf</code></p> bonus result example SNPs sorted by number <pre><code>1182 C-&gt;T  \n1133 G-&gt;A  \n 932 A-&gt;G  \n 929 A-&gt;T  \n 892 T-&gt;A  \n 880 T-&gt;C  \n 639 G-&gt;T  \n 621 C-&gt;A  \n 436 A-&gt;C  \n 396 T-&gt;G  \n 372 G-&gt;C  \n 357 C-&gt;G\n</code></pre>"},{"location":"Case_studies/manipulating_vcf/#genes-with-variants","title":"Genes with variants","text":"Hint How do we get those? <p>Compare back and separate the annotation into features that do and don\u2019t have variants. For a bonus, also record the number of variants in each feature</p> Hint <p>Make an index using the previous output to identify positions of variants</p> Hint <p>For each feature in the gff, check all position it covers to see if they are in your index, if so print to one file. If not, print to another.</p> Solution <p><code>awk 'FNR==NR{a[$1,$2]=\"T\"; next}{ hits=0; for(N=$4; N&lt;=$5; N++) { if (a[$1,N] == \"T\") {hits+=1}} if (hits&gt;0) {print hits \"\\t\" $0 &gt; \"haveSNPINDEL_Drosophila_chr4.gff\"} else {print $0 &gt; \"noSNPINDEL_Drosophila_chr4.gff\"}}' indels_Drosophila_chr4 Drosophila_melanogaster.chr4.gff3</code>  or  <code>./genes_var.awk indels_Drosophila_chr4 Drosophila_melanogaster.chr4.gff3</code></p> genes_var.awk <pre><code>#!/usr/bin/awk -f\n\nFNR==NR{\n    a[$1,$2]=\"T\";\n    next\n}\n{\n    hits=0;\n    for(N=$4; N&lt;=$5; N++) {\n        if (a[$1,N] == \"T\") {\n         hits+=1\n        }\n    }\n    if (hits&gt;0) {\n        print hits \"\\t\" $0 &gt; \"haveSNPINDEL_Drosophila_chr4.gff\"\n    }\n    else {\n        print $0 &gt; \"noSNPINDEL_Drosophila_chr4.gff\"\n    }\n}\n</code></pre>"},{"location":"Case_studies/manipulating_vcf/#genescdss-only","title":"Genes/CDSs only","text":"Hint What features do we look for? <p>Filter for genes and CDSs before doing the analysis.</p> Hint <p>Only genes and CDSs are interesting to us. Make a gff without the rest of the features.</p> Solution <p><code>awk '$3 ~ /gene|CDS/' Drosophila_melanogaster.chr4.gff3 &gt; Drosophila_melanogaster.chr4_genesCDSs.gff3</code></p>"},{"location":"Case_studies/manipulating_vcf/#list-of-variants","title":"List of variants","text":"Hint How do we classify the variants? <p>Repeat step 3 for the SNPs/INDELs themselves, to see which are actually located inside genes</p> Hint <p>Make an index of all genes/CDSs (from your gff), where start and stop are paired</p> Hint <p>For each feature from your step 2 file, check the position against the index and print whether or not the variant is inside a gene.</p> Solution <p><code>awk 'FNR==NR{ingene[$1,$4]=$5; next}{state=\"Not in gene\";for (pair in ingene) {split(pair, t, SUBSEP); if ($1==t[1] &amp;&amp; $2&gt;=t[2] &amp;&amp; $2&lt;=ingene[t[1],t[2]]) {state=(t[1] \" \" t[2] \" \" ingene[t[1],t[2]])}} print $0, \" \", state }' Drosophila_melanogaster.chr4_genesCDSs.gff3 indels_Drosophila_chr4 &gt; SNPsInGenes_Drosophila_ch4</code>  or  <code>./varlist.awk Drosophila_melanogaster.chr4_genesCDSs.gff3 indels_Drosophila_chr4 &gt; SNPsInGenes_Drosophila_ch4</code></p> varlist.awk <pre><code>#!/usr/bin/awk -f\n\nFNR==NR{\n    ingene[$1,$4]=$5; \n    next\n}\n{\n    state=\"Not in gene\";\n    for (pair in ingene) {\n        split(pair, t, SUBSEP); \n        if ($1==t[1] &amp;&amp; $2&gt;=t[2] &amp;&amp; $2&lt;=ingene[t[1],t[2]]) {\n            state=(t[1] \" \" t[2] \" \" ingene[t[1],t[2]])\n        }\n    } \n    print $0, \" \", state \n}\n</code></pre> Hint Finding the names <p>Find a common field</p> Hint Which field? <p>gene ID</p> Hint Where is that? <p>column nine, not awk!</p> Solution <p><code>awk 'FNR==1{++fileidx} fileidx==1{split($9,a,\";|:\");ingene[$1,$4,$5]=a[2]} fileidx==2{FS=\"\\t\";name[$3]=$5} fileidx==3{state=\"Not in gene\";for (trip in ingene) {split(trip, t, SUBSEP); if ($1==t[1] &amp;&amp; $2&gt;=t[2] &amp;&amp; $2&lt;=t[3]) {state=(t[1] \"\\t\" t[2] \"\\t\" t[3] \"\\t\" name[ingene[t[1],t[2],t[3]]])}} print $0, \"\\t\", state }' Drosophila_melanogaster.chr4_genesCDSs.gff3 fbgn_fbtr_fbpp_expanded_fb_2020_06.tsv indels_Drosophila_chr4 &gt; SNPsInNamedGenes_Drosophila_ch4</code>  or  <code>./in_named.awk Drosophila_melanogaster.chr4_genesCDSs.gff3 fbgn_fbtr_fbpp_expanded_fb_2020_06.tsv indels_Drosophila_chr4 &gt; SNPsInNamedGenes_Drosophila_ch4</code></p> in_named.awk <pre><code>#!/usr/bin/awk -f\nFNR==1{\n    ++fileidx\n}\n{\n    if (fileidx==1){\n        split($9,a,\";|:\");\n        ingene[$1,$4,$5]=a[2]\n    } \n    if (fileidx==2){\n        FS=\"\\t\";\n        name[$3]=$5\n    } \n    if (fileidx==3){\n        state=\"Not in gene\";\n        for (trip in ingene) {\n            split(trip, t, SUBSEP); \n            if ($1==t[1] &amp;&amp; $2&gt;=t[2] &amp;&amp; $2&lt;=t[3]) {\n                state=(t[1] \"\\t\" t[2] \"\\t\" t[3] \"\\t\" name[ingene[t[1],t[2],t[3]]])\n            }\n        } \n        print $0, \"\\t\", state \n    }\n}\n</code></pre> <p>Look at the distribution of genes:</p> <p><code>awk -F\"\\t\" '{print $10}' SNPsInNamedGenes_Drosophila_ch4 | sort | uniq -c | sort -n</code></p>"},{"location":"Case_studies/multiple_files_I/","title":"Multiple input files - first approach","text":"<p>The script will collect the data in sequence and store the data in memory, then print the arranged data at the end.</p> <p>Here is an example problem that is easy to solve with awk. </p> <p>Let's assume that we have collected some data about some persons. It is not systematic, so the data files are not complete and rows are not in the same order... </p> <p>file: 1.dat</p> <pre><code>Daniel    10\nAnders     7\nSven      56\nAli       17\nPeter      6\n</code></pre> <p>file: 2.dat</p> <pre><code>Peter     Monday\nSven      Sunday\nDavid     Tuesday\n</code></pre> <p>Let's put them together! If the files had all names with missing data marked - sorting the files and pasting them together will essentially be enough. </p> <p>Below, it is just one possible way to do it. First we need to have a list of all names, collect the data, then try to print whatever we have collected.</p> <p>Warning</p> <p>awk under OS X is not fully compatible with Gawk (GNU awk) and the internal variable <code>ARGIND</code> is not available - the script will not work as intended.</p> <p>script.awk</p> <pre><code>#!/usr/bin/awk -f\n\n{ \n  data[$1][ARGIND]= $2\n}\n\nEND {\n  for (i in data) print i\"\\t\"data[i][1]\"\\t\"data[i][2]\n}\n</code></pre> <p>Run the script like this:</p> <pre><code>./script.awk 1.dat 2.dat\n</code></pre> <p>The script runs over the two files in a row and on each line it uses associative arrays to collect the names from the first column in <code>data[$1][ARGIND]</code>. <code>data[$1][ARGIND]</code> is two dimensional array with indexes [name][ number of current file/argument]. At the end we will have elements like this:</p> <pre><code>...\ndata[\"Sven\"][1] = 56\ndata[\"Sven\"][2] = \"Sunday\"\n...\n</code></pre> <p>The <code>END</code> section runs over the collected names and prints the collected data - so we recover as much as possible from both files.</p> <p>Here is the output. <pre><code>Anders          7\nDaniel          10\nAli             17\nSven            56              Sunday\nDavid                           Tuesday\nPeter           6               Monday\n</code></pre></p> <p>Alternative, perhaps better solution in this case, might the specially written tool for this purpose i.e.</p> <pre><code>$ join -a1 -a2  -j 1 -o 0,1.2,2.2 -e \"NULL\"   &lt;(sort 1.dat)  &lt;(sort 2.dat)\nAli 17 NULL\nAnders 7 NULL\nDaniel 10 NULL\nDavid NULL Tuesday\nPeter 6 Monday\nSven 56 Sunday\n</code></pre> <p>Note</p> <p>Note that the files need to be sorted by the field they will be joined, since the program is trying to avoid loading the whole data in the memory. If the data is sorted, awk also can join the data without loading it into the memory. http://unix.stackexchange.com/questions/194968/why-isnt-this-awk-command-doing-a-full-outer-join Credits to Mahesh Panchal for the tip</p>"},{"location":"Case_studies/multiple_files_I/#exercise","title":"Exercise","text":"<p>Copy/paste the text in to two files with the suggested names</p> scientific<pre><code>2       |       Bacteria        |       Bacteria &lt;bacteria&gt;     |       scientific name |\n29      |       Myxococcales    |               |       scientific name |\n139     |       Borreliella burgdorferi |               |       scientific name |\n161     |       Treponema pallidum subsp. pallidum      |               |       scientific name |\n168     |       Treponema pallidum subsp. pertenue      |               |       scientific name |\n356     |       Rhizobiales     |               |       scientific name |\n638     |       Arsenophonus nasoniae   |               |       scientific name |\n</code></pre> genbank<pre><code>2       |       eubacteria      |               |       genbank common name     |\n29      |       fruiting gliding bacteria       |               |       genbank common name     |\n139     |       Lyme disease spirochete |               |       genbank common name     |\n161     |       syphilis treponeme      |               |       genbank common name     |\n168     |       yaws treponeme  |               |       genbank common name     |\n356     |       rhizobacteria   |               |       genbank common name     |\n638     |       son-killer infecting Nasonia vitripennis        |               |       genbank common name     |\n</code></pre> <p>Can you join the information from both files to collect the data in better format?</p> <p><code>ID | scientific name | genbank common name</code></p> <pre><code>2 | Bacteria | eubacteria\n29 | Myxococcales | fruiting gliding bacteria\n139 | Borreliella burgdorferi | Lyme disease spirochete\n</code></pre> <p>Leave the extra blanks for the first attempt. We will use this problem (cleaning the remaining spaces before and after) to introduce user defined functions.</p> <p>Hint</p> <p>Using <code>FILENAME</code> might come handy.</p> Possible solution <pre><code>#!/usr/bin/awk -f\nBEGIN{ FS=\"|\" }\n\n{\n  data[$1][FILENAME]= $2\n}\n\nEND {\n  for (i in data) print trim(i)\"|\"trim(data[i][\"scientific\"])\"|\"trim(data[i][\"genbank\"])\n}\n\nfunction trim (x) {\n  sub(/^[ \\t]*/,\"\",x);\n  sub(/[ \\t]*$/,\"\",x);\n  return x\n}\n</code></pre> Solution usung join suggested by Amrei Binzer-Panchal, 2021.01.18 <pre><code>$ join -a1 -a2  -j 1 -o 0,1.2,2.2 -e \"NULL\" -t \"|\"  &lt;(sort scientific)  &lt;(sort genbank)\n\n139     |       Borreliella burgdorferi |       Lyme disease spirochete \n161     |       Treponema pallidum subsp. pallidum      |       syphilis treponeme      \n168     |       Treponema pallidum subsp. pertenue      |       yaws treponeme  \n2       |       Bacteria        |       eubacteria      \n29      |       Myxococcales    |       fruiting gliding bacteria       \n356     |       Rhizobiales     |       rhizobacteria   \n638     |       Arsenophonus nasoniae   |       son-killer infecting Nasonia vitripennis \n</code></pre>"},{"location":"Case_studies/multiple_files_II/","title":"Multiple input files - second approach","text":"<p>Let's have the same input data as in the previous case, but this time, for each line in the first file we will read the second file and look for a match. For large files, this will be significantly slower than the first case since we will read multiple times the second file, but this approach reduces the memory requirement, since at no point we need to load both files into memory like in the first case. In fact this approach needs to keep only one line per file in the memory at he same time.</p> <p>file: 1.dat</p> <pre><code>Daniel    10\nAnders     7\nSven      56\nAli       17\nPeter      6\n</code></pre> <p>file: 2.dat</p> <pre><code>Peter     Monday\nSven      Sunday\nDavid     Tuesday\n</code></pre> <p>Let's start with simple working example, that we will improve later.</p> <p>script1.awk</p> <pre><code>#!/usr/bin/awk -f\n\nNR == FNR {\n  printf(\"%s  \",$0)\n  while(getline line &lt; ARGV[2]){\n    split(line,data)\n    if(data[1]==$1) printf(\"%s  \",data[2])\n  }\n  close(ARGV[2])\n  print \"\"\n}\n</code></pre> <p>Awk will read both files in order. While <code>FNR</code> will count the line number with respect to the current file, <code>NR</code> will count the concatenated number lines i.e. <code>NR == FNR</code> ensures that awk will run the code block only on the first file. Unfortunately, awk will read the second file anyway - as designed juts to find no match...</p> <p><code>printf(\"%s  \",$0)</code> will print the content of the line from the first file and add 2 spaces without the new line.</p> <p><code>while(getline line &lt; ARGV[2])</code> loop will be executed on each line for the first file. It will read one line from the second file per loop and store it in line variable.</p> <p><code>split(line,data)</code> this time we need to split it manually in data array variable.</p> <p><code>if(data[1]==$1) printf( data[2])</code> we print what we find in second column only if we match the values for the first.</p> <p><code>close(ARGV[2])</code> we need to close the second file to be able to start reading from the beginning next time.</p> <p><code>print \"\"</code>  regardless if we found match or not, we print the new line character.</p> <p>Output <pre><code>$ ./script1.awk 1.dat 2.dat\nDaniel    10\nAnders     7\nSven      56  Sunday\nAli       17\nPeter      6  Monday\n</code></pre></p> <p>Warning</p> <p>Note that only names from the first file were processed i.e. the data for David will not appear in the output.</p>"},{"location":"Case_studies/multiple_files_II/#second-round","title":"Second round","text":"<p>The above solution works but for large files this is rather inefficient, let's improve a bit the code.</p> <p>script2.awk</p> <pre><code>#!/usr/bin/awk -f\n\nFNR != NR {exit 0}\n\n{\n  printf(\"%s  \",$0)\n  while(getline line &lt; ARGV[2]){\n    split(line,data)\n    if(data[1]==$1) { printf(\"%s  \",data[2]); break }\n  }\n  close(ARGV[2])\n  print \"\"\n}\n</code></pre> <p>Only two changes: </p> <p><code>FNR != NR {exit 0}</code> serves the same purpose but this time matches when we start reading the next file and exits the program preventing awk from reading the second file.</p> <p><code>if(data[1]==$1) { printf( data[2]); break }</code> we added the brake statement, when we have found the first match from the second file, the program will exit the while loop i.e. will not read the remaining of the second file. </p> <p>Warning</p> <p>This was made under the assumption that you have only one unique entry for each name in the second file. Several year ago, during the exercises, while gradually building the code, the first version of this solution was able to find multiple entries in the second file (real life problem) which exposed a problem with the data that was not identified before. </p> <p>On small files speed up will not be noticeable, but it will be significant on large files. Note: this problem is rather general, not awk specific.</p>"},{"location":"Case_studies/multiple_files_II/#third-round","title":"Third round","text":"<p>Now, what happens if we have more files? One reference and multiple matching files? Let's make an improved version which will demonstrate some other interesting and handy awk features.</p> <p>script3.awk</p> <pre><code>#!/usr/bin/awk -f\n\nBEGIN{argc=ARGC; ARGC=2}\n\n{\n  printf(\"%s  \",$0)\n  for(i=2; i &lt;= argc-1; i++){\n    while(getline line &lt; ARGV[i]){\n      split(line,data)\n      if(data[1]==$1) { printf(\"%s  \",data[2]); break }\n    }\n    close(ARGV[i])\n  }\n  print \"\"\n}\n</code></pre> <p>What awk is doing is storing the name of the program + all the filenames passed as arguments in <code>ARGV</code> array in which element 0 is the program's name itself then all the filenames in order. <code>ARGC</code> will contain the number of elements, so awk will loop reading through the files. The trick is if we change the <code>ARGC</code> to 2, then awk will loop only over the first file and stop. This make it easy to use the rest of the filenames as input parameters - they can be anything you want - they do not need to be real file names.   </p> <p><code>BEGIN{argc=ARGC; ARGC=2}</code> This is the best way to force reading only the first file. Since we need to know how many they were originally, we keep it in a argc before changing <code>ARGC=2</code>. </p> <p><code>for(i=2; i &lt;= argc-1; i++)</code> we manually loop over the remaining files with <code>ARGV[i]</code>.</p> <p>Output <pre><code>$ ./script3.awk 1.dat 2.dat 2.dat 2.dat\nDaniel    10\nAnders     7\nSven      56  Sunday  Sunday  Sunday\nAli       17\nPeter      6  Monday  Monday  Monday\n</code></pre></p>"},{"location":"Exercises/01.Simple_arithmetic/","title":"01.Simple arithmetic *","text":""},{"location":"Exercises/01.Simple_arithmetic/#task-1","title":"Task 1","text":"<p>List the files in your home folder: <code>ls -l</code> The output should look something like this:</p> <pre><code>$ ls -l \ntotal 88608\ndrwxr-sr-x 6 user uu_mkem 2048     Dec 17 2007 acml4.0.1 \ndrwxr-sr-x 7 user uu_mkem 10240    Jan 21 2014 bin \ndrwxr-sr-x 2 user uu_mkem 2048     Mar 19 2010 bin64 \ndrwxr-sr-x 9 user uu_mkem 2048     Oct 18 2007 castep \ndrwxrwsr-x 2 user uu_mkem 2048     Jan 18 2013 ddt \ndrwxr-sr-x 2 user uu_mkem 2048     Dec 20 2006 _del \ndrwxr-sr-x 2 user uu_mkem 2048     Mar 26 2013 Desktop \n-rw-r--r-- 1 user uu_mkem 319954   Aug 15 2012 dftbp_1.2_src.tar.gz \ndrwxr-sr-x 2 user uu_mkem 2048     Mar 26 2013 Documents \ndrwxr-sr-x 2 user uu_mkem 2048     Mar 26 2013 Downloads \n-rw-r----- 1 user uu_mkem 20730896 Aug 8  2011 espresso-4.3.2-examples.tar.gz \n-rw-r----- 1 user uu_mkem 15116712 Aug 8  2011 espresso-4.3.2.tar.gz \n-rw-r----- 1 user uu_mkem 8101479  Jul 5  2011 etc.tgz\n</code></pre> <ol> <li>Write a one-line awk script to calculate the occupied space (5<sup>th</sup> column) in this folder. ( * )  </li> <li>Modify the script to count only the files in your home directory (hopefully you have some in your home folder) (hint: directories begin with \u201dd\u201d in the access field) ( * )</li> </ol>"},{"location":"Exercises/01.Simple_arithmetic/#task-2","title":"Task 2","text":"<p>The table below contains coordinates of atomic positions in Angstrom. Copy/paste the text in a file <code>coord.dat</code> on your computer.</p> <p>coord.dat</p> <pre><code>0.8997508593245822  0.2048785172349154  0.0717195259190714\n0.9490486395984686  0.9146565172390164  0.7689034265935394\n0.8752470144992109  0.8523867265774260  0.8626469653565847\n0.8640857283636117  0.9207732244914749  0.1291593060111536\n0.9392794897450530  0.7856183128719006  0.1151153726165936\n0.3493798807138794  0.1485424348042750  0.4300844752418486\n0.3877406330134096  0.1931874740146635  0.5746869696493597\n0.4376693404820234  0.9063674424632875  0.2740320628086707\n</code></pre> <ol> <li>Write a one-line awk script to add 1 to each number in the second column. ( * )  </li> <li>Modify the script to multiply all numbers by factor of 1.8897 . ( * )</li> </ol> Possible solutions <p>Task 1. <pre><code>1) ls -l | awk '       {s=s+$5} END {print s}'\n2) ls \u2013l | awk ' !/^d/ {s=s+$5} END {print s}'\n</code></pre></p> <p>Task 2. <pre><code>1) awk '{print $1,$2+1.,$3}' coord.dat\n   or \n   awk '{$2=$2+1; print}' coord.dat\n2) awk 'BEGIN{a2b=1.8897261}  {print $1*a2b,$2*a2b,$3*a2b}' coord.dat\n</code></pre></p>"},{"location":"Exercises/02.Data_extraction/","title":"02.Data extraction *","text":"<p>Here is a file containing some data</p> <p>data.dat</p> <pre><code>23 34 567 3 4556 345 22 45 6\n34 5 677 787 234 124 5 5 47\n1 34 98 45 24 333 345 121 17\n100 345 48 65 90 345 665 12\n55 73 34 33 23 25 234 17 19\n</code></pre> <p>1) Write a script to print the largest number from each line.</p> <p>output: <pre><code>4556\n787\n345\n665\n234\n</code></pre></p> Possible solution <pre><code>awk '{max=$1; for (i=2;i&lt;=NF;i++) { if ($i&gt;max) max=$i } ; print max}' data.dat\n</code></pre> <p>2) ... to print the sum of all numbers on each line.</p> <p>output: <pre><code>5601\n1918\n1018\n1670\n513\n</code></pre></p> Possible solution <pre><code>awk '{sum=0; for (i=1;i&lt;=NF;i++) { sum=sum+$i } ; print sum}' data.dat\n</code></pre> <p>3) ... to print new data in which all numbers smaller than 55 are replaced with 0 (zero).</p> <p>output: <pre><code>0 0 567 0 4556 345 0 0 0 \n0 0 677 787 234 124 0 0 0 \n0 0 98 0 0 333 345 121 0 \n100 345 0 65 90 345 665 0 \n55 73 0 0 0 0 234 0 0\n</code></pre></p> Possible solution inspired by Jessica De Loma <pre><code>awk '{ for(i=1;i&lt;=NF;i++){if($i &lt; 55) $i=0 } print $0 }' data.dat\n</code></pre>"},{"location":"Exercises/03.Data_extraction/","title":"03.Data extraction **","text":"<p>The output from a molecular dynamics (MD) program looks like the one bellow. Although it is easy to read, it is rather inconvenient to analyse the data. </p> <p>md.out</p> <pre><code>MD step:           0\nPressure:                           0.332328E-03 au     0.977743E+10 Pa\nPotential Energy:               -1039.8363386148 H       -28295.3864 eV\nMD Kinetic Energy:                  1.0930263164 H           29.7428 eV\nTotal MD Energy:                -1038.7433122983 H       -28265.6437 eV\nMD Temperature:                     0.0009500446 au         300.0000 K\nMD step:          10\nPressure:                           0.335165E-03 au     0.986088E+10 Pa\nPotential Energy:               -1039.9197980557 H       -28297.6575 eV\nMD Kinetic Energy:                  1.1714204733 H           31.8760 eV\nTotal MD Energy:                -1038.7483775824 H       -28265.7815 eV\nMD Temperature:                     0.0010181838 au         321.5166 K\nMD step:          20\nPressure:                           0.348121E-03 au     0.102421E+11 Pa\nPotential Energy:               -1039.9631365733 H       -28298.8368 eV\nMD Kinetic Energy:                  1.2021764632 H           32.7129 eV\nTotal MD Energy:                -1038.7609601101 H       -28266.1239 eV\nMD Temperature:                     0.0010449165 au         329.9581 K\n</code></pre> <p>a) Write an Awk script to collect the \"MD temperature\" in one column vs. the \"MD step\" i.e. which is much nicer format to plot, analyze or read by other programs.</p> <pre><code>0  300.0000\n10 321.5166\n20 329.9581\n</code></pre> <p>b) Can you tabulate all values against the \"MD step\"</p> <p>Output: <pre><code>0  0.977743E+10 -28295.3864 29.7428 -28265.6437 300.0000\n10 0.986088E+10 -28297.6575 31.8760 -28265.7815 321.5166\n20 0.102421E+11 -28298.8368 32.7129 -28266.1239 329.9581\n</code></pre></p> Possible solutions: <p>a)  <pre><code>$ awk '/MD step/{step= $3} /MD Temperature:/ {print step, $5 }' md.out\n</code></pre></p> <p>b) here is an easy solution: <pre><code>$ awk '/MD step/{step= $3; getline; press= $4; getline; pe= $5; getline; ke= $6; getline; te= $6; getline; t=$5; print step,press,pe,ke,te,t} ' md.out\n</code></pre> here is an alternative solution using \"multi-line records\". Each \"MD step\" ends with space,\"K\", then new line. Note that there is no other line ending the same way, otherwise this will not work. So, we define \"Record separator\" RS=\" K\\n\" then each record will have 33 fields (print NF to see if it is correct). <pre><code>$ awk 'BEGIN {RS=\" K\\n\"} {print $3, $7, $13, $20, $27, $33}' md.out\n</code></pre></p> gnuplot tips <p>Here is an example if you want to quickly visualize the output with <code>gnuplot</code>. Note the additional escape character<code>\\</code> in front of <code>\"</code> and \"\\n\". $ gnuplot<pre><code>plot \"&lt; awk 'BEGIN {RS=\\\" K\\\\n\\\"} {print $3, $7, $13, $20, $27, $33}' md.out \" u 1:6 w lp lt 7\n</code></pre> </p>"},{"location":"Exercises/04.Data_manipulation/","title":"04.Data manipulation **","text":"<p>You have 2 files containing results from two similar experiments. You want to calculate the difference between the numbers in the second columns. </p> 1.dat <pre><code>0.0   0.00\n0.1   1.23\n0.2   1.34\n0.3   1.67\n0.4   2.34\n0.5   3.17\n</code></pre> 2.dat <pre><code>0.0   0.00\n0.1   1.25\n0.2   1.24\n0.3   1.61\n0.4   2.44\n0.5   3.27\n</code></pre> <p>Try to use awk and any other tools to produce a file that has the results from both files and the difference between the second column in last column in the new file i.e.</p> <pre><code>0.0  0.00  0.00  0\n0.1  1.23  1.25  -0.02\n0.2  1.34  1.24  0.1\n0.3  1.67  1.61  0.06\n0.4  2.34  2.44  -0.1\n0.5  3.17  3.27  -0.1\n</code></pre> Possible solutions: <pre><code>$ paste 1.dat 2.dat | awk '{print $1,$2,$4,$2-$4}'\n</code></pre> <p>Note: This will not work under OS X <pre><code>$ awk 'ARGIND==1 {x1=$1;y1=$2; getline &lt; ARGV[2]; printf(\"%g  %g  %g  %g  %g\\n\",x1,y1,$1,$2,y1-$2);}' 1.dat 2.dat\n</code></pre></p> <p>Note: This might work under OS X <pre><code>$ awk 'FILENAME==\"1.dat\" {x1=$1;y1=$2; getline &lt; ARGV[2]; printf(\"%g  %g  %g  %g  %g\\n\",x1,y1,$1,$2,y1-$2);}' 1.dat 2.dat\n</code></pre></p> <pre><code>$ awk 'BEGIN{ while (getline &lt; ARGV[1]) {x1=$1;y1=$2; getline &lt; ARGV[2]; printf(\"%g  %g  %g  %g  %g\\n\",x1,y1,$1,$2,y1-$2);}}' 1.dat 2.dat\n</code></pre> <pre><code># contribution by Johan Zvrskovec 2020.08.28\nawk 'NR==FNR{n1[$1]=$2;} NR!=FNR{print ($1,n1[$1],$2,$2-n1[$1])}' 1.dat 2.dat\n</code></pre> gnuplot tips <p>Here is an example that visualizes the newly tabulated data. $ gnuplot<pre><code>plot \"&lt; paste 1.dat 2.dat | awk '{print $1,$2,$4,$2-$4}' \" u 1:2 w l, \"\" u 1:3 w l, \"\" u 1:2:4  w labels\n</code></pre> </p> <p>Or even simpler, since gnuplot can make some calculations on the fly. $ gnuplot<pre><code>plot \"&lt; paste 1.dat 2.dat \" u 1:2 w l, \"\" u 1:4 w l, \"\" u 1:2:($2-4)  w labels\n</code></pre></p>"},{"location":"Exercises/04.Data_manipulation/#trimming-data","title":"Trimming data","text":"<p>Let's have some data file that that contains 60 lines with one number per line. We can think that this is a result of some measurement at every one minute. You can create a file with name 'rnd.dat' containing 60 lines with randomly generated numbers between 0 and 1 with the following awk script (redirect the output to a file).</p> <pre><code>$ awk 'BEGIN{ for(i=1;i&lt;=60;i++) print rand() }'\n\n0.924046\n0.593909\n0.306394\n...\n0.160044\n0.438791\n0.98955\n</code></pre> <p>Note that it is not coincidence that you have generated the same numbers as on this page. It becomes easier to check against the results in the suggested solutions. To randomize the random seed you need to add <code>srand()</code> in the script like this: <code>awk 'BEGIN{ srand(); for(i=1;i&lt;=60;i++) print rand() }'</code></p> <p>Can you print/extract the values for every 5<sup>th</sup> minute/line?</p> Possible solution <pre><code>awk 'NR%5 == 0 {print $0}' rnd.dat\n\n0.740133\n0.100887\n0.426298\n0.119752\n0.399686\n0.956569\n0.984306\n0.173531\n0.84918\n0.433507\n0.884823\n0.98955\n</code></pre> <p>Can you print the average for each 5 rows of the data?</p> Possible solution <pre><code>awk '{s=s+$1} NR%5 == 0 {print $0/5;s=0}' rnd.dat\n\n0.148027\n0.0201774\n0.0852596\n0.0239504\n0.0799372\n0.191314\n0.196861\n0.0347062\n0.169836\n0.0867014\n0.176965\n0.19791\n</code></pre>"},{"location":"Exercises/05.Easy_tricks/","title":"05.Easy tricks *","text":"<p>This exercise will illustrate another convenient feature of Awk - generating data from simple functions, simple list or even some more advanced data sets which does not require files to read/analyse. If you use only the BEGIN block, awk will not try to read any file. One can simply write, which will simply print to the terminal: <pre><code>$ awk 'BEGIN{print \"Hello, world!\"}'\nHello, world!\n</code></pre></p>"},{"location":"Exercises/05.Easy_tricks/#task-1","title":"Task 1.","text":""},{"location":"Exercises/05.Easy_tricks/#a-print-numbers-from-1-to-7-ie-produce-such-output","title":"a) print numbers from 1 to 7 i.e. produce such output","text":"<pre><code>1\n2\n3\n4\n5\n6\n7\n</code></pre> Solution... <pre><code>$ awk 'BEGIN{ for(i=1;i&lt;=7;i=i+1) print i }'\n</code></pre>"},{"location":"Exercises/05.Easy_tricks/#b-print-the-same-numbers-on-a-single-line-ie","title":"b) print the same numbers on a single line i.e.","text":"<pre><code>1 2 3 4 5 6 7\n</code></pre> Solution... <p>Note that here we use printf which does not print the new line. <pre><code>$ awk 'BEGIN{ for(i=1; i&lt;=7; i=i+1) printf i\" \"}'\n</code></pre></p>"},{"location":"Exercises/05.Easy_tricks/#c-print-the-numbers-from-1-to-7-in-reverse-order","title":"c) print the numbers from 1 to 7 in reverse order","text":""},{"location":"Exercises/05.Easy_tricks/#d-print-every-other-number-from-1-to-7-ie","title":"d) print every other number from 1 to 7 i.e.","text":"<pre><code>1\n3\n5\n7\n</code></pre>"},{"location":"Exercises/05.Easy_tricks/#e-print-the-numbers-from-1-to-2-with-increments-of-01-ie","title":"e) print the numbers from 1 to 2 with increments of 0.1 i.e","text":"<pre><code>1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2\n</code></pre> Solution... <p><pre><code>$ awk 'BEGIN{ for(i=1; i&lt;=2.01; i=i+0.1) print i}'\n</code></pre> Yes, Awk allow for fractional increments... Note the upper limit <code>2.01</code>!</p>"},{"location":"Exercises/05.Easy_tricks/#f-can-you-add-on-each-line-the-square-of-the-number-exp-sin-the-argument-is-in-radians-dont-worry","title":"f) can you add on each line the square of the number, exp(), sin() /the argument is in radians, don't worry/","text":"<pre><code>1 1 2.71828 0.841471\n1.1 1.21 3.00417 0.891207\n1.2 1.44 3.32012 0.932039\n1.3 1.69 3.6693 0.963558\n1.4 1.96 4.0552 0.98545\n1.5 2.25 4.48169 0.997495\n1.6 2.56 4.95303 0.999574\n1.7 2.89 5.47395 0.991665\n1.8 3.24 6.04965 0.973848\n1.9 3.61 6.68589 0.9463\n2 4 7.38906 0.909297\n</code></pre> Solution... <pre><code>$ awk 'BEGIN{ for(i=1.; i&lt;=2.01; i=i+0.1) print i,i**2,exp(i),sin(i)}'\n</code></pre>"},{"location":"Exercises/05.Easy_tricks/#g-make-awk-script-that-prints-such-output-ie-2-students-in-each-group","title":"g) make awk script that prints such output i.e. 2 students in each group. (**)","text":"<pre><code>Group1 =&gt; Student1, Student2\nGroup2 =&gt; Student3, Student4\nGroup3 =&gt; Student5, Student6\nGroup4 =&gt; Student7, Student8\nGroup5 =&gt; Student9, Student10\nGroup6 =&gt; Student11, Student12\nGroup7 =&gt; Student13, Student14\n</code></pre> Solution... <pre><code>$ awk 'BEGIN{ for(i=1;i&lt;=7;i++) print \"Group\"i\" =&gt; Student\"(i-1)*2+1\", Student\"(i-1)*2+2 }'\n</code></pre> <p>solution suggested by Ageo Meier de Andrade <pre><code>$ awk 'BEGIN{f=1;for (i=1; i&lt;=7; i++){print \"Group\"i\" =&gt; Student\"f++\",Student\"f++}}'\n</code></pre></p>"},{"location":"Exercises/06.Bioinformaticians_corner/","title":"06.Bioinformaticians corner *","text":"<p>Right, at this point I always feel that I am failing to address the bioinformatician community.  The examples until this point are absolutely generic, but I am convinced that the best way to explain or illustrate something is explaining it in terms of your own field of interest. </p> <p>Here I found an excellent tutorial, essentially doing the same things or slightly different but on \"transcriptome\" /whatever that means/ data file. If you are an bioinformatician, I strongly recommend following the tutorial, which might be more beneficial for you rather than jumping in less common situations that I am trying to present here.</p> <p>Just a teaser for the tutorial as an introduction to the material on the page.</p> <p>AWK GTF! How to Analyze a Transcriptome Like a Pro - Part 1</p> <p>It is really nice exercise for the rest as well /the ones that do not know what is transcriptome/.  If you want to experience, how the bioinformatician feels doing the rest of the exercises on this page /i.e. unaddressed/ - go for it! </p>"},{"location":"Exercises/Advanced_data_analysis/","title":"Advanced data analysis ****","text":"<p>You are given a file with numbers on each row - 5 in this case. </p> <p>data1</p> <pre><code>1 2 3 4 5\n1 3 5 7 9\n1 2 4 5 0\n2 6 7 8 9\n</code></pre> <p>Then you are given 5 numbers (let's say \"1, 3, 5, 6 and 7\") and you want to find how many of these numbers are matching a number on each line - think like you are about to check your lottery tickets ;-)</p> <p>The solution bellow is using an \"associative arrays\" trick to make it easier to loop over the reference numbers.</p> Possible solution <p>Not very elegant but illustrates nicely a convenient use of associated arrays as list - if ($i in n) : <pre><code>awk 'BEGIN{n[1]=n[3]=n[5]=n[6]=n[7]=1} {count=0; for (i=1;i&lt;=NF;i++){if ($i in n) count++} print count} ' data1\n</code></pre></p> <p>Can you improve the script so it could pick up the numbers from the first line, i.e. the winning numbers are on the first line?</p> <p>data2</p> <pre><code>1 3 5 6 7\n1 2 3 4 5\n1 3 5 7 9\n1 2 4 5 0\n2 6 7 8 9\n</code></pre> Possible solution <pre><code>awk 'NR==1 {for (i=1;i&lt;=NF;i++) n[$i]=1}    NR&gt;1 {count=0; for (i=1;i&lt;=NF;i++){if ($i in n) count++} print count} ' data2\n</code></pre> <p>a bit more readable in a script <pre><code>#!/bin/awk -f\nNR==1 {for (i=1; i&lt;=NF; i++) n[$i]=1}    \nNR&gt;1   {count=0; for (i=1; i&lt;=NF; i++) {if ($i in n) count++} print count}\n</code></pre></p>"},{"location":"Exercises/Carpool/","title":"Carpool ***","text":"<p>Some friends have organized carpool them-self. At the beginning of each accounting period, they write down the numbers of the car's odometer. Every time somebody drives the car, she/he writes down the numbers when the car is returned and hers/his name.</p> <p>carpool.dat</p> <pre><code>17000  start\n17100  Daniel\n17220  Sara\n17310  David\n17410  Daniel\n17550  Sara\n17800  David\n</code></pre> <p>Try to write an awk script that calculates what distance has everyone traveled with the car. Hints:  Differences between numbers in two consecutive lines might be a good start.  Associative arrays might come handy.</p> <p>Answer for the data above: <pre><code>           Sara: 260\n         Daniel: 200\n          David: 340\n</code></pre></p> <p>The problem is rather easy to solve with awk (it is my humble opinion). Can it be solved easier with some of your favorite tools? (I am interested of alternative approaches)</p> Possible solution: <pre><code>awk '{dist[$2]=dist[$2]+($1-prev); prev=$1} END{ for (i in dist) if (i !=\"start\") printf(\"%15s: %g\\n\",i,dist[i]) }' carpool.dat\n</code></pre> <p>solution in Python (credits to Mihai Croicu)</p> <pre><code>import pandas as pd\nframe = pd.read_csv('odometer',sep='  ',header=None)                                                                                         \nframe['driven']=frame[0]-frame[0].shift(1)                                                                                                                          \nframe.groupby([1])['driven'].sum()\n</code></pre>"},{"location":"Exercises/Difficult_data/","title":"Difficult to extract data ****","text":"<p>Let's assume that we get such an output from a Python program. <code>[2, 261, 262]</code> is a list variable that is easy to print but kind off difficult to deal with in this form... <pre><code>1 [2, 261, 262] 4.22143226361\n2 [96, 447, 448] 3.9916056595\n3 [103, 461, 462] 2.94525993079\n</code></pre> </p> <p>Use the data above and try to produce output like this (color images are for guidance) <pre><code>O002.H261  O002.H262\nO096.H447  O096.H448\nO103.H461  O103.H462\n</code></pre> </p> Possible solutions: <p><pre><code>awk -F '[][,]' '{printf(\"O%03d.H%03d  O%03d.H%03d\\n\",$2,$3,$2,$4)}' data\n</code></pre> or  just by removing the problematic characters... <pre><code>awk '{gsub(\",\",\" \",$0); gsub(\"\\\\[\",\" \",$0); gsub(\"\\\\]\",\" \",$0); printf(\"O%03d.H%03d  O%03d.H%03d\\n\",$2,$3,$2,$4)}' data\n# or with single gsub command\nawk '{gsub(\",\\\\|\\\\]\\\\|\\\\[\",\" \",$0); printf(\"O%03d.H%03d  O%03d.H%03d\\n\",$2,$3,$2,$4)}' data\n</code></pre></p> <p>Credits to Jonas S\u00f6derberg for the solution bellow: <pre><code>awk '{red = substr($2,2,length($2)-2); green = substr($3,1,length($3)-1); blue = substr($4,1,length($4)-1); printf( \"O%03d.H%03d  O%03d.H%03d\\n\", red, green, red, blue)} data\n</code></pre></p>"},{"location":"Exercises/Exercises/","title":"Exercises - warming up","text":""},{"location":"Exercises/Exercises/#task-1-make-an-awk-script-to-calculate","title":"Task 1. Make an awk script to calculate:","text":"<p>a) sum of the values b) the average i.e arithmetic mean value (can you write the script so it is independent from the number of rows) c) find the maximum value d) calculate the difference between numbers in the first column, i.e 2<sup>nd</sup>-1<sup>st</sup>, 3<sup>rd</sup>-2<sup>nd</sup> etc.</p> <p>Copy/paste the numbers in a file for convenience.</p> <p>num.dat</p> <pre><code>2\n4\n8\n9\n7\n3\n</code></pre> Possible solutions <p>1.a) <pre><code>$ awk '{sum= sum+$1} END{print sum}' num.dat\n33\n</code></pre></p> <p>1.b) <pre><code>$ awk '{sum= sum+$1} END{print sum/NR}' num.dat\n5.5\n</code></pre></p> <p>1.c) NR==1 {max=$1} makes sure that you have reasonable initial value. What could go wrong if you skip it? <pre><code>$ awk 'NR==1 {max=$1}  {if (max &lt; $1) max=$1}  END{print max}' num.dat\n9\n</code></pre></p> <p>1.d) Possible solution (note that the first number is the value of the first column): <pre><code>$ awk '{print $1-prev;prev=$1}' num.dat\n2\n2\n4\n1\n-2\n-4\n</code></pre></p> <p>Can you improve the script to avoid the problem with the first line?</p>"},{"location":"Exercises/Exercises/#task-2-providing-you-have-the-following-data","title":"Task 2. Providing you have the following data","text":"<p>10.dat</p> <pre><code>67\n4\n33\n53\n21\n99\n88\n69\n79\n8\n</code></pre> <p>Can you write a script to print the cumulative sum i.e. on each row, next to the original value, you print the sum of all above values?</p> <p>Output: <pre><code>67 67\n4 71\n33 104\n53 157\n21 178\n99 277\n88 365\n69 434\n79 513\n8 521\n</code></pre></p> Possible solution <pre><code>$ awk '{sum=sum+$1; print $1,sum}' 10.dat\n</code></pre>"},{"location":"Exercises/Linear_regression/","title":"Simple Linear Regression Wikipedia","text":"<p>The purpose of this exercise is not to learn about \"Linear regression\" but to exercise some simple awk operations on simple data sets that will be equivalent to performing of a simple linear regression calculation. You might be surprised how easy it could be done...</p> <p>The file \"regression.dat\" contains 2 rows of numbers, for convenience named as \\(x_i\\) and \\(y_i\\), where \\(i\\) is the row (line) number. I will try to follow the nomenclature described on the wikipage.</p> <p>Suppose there are \\(n\\) data points \\({(x_i, y_i), i = 1, ..., n}\\). The function that describes \\(x\\) and \\(y\\) is:</p> <p>\\(y_i = \\alpha + \\beta x_i + \\varepsilon_i\\).</p> <p>The goal is to find the equation of the straight line</p> <p>\\(y=\\alpha +\\beta x\\),</p> <p>which would provide a \"best\" fit for the data points. Let, at the begging, assume \\(\\alpha=0\\)  which will result to a \"best\" fit of a straight line which passes through the origin \\((0,0)\\). There is a simple expression on how to obtain the \\(\\beta\\) parameter</p> <p>\\({\\displaystyle {\\hat {\\beta }}={\\frac {\\sum _{i=1}^{n}x_{i}y_{i}}{\\sum _{i=1}^{n}x_{i}^{2}}}={\\frac {\\overline {xy}}{\\overline {x^{2}}}}}\\)</p> <ol> <li>Write an awk script (or one line) that will calculate the beta from the equation above. You will be sutprised how easy it could be calculated...    The correct answer for \\(\\displaystyle {\\hat {\\beta}}\\) is 0.928142</li> <li>Write two scripts, one for the average values \\(\\overline {x}, \\overline {y}\\) and then use the numbers in the second (directly in the code), to obtain the results in the case when \\(\\alpha\\) is not constrained to \\(0\\).    Answer: \\(\\displaystyle {\\hat {\\alpha}}=-1314.16; \\displaystyle {\\hat {\\beta}}=1.30453\\).</li> </ol> <p>\\({\\displaystyle {\\hat {\\beta }}={\\frac {\\sum _{i=1}^{n}(x_{i}-\\overline {x})i(y_{i}-\\overline {y})}{\\sum _{i=1}^{n}(x_{i}-\\overline {x})^{2}}}}\\)</p> <p>\\({\\displaystyle {\\hat {\\alpha }}=\\overline {y}-\\displaystyle {\\hat {\\beta }}\\overline {x}}\\)</p> <p>You can use any program you want, to compare the results for the corresponding linear fit. Here is a solution in gnuplot: <pre><code>f(x)=a+b*x\na=0\nfit f(x) \"regression.dat\" via b\n\nFinal set of parameters            Asymptotic Standard Error\n=======================            ==========================\nb               = 0.928142         +/- 0.004221     (0.4548%)\n</code></pre> </p> <pre><code>f(x)=a+b*x\nfit f(x) \"regression.dat\" via a,b\n\nFinal set of parameters            Asymptotic Standard Error\n=======================            ==========================\na               = -1314.16         +/- 27.58        (2.099%)\nb               = 1.30453          +/- 0.00794      (0.6086%)\n</code></pre> <p></p> Possible solutions: <p>1) <pre><code>$ awk '{up= up + $1*$2; down= down + $1*$1;} END{print up/down}' regression.dat\n0.928142\n</code></pre></p> <p>2) <pre><code>$ awk '{x= x + $1; y= y + $2;N=NR} END{print x/N, y/N}' regression.dat\n3455.49  3193.63\n</code></pre> <pre><code>$ awk '{up= up + ($1-3455.49)*($2-3193.63); down= down + ($1-3455.49)**2} END{print \"b=\"up/down\" a=\"3193.63-up/down*3455.49}' regression.dat\nb=1.30453  a=-1314.16\n</code></pre></p> <p>Files</p> <ul> <li>regression.dat</li> </ul>"},{"location":"Exercises/Scrubbing/","title":"Scrubbing a web page **","text":"<p>Warning</p> <p>2021.09.10: The new link is active, if for some reason it is changed or it is not working again, please use the webarchived version bellow.  </p> <p>Unfortunately, the web engine is changing quite often and even the address might not be correct, so it might not be possible to solve the problem as described... To make it work, please use a copy of the page at https://web.archive.org/ https://web.archive.org/web/20190323025902/http://www.kemi.uu.se/about-us/people-angstrom/ </p> <p>Here is another example when awk comes handy. You can get some information on a web page which is more or less well structured and you want to make some statistics from the numbers you can extract. Disclaimer: there are better tools to this but honestly they come with their own overhead...</p> <p>For instance, on the following web address https://kemi.uu.se/angstrom/about-us/staff one can find list of the people employed at the department of chemistry with their positions. Try to make a simple awk script which will count how many people are employed on different positions.</p> <p>To make the example as general as possible let's work with the HTML source code of the web page (do not worry if you are not familiar with HTML). There are many ways to get the web page from the command line but let's consider  two standard tools curl or wget. The commands below will produce identical output with a lot of irrelevant HTML code. <pre><code>$ curl -s https://kemi.uu.se/angstrom/about-us/staff\n$ wget -O - https://kemi.uu.se/angstrom/about-us/staff\n</code></pre></p> <pre><code>!DOCTYPE html&gt;\n&lt;!--[if lt IE 7]&gt;      &lt;html lang=\"sv\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"&gt; &lt;![endif]--&gt;\n&lt;!--[if IE 7]&gt;         &lt;html lang=\"sv\" class=\"no-js lt-ie9 lt-ie8\"&gt; &lt;![endif]--&gt;\n&lt;!--[if IE 8]&gt;         &lt;html lang=\"sv\" class=\"no-js lt-ie9\"&gt; &lt;![endif]--&gt;\n&lt;!--[if gt IE 8]&gt;&lt;!--&gt; &lt;html lang=\"sv\" class=\"no-js\"&gt; &lt;!--&lt;![endif]--&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\"/&gt;\n&lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" /&gt;\n...\n</code></pre> <p>Luckily, all employee titles are on a new line after a line with the following content <code>&lt;span class=\"emp-title\"&gt;</code> . Let's <code>grep</code> for this string and print the line bellow as well with the <code>-A 1</code> option.</p> <pre><code>curl -s https://kemi.uu.se/angstrom/about-us/staff | grep -A 1 \"emp-title\" | head\n&lt;span class=\"emp-title\"&gt;\nvisiting researcher\n--\n&lt;span class=\"emp-title\"&gt;\ndegree project worker\n--\n&lt;span class=\"emp-title\"&gt;\ndoctoral/PhD student\n--\n&lt;span class=\"emp-title\"&gt;\n</code></pre> <p>Can you come with a solution (awk is a good choice) on how to count how many people are employed on each position?</p> Possible solution <p><pre><code>curl -s https://kemi.uu.se/angstrom/about-us/staff | awk ' /emp-title/{getline;title[$0]++} END{for (i in title) print title[i],i}'\n# or from the WebArchive\ncurl -s https://web.archive.org/web/20190323025902/http://www.kemi.uu.se/about-us/people-angstrom/ | awk ' /emp-title/{getline;title[$0]++} END{for (i in title) print title[i],i}'\n</code></pre> <pre><code>1 computer coordinator\n3 Assistant Professor\n1 administrator\n8 assistant undergoing research training\n35 visiting researcher\n3 senior professor\n1 administrative manager\n1 administrative assistant\n1 financial coordinator\n10 professor emeritus\n44 post doctoral\n1 course administrator\n20 degree project worker\n4 master's thesis students\n1 personnel coordinator\n1 systems administrator\n3 economist\n16 professor\n4 research engineer\n38 researcher\n1 instrument maker\n1 project coordinator\n1 hr-generalist\n1 information officer\n1 technician\n79 doctoral/PhD student\n1 personnel administrator\n1 senioruniversitetlektor i oorganisk kemi \n6 guest doctoral student\n4 visiting professor\n23 senior lecturer\n4 associate senior lecturer\n3 research assistant\n20 visiting student\n6 senior research engineer\n2 financial administrator\n3 visiting senior lecturer\n</code></pre></p>"},{"location":"Exercises/String_manipulation/","title":"String manipulation **","text":"<p>This exercise is not specific for awk, but I keep getting questions of this kind, since it is rather common situation. Anyway, if this is what you find interesting, here are few task that will require you to read a bit on how to use the special formatting. Feel free to look in the answers and decode the code.</p> <p>Let us use the following file.</p> <p>strfunc.dat</p> <pre><code>Daniel    10.32\nAnders    7.44\nSven      56.898\nAli      -17.2\nPeter      6\n</code></pre>"},{"location":"Exercises/String_manipulation/#task-1","title":"Task 1","text":"<p>Write an awk script that aligns the data in such way.</p> <pre><code>Daniel     10.320\nAnders      7.440\nSven       56.898\nAli       -17.200\nPeter       6.000\n</code></pre> Possible solution <pre><code>$ awk '{printf(\"%-10s%7.3f\\n\",$1,$2) }' strfunc.dat\n</code></pre>"},{"location":"Exercises/String_manipulation/#task-2","title":"Task 2","text":"<p>There is away to align without taking into account the sign...</p> <pre><code>Daniel    10.320\nAnders    7.440\nSven      56.898\nAli      -17.200\nPeter     6.000\n</code></pre> Possible solution <pre><code>$ awk '{printf(\"%-7s  % .3f\\n\",$1,$2) }' strfunc.dat\n</code></pre>"},{"location":"Exercises/String_manipulation/#task-3","title":"Task 3","text":"<p>Can you modify the script so you get this form which preserves the original data and formatting, but still makes it more readable. <pre><code> Daniel  10.32\n Anders  7.44\n   Sven  56.898\n    Ali  -17.2\n  Peter  6\n</code></pre></p> Possible solution <pre><code>$ awk '{printf(\"%7s  %-7s\\n\",$1,$2) }' strfunc.dat\n</code></pre>"},{"location":"Exercises/String_manipulation/#task-4","title":"Task 4","text":"<p>Can you use the data to compose strings like this? <pre><code>Dan+10.320\nAnd+7.440\nSve+56.898\nAli-17.200\nPet+6.000\n</code></pre></p> Possible solution <pre><code>$ awk '{printf(\"%.3s%+.3f\\n\",$1,$2) }' strfunc.dat\n</code></pre>"},{"location":"Exercises/awk-json/","title":"Awk and json","text":"<p>To begin with, awk is not strong at dealing with json or csv. Having said that, one can still tackle particular or specific problems.</p> <p>Here is a small piece from the stream of json formatted data coming from different sensors.</p> <p>rtl_433a.json</p> <pre><code>{\"time\" : \"2020-12-19 08:50:39\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 183, \"temperature_C\" : 21.800, \"humidity\" : 45, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:50:40\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.900, \"humidity\" : 74, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:50:44\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 184, \"temperature_C\" : 20.200, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:50:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 20, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:50:48\", \"model\" : \"Acurite-Rain\", \"id\" : 34, \"rain_mm\" : 851.000}\n{\"time\" : \"2020-12-19 08:50:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 16, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:50:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 16, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:50:54\", \"model\" : \"Fineoffset-WH2\", \"id\" : 215, \"temperature_C\" : 22.600, \"humidity\" : 45, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:50:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:50:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:50:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:51:00\", \"model\" : \"Acurite-Rain\", \"id\" : 9, \"rain_mm\" : 2032.000}\n{\"time\" : \"2020-12-19 08:51:16\", \"model\" : \"Nexus-TH\", \"id\" : 4, \"channel\" : 2, \"battery\" : 71, \"temperature_C\" : 6.800, \"humidity\" : 89}\n{\"time\" : \"2020-12-19 08:51:16\", \"model\" : \"Eurochron-TH\", \"id\" : 4, \"battery\" : 40, \"temperature_C\" : -16.700, \"humidity\" : 68, \"button\" : 1}\n{\"time\" : \"2020-12-19 08:51:27\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 183, \"temperature_C\" : 21.800, \"humidity\" : 45, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:51:28\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.900, \"humidity\" : 74, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:51:32\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 184, \"temperature_C\" : 20.200, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:51:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:51:48\", \"model\" : \"Acurite-Rain\", \"id\" : 34, \"rain_mm\" : 851.000}\n{\"time\" : \"2020-12-19 08:51:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:51:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:51:54\", \"model\" : \"Fineoffset-WH2\", \"id\" : 216, \"temperature_C\" : 21.600, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:51:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:51:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:52:15\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 183, \"temperature_C\" : 21.800, \"humidity\" : 45, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:52:16\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.800, \"humidity\" : 74, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:52:16\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.800, \"humidity\" : 74, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:52:16\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.800, \"humidity\" : 74, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:52:20\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 184, \"temperature_C\" : 20.100, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:52:23\", \"model\" : \"Nexus-TH\", \"id\" : 4, \"channel\" : 2, \"battery\" : 71, \"temperature_C\" : 6.800, \"humidity\" : 89}\n{\"time\" : \"2020-12-19 08:52:23\", \"model\" : \"Eurochron-TH\", \"id\" : 4, \"battery\" : 30, \"temperature_C\" : -16.700, \"humidity\" : 68, \"button\" : 1}\n{\"time\" : \"2020-12-19 08:52:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 14, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:52:48\", \"model\" : \"Acurite-Rain\", \"id\" : 34, \"rain_mm\" : 851.000}\n{\"time\" : \"2020-12-19 08:52:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:52:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:52:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:52:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:52:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:53:03\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 183, \"temperature_C\" : 21.900, \"humidity\" : 45, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:53:04\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.800, \"humidity\" : 74, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:53:08\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 184, \"temperature_C\" : 20.100, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:53:30\", \"model\" : \"Nexus-TH\", \"id\" : 4, \"channel\" : 2, \"battery\" : 71, \"temperature_C\" : 6.800, \"humidity\" : 89}\n{\"time\" : \"2020-12-19 08:53:30\", \"model\" : \"Eurochron-TH\", \"id\" : 4, \"battery\" : 20, \"temperature_C\" : -16.700, \"humidity\" : 68, \"button\" : 1}\n{\"time\" : \"2020-12-19 08:53:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 13, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:53:48\", \"model\" : \"Acurite-Rain\", \"id\" : 34, \"rain_mm\" : 851.000}\n{\"time\" : \"2020-12-19 08:53:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 12, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:53:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 12, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:53:51\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 183, \"temperature_C\" : 21.900, \"humidity\" : 45, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:53:52\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.800, \"humidity\" : 74, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:53:54\", \"model\" : \"Fineoffset-WH2\", \"id\" : 216, \"temperature_C\" : 21.600, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:53:56\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 184, \"temperature_C\" : 20.100, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:53:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:53:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:53:58\", \"model\" : \"WT450-TH\", \"id\" : 15, \"channel\" : 1, \"battery\" : 100, \"temperature_C\" : 7.125, \"humidity\" : 90, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:54:37\", \"model\" : \"Nexus-TH\", \"id\" : 4, \"channel\" : 2, \"battery\" : 71, \"temperature_C\" : 6.800, \"humidity\" : 89}\n{\"time\" : \"2020-12-19 08:54:37\", \"model\" : \"Eurochron-TH\", \"id\" : 4, \"battery\" : 20, \"temperature_C\" : -16.700, \"humidity\" : 68, \"button\" : 1}\n{\"time\" : \"2020-12-19 08:54:39\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 183, \"temperature_C\" : 21.800, \"humidity\" : 45, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:54:40\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.800, \"humidity\" : 75, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:54:40\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.800, \"humidity\" : 75, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:54:40\", \"model\" : \"Fineoffset-WH5\", \"id\" : 200, \"temperature_C\" : -31.800, \"humidity\" : 75, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:54:44\", \"model\" : \"Fineoffset-TelldusProove\", \"id\" : 184, \"temperature_C\" : 20.100, \"mic\" : \"CRC\"}\n{\"time\" : \"2020-12-19 08:54:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 10, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:54:48\", \"model\" : \"Acurite-Rain\", \"id\" : 34, \"rain_mm\" : 851.000}\n{\"time\" : \"2020-12-19 08:54:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 10, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:54:54\", \"model\" : \"Fineoffset-WH2\", \"id\" : 215, \"temperature_C\" : 22.600, \"humidity\" : 45, \"mic\" : \"CRC\"}\n</code></pre>"},{"location":"Exercises/awk-json/#task-1","title":"Task 1","text":"<p>We are interested only from the temperature values for <code>Fineoffset-TelldusProove</code> with <code>id : 183</code>, so let's tabulate them to something we can easily read and plot. ( * )</p> <pre><code>2020-12-19T08:50:39 21.800\n2020-12-19T08:51:27 21.800\n2020-12-19T08:52:15 21.800\n2020-12-19T08:53:03 21.900\n2020-12-19T08:53:51 21.900\n2020-12-19T08:54:39 21.800\n</code></pre> Possible solution <pre><code>awk '/\"Fineoffset-TelldusProove\", \"id\" : 183/ {gsub(\",|\\\"\",\"\"); print $3\"T\"$4,$13}' rtl_433a.json\n</code></pre>"},{"location":"Exercises/awk-json/#task-2","title":"Task 2","text":"<p>We want to filter the stream and print the original line for sensors with value for battery less than 20%. ( *** )</p> <pre><code>{\"time\" : \"2020-12-19 08:50:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 20, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:50:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 16, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:50:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 16, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:51:16\", \"model\" : \"Eurochron-TH\", \"id\" : 4, \"battery\" : 40, \"temperature_C\" : -16.700, \"humidity\" : 68, \"button\" : 1}\n{\"time\" : \"2020-12-19 08:51:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:51:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:51:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:52:23\", \"model\" : \"Eurochron-TH\", \"id\" : 4, \"battery\" : 30, \"temperature_C\" : -16.700, \"humidity\" : 68, \"button\" : 1}\n{\"time\" : \"2020-12-19 08:52:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 14, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:52:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:52:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 15, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:53:30\", \"model\" : \"Eurochron-TH\", \"id\" : 4, \"battery\" : 20, \"temperature_C\" : -16.700, \"humidity\" : 68, \"button\" : 1}\n{\"time\" : \"2020-12-19 08:53:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 13, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:53:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 12, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n{\"time\" : \"2020-12-19 08:53:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 12, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 2}\n{\"time\" : \"2020-12-19 08:54:37\", \"model\" : \"Eurochron-TH\", \"id\" : 4, \"battery\" : 20, \"temperature_C\" : -16.700, \"humidity\" : 68, \"button\" : 1}\n{\"time\" : \"2020-12-19 08:54:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 10, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 0}\n{\"time\" : \"2020-12-19 08:54:48\", \"model\" : \"WT450-TH\", \"id\" : 13, \"channel\" : 1, \"battery\" : 10, \"temperature_C\" : 23.250, \"humidity\" : 36, \"seq\" : 1}\n</code></pre> Possible solution <pre><code>awk -F '\"battery\" :' '/battery/{if ($2+0&lt;=50) print $0 }' rtl_433a.json\n\n# Same result using proper tool (something like advanced grep or basic awk for json)\n# https://stedolan.github.io/jq/\njq -r -c  'select(.battery &gt; 0 and .battery &lt;= 20)' rtl_433a.json\n</code></pre>"},{"location":"Exercises/gtf-teaser/","title":"How to Analyze a Transcriptome Like a Pro","text":"<p>This is just a teaser for the full tutorial: AWK GTF! How to Analyze a Transcriptome Like a Pro</p> <p>Let's use small file to exercise a bit with the content.</p> <pre><code>$ wget https://raw.github.com/nachocab/nachocab.github.io/master/assets/transcriptome.gtf\n</code></pre> <p>Hint: to get the line unwrapped in the terminal pipe the output to <code>less -S</code></p> <pre><code>$ head transcriptome.gtf | less -S\n##description: evidence-based annotation of the human genome (GRCh37), version 18 (Ensembl 73)\n##provider: GENCODE\n##contact: gencode@sanger.ac.uk\n##format: gtf\n##date: 2013-09-02\nchr1    HAVANA  exon    173753  173862  .   -   .   gene_id \"ENSG00000241860.2\"; transcript_id \"ENST00000466557.2\"; gene_type \"processed_transcript\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.13\"; transcript_type \"lincRNA\"; transcript_status \"KNOWN\"; transcript_name \"RP11-34P13.13-001\"; exon_number 1;  exon_id \"ENSE00001947154.2\";  level 2; tag \"not_best_in_genome_evidence\"; havana_gene \"OTTHUMG00000002480.3\"; havana_transcript \"OTTHUMT00000007037.2\";\nchr1    HAVANA  transcript  1246986 1250550 .   -   .   gene_id \"ENSG00000127054.14\"; transcript_id \"ENST00000478641.1\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"CPSF3L\"; transcript_type \"retained_intron\"; transcript_status \"KNOWN\"; transcript_name \"CPSF3L-006\"; level 2; havana_gene \"OTTHUMG00000003330.11\"; havana_transcript \"OTTHUMT00000009365.1\";\nchr1    HAVANA  CDS 1461841 1461911 .   +   0   gene_id \"ENSG00000197785.9\"; transcript_id \"ENST00000378755.5\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"ATAD3A\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"ATAD3A-003\"; exon_number 13;  exon_id \"ENSE00001664426.1\";  level 2; tag \"basic\"; tag \"CCDS\"; ccdsid \"CCDS31.1\"; havana_gene \"OTTHUMG00000000575.6\"; havana_transcript \"OTTHUMT00000001365.1\";\nchr1    HAVANA  exon    1693391 1693474 .   -   .   gene_id \"ENSG00000008130.11\"; transcript_id \"ENST00000341991.3\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"NADK\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"NADK-002\"; exon_number 3;  exon_id \"ENSE00003487616.1\";  level 2; tag \"basic\"; tag \"CCDS\"; ccdsid \"CCDS30565.1\"; havana_gene \"OTTHUMG00000000942.5\"; havana_transcript \"OTTHUMT00000002768.1\";\nchr1    HAVANA  CDS 1688280 1688321 .   -   0   gene_id \"ENSG00000008130.11\"; transcript_id \"ENST00000497186.1\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"NADK\"; transcript_type \"nonsense_mediated_decay\"; transcript_status \"KNOWN\"; transcript_name \"NADK-008\"; exon_number 2;  exon_id \"ENSE00001856899.1\";  level 2; tag \"mRNA_start_NF\"; tag \"cds_start_NF\"; havana_gene \"OTTHUMG00000000942.5\"; havana_transcript \"OTTHUMT00000002774.3\";\n</code></pre> <p>The transcriptome has 9 columns. The first 8 are separated by tabs and look reasonable (chromosome, annotation source, feature type, start, end, score, strand, and phase), the last one is kind of hairy: it is made up of key-value pairs separated by semicolons, some fields are mandatory and others are optional, and the values are surrounded in double quotes. That\u2019s no way to live a decent life. (text copied from the source)</p> <p>let's get only the lines that have <code>gene</code> in the 3<sup>th</sup> column.</p> <pre><code>$ awk -F '$3 == \"gene\"' transcriptome.gtf | head | less -S\n\nchr1    HAVANA  gene    11869   14412   .   +   .   gene_id \"ENSG00000223972.4\"; transcript_id \"ENSG00000223972.4\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\";\nchr1    HAVANA  gene    14363   29806   .   -   .   gene_id \"ENSG00000227232.4\"; transcript_id \"ENSG00000227232.4\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"WASH7P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"WASH7P\"; level 2; havana_gene \"OTTHUMG00000000958.1\";\nchr1    HAVANA  gene    29554   31109   .   +   .   gene_id \"ENSG00000243485.2\"; transcript_id \"ENSG00000243485.2\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"MIR1302-11\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"MIR1302-11\"; level 2; havana_gene \"OTTHUMG00000000959.2\";\nchr1    HAVANA  gene    34554   36081   .   -   .   gene_id \"ENSG00000237613.2\"; transcript_id \"ENSG00000237613.2\"; gene_type \"lincRNA\"; gene_status \"KNOWN\"; gene_name \"FAM138A\"; transcript_type \"lincRNA\"; transcript_status \"KNOWN\"; transcript_name \"FAM138A\"; level 2; havana_gene \"OTTHUMG00000000960.1\";\nchr1    HAVANA  gene    52473   54936   .   +   .   gene_id \"ENSG00000268020.2\"; transcript_id \"ENSG00000268020.2\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"OR4G4P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"OR4G4P\"; level 2; havana_gene \"OTTHUMG00000185779.1\";\nchr1    HAVANA  gene    62948   63887   .   +   .   gene_id \"ENSG00000240361.1\"; transcript_id \"ENSG00000240361.1\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"OR4G11P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"OR4G11P\"; level 2; havana_gene \"OTTHUMG00000001095.2\";\nchr1    HAVANA  gene    69091   70008   .   +   .   gene_id \"ENSG00000186092.4\"; transcript_id \"ENSG00000186092.4\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F5\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F5\"; level 2; havana_gene \"OTTHUMG00000001094.1\";\nchr1    HAVANA  gene    89295   133566  .   -   .   gene_id \"ENSG00000238009.2\"; transcript_id \"ENSG00000238009.2\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.7\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"RP11-34P13.7\"; level 2; havana_gene \"OTTHUMG00000001096.2\";\nchr1    HAVANA  gene    89551   91105   .   -   .   gene_id \"ENSG00000239945.1\"; transcript_id \"ENSG00000239945.1\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.8\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"RP11-34P13.8\"; level 2; havana_gene \"OTTHUMG00000001097.2\";\nchr1    HAVANA  gene    131025  134836  .   +   .   gene_id \"ENSG00000233750.3\"; transcript_id \"ENSG00000233750.3\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"CICP27\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"CICP27\"; level 1; tag \"pseudo_consens\"; havana_gene \"OTTHUMG00000001257.3\";\n</code></pre> <p>Perhaps filter a bit more and print the content of the 9<sup>th</sup> column in the file.</p> <pre><code>$ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | head | less -S\n\ngene_id \"ENSG00000223972.4\"; transcript_id \"ENSG00000223972.4\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\";\ngene_id \"ENSG00000227232.4\"; transcript_id \"ENSG00000227232.4\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"WASH7P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"WASH7P\"; level 2; havana_gene \"OTTHUMG00000000958.1\";\ngene_id \"ENSG00000243485.2\"; transcript_id \"ENSG00000243485.2\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"MIR1302-11\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"MIR1302-11\"; level 2; havana_gene \"OTTHUMG00000000959.2\";\ngene_id \"ENSG00000237613.2\"; transcript_id \"ENSG00000237613.2\"; gene_type \"lincRNA\"; gene_status \"KNOWN\"; gene_name \"FAM138A\"; transcript_type \"lincRNA\"; transcript_status \"KNOWN\"; transcript_name \"FAM138A\"; level 2; havana_gene \"OTTHUMG00000000960.1\";\ngene_id \"ENSG00000268020.2\"; transcript_id \"ENSG00000268020.2\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"OR4G4P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"OR4G4P\"; level 2; havana_gene \"OTTHUMG00000185779.1\";\ngene_id \"ENSG00000240361.1\"; transcript_id \"ENSG00000240361.1\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"OR4G11P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"OR4G11P\"; level 2; havana_gene \"OTTHUMG00000001095.2\";\ngene_id \"ENSG00000186092.4\"; transcript_id \"ENSG00000186092.4\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F5\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F5\"; level 2; havana_gene \"OTTHUMG00000001094.1\";\ngene_id \"ENSG00000238009.2\"; transcript_id \"ENSG00000238009.2\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.7\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"RP11-34P13.7\"; level 2; havana_gene \"OTTHUMG00000001096.2\";\ngene_id \"ENSG00000239945.1\"; transcript_id \"ENSG00000239945.1\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.8\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"RP11-34P13.8\"; level 2; havana_gene \"OTTHUMG00000001097.2\";\ngene_id \"ENSG00000233750.3\"; transcript_id \"ENSG00000233750.3\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"CICP27\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"CICP27\"; level 1; tag \"pseudo_consens\"; havana_gene \"OTTHUMG00000001257.3\";\n</code></pre> <p>What about if we want just a specific piece from this information? We can <code>|</code> the output from the first awk script in to a second one. Note that we will use different field separator <code>\"; \"</code>.</p> <pre><code>$ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head\n\ngene_type \"pseudogene\"\ngene_type \"pseudogene\"\ngene_type \"lincRNA\"\ngene_type \"lincRNA\"\ngene_type \"pseudogene\"\ngene_type \"pseudogene\"\ngene_type \"protein_coding\"\ngene_type \"lincRNA\"\ngene_type \"lincRNA\"\ngene_type \"pseudogene\"\n</code></pre> <p>At this point I will suggest to follow the original tutorial: AWK GTF! How to Analyze a Transcriptome Like a Pro</p>"},{"location":"Exercises/mail-merge/","title":"Mail merge ****","text":"<p>This exercise is for those of you that feel comfortable experimenting with awk and to improve further. Here is the link to the original task https://opensource.com/article/19/10/advanced-awk</p> <p>To recap... We want to compose text from a template <code>email_template.txt</code> in which we will replace the fields with the data provided on each line in the second file <code>proposals.csv</code>.</p> <p>email_template.txt</p> <pre><code>From: Program committee &lt;pc@event.org&gt;\nTo: {firstname} {lastname} &lt;{email}&gt;\nSubject: Your presentation proposal\n\nDear {firstname},\n\nThank you for your presentation proposal:\n  {title}\n\nWe are pleased to inform you that your proposal has been successful! We\nwill contact you shortly with further information about the event \nschedule.\n\nThank you,\nThe Program Committee\n</code></pre> <p>proposals.csv</p> <pre><code>firstname,lastname,email,title\nHarry,Potter,hpotter@hogwarts.edu,\"Defeating your nemesis in 3 easy steps\"\nJack,Reacher,reacher@covert.mil,\"Hand-to-hand combat for beginners\"\nMickey,Mouse,mmouse@disney.com,\"Surviving public speaking with a squeaky voice\"\nSanta,Claus,sclaus@northpole.org,\"Efficient list-making\"\n</code></pre> <p>If you feel confident, go ahead and try to solve the problem. Or...</p> <ul> <li>Follow the tutorial for the first part till the end of the \"Advanced awk: Mail merge\" section.</li> <li>Try to have the script running and understand the way it works.</li> <li>Then return to this exercise and let's make this really advanced!</li> </ul> <p>The solution presented at the link has too much features hard coded i.e. the names and the numbers of the fields, file names... Let's make this really generic and flexible. Here I would like to run the tool like this.</p> <p>Merge the data (first argument) with the provided template (second argument) for participant 1 (provided after the 2 filenames)</p> <pre><code>$ ./mail-merge.awk proposals.csv email_template.txt 1\n\nFrom: Program committee &lt;pc@event.org&gt;\nTo: Harry Potter &lt;hpotter@hogwarts.edu&gt;\nSubject: Your presentation proposal\n\nDear Harry,\n\nThank you for your presentation proposal:\n  \"Defeating your nemesis in 3 easy steps\"\n\nWe are pleased to inform you that your proposal has been successful! We\nwill contact you shortly with further information about the event\nschedule.\n\nThank you,\nThe Program Committee\n</code></pre> <p>Here is the strategy I would like to propose.</p> <ul> <li>use the trick <code>NR==NFR</code> to load the relevant data from the first file.</li> <li>hard code the third argument i.e. make the script running for the firs data line.</li> <li>while reading the second file run the substitutions with the data collected from the first file.</li> <li>the ultimate goal is to have tool that is independent of the numbers and the names of the fields for substitution and the file names containing the data and the template...</li> </ul> Possible solution <pre><code>#!/usr/bin/awk -f\nBEGIN {\n  FS=\",\"\n  if (ARGC &lt;=3 ) exit # exit if not enough arguments provided\n  #print ARGC,ARGV[3] # for debugging purposes\n  argc= ARGC;  ARGC=3 # do not read after the second file\n}\n\nNR==1 {\n  for (i=1;i&lt;=NF;i++)  {\n    varnames[$i]=i   # list with variables and the corresponding column\n    varcolumns[i]=$i # the opposite...\n  }\n  next\n}\n\nNR==FNR {\n  for(i=1;i&lt;=NF;i++){ \n    data[varcolumns[i]][NR-1]= $i # collect the data\n  } \n}\n\nNR!=FNR {\n  for(i in varnames){ gsub(\"{\"i\"}\",data[i][ARGV[3]])  }\n  print\n}\n</code></pre>"},{"location":"More_awk/Command_params/","title":"Trick to pass parameters on the command line","text":"<p>The common way to pass options and parameters to an Awk script is via the awk option <code>-v varname=value</code> </p> <pre><code>$ script.awk -v variablename1=value2 -v variablename=value2 filename1 filename2\n</code></pre> <p>This is really flexible and has many advantages, but in most cases, you want to use something more common - positional parameters i.e. something like this</p> <pre><code>$ script.awk filename parameter1 parameter2\n</code></pre> <p>Without some precautions, parameter1 and parameter2 will be treated as file names...</p> <p>When Awk starts, the information from the command line will be stored in two internal variables <code>ARGV</code> - array with argument values and <code>ARGC</code> - scalar variable with the count of elements in <code>ARGV</code>. <code>ARGV[0]</code> will contain the name of the script itself (<code>script.awk</code> in this example), <code>ARGV[1]</code> - the name of the first file on the command line etc.</p> <pre><code>ARGV[0]=\"script.awk\"\nARGV[1]=\"filename\"\nARGV[2]=\"parameter1\"\nARGV[3]=\"parameter2\"\n\nARGC=4\n</code></pre> <p>Essentially, awk starts to read files with names taken from the elements of the <code>ARGV</code> array - except for <code>ARGV[0]</code>. Here is the neat trick - if you change the value <code>ARGC=2</code>, awk will \"think\" that there are only 2 elements - program name and first file and run the loop as usual for these 2 elements i.e. reading only <code>filename</code>. Without any surprise, the values in <code>ARGV</code> continue to be available...</p> <p>input.dat</p> <pre><code>1 2\n3 4\n5 6\n7 8\n</code></pre> <p>add_to_columns.awk</p> <pre><code>#!/usr/bin/awk -f\nBEGIN {ARGC=2}\n\n{print $1+ARGV[2],$2+ARGV[3]}\n</code></pre> <pre><code>$ ./add_to_column.awk input.dat 1 2\n2 4\n4 6\n6 8\n8 10\n</code></pre> <p>This \"trick\" could be used in other ways - Rereading the Current File, changing the name of the next file to read as well (if you want) etc.</p>"},{"location":"More_awk/Input_output/","title":"Input/Output to an external program from within awk","text":""},{"location":"More_awk/Input_output/#reading-output-from-external-program","title":"Reading output from external program","text":"<p>Awk has a way to read output from external programs. Here is an example where we will use only the BEGIN block in order to simplify the discussion.</p> <p>read_ext1.awk</p> <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  while (\"lsb_release -a\" | getline){\n    print \"awk:\",$0\n  }\n}\n</code></pre> <pre><code>$ ./read_ext1.awk\nNo LSB modules are available.\nawk: Distributor ID:    Ubuntu\nawk: Description:       Ubuntu 18.04.4 LTS\nawk: Release:   18.04\nawk: Codename:  bionic\n</code></pre> <p>Note</p> <p><code>No LSB modules are available.</code> was sent to <code>/dev/stderr</code> by <code>lsb_release</code> and awk newer got to read it on first place.</p> <p>Warning</p> <p>Kepp in mind that <code>getline</code> will read one line and store it in <code>$0</code> by replacing the content from the common lines read by awk.  To avoid this use <code>getline variablename</code> to store the line in new variable. more...</p> <p>Info</p> <p>Awk can <code>getline</code> directly from another file instead of the one that awk is currently reading - <code>getline &lt; filename</code> more...</p> <p>This second variant will produce the same result, but also illustrates the use of <code>close()</code>. </p> <p>read_ext2.awk</p> <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  cmd=\"lsb_release -a\"\n  while (cmd | getline){\n    print \"awk:\",$0\n  }\n  close(cmd)\n}\n</code></pre> <p>Question</p> <p>What happens if you try to read the output second time without closing?</p> <p>How about if we want to get only the <code>bionic</code> from the Codename (ignore that you can request this by <code>lsb_release -c</code>) This version will print only what we need.</p> <p>read_ext3.awk</p> <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  cmd=\"lsb_release -a\"\n  while (cmd | getline){\n    if($1 == \"Codename:\") print $2\n  }\n  close(cmd)\n}\n</code></pre> <p>Note</p> <p>You need to redirect standard error to get the clean output. <pre><code>$ ./read_ext3.awk 2&gt; /dev/null\nbionic\n</code></pre></p>"},{"location":"More_awk/Input_output/#sending-data-to-external-program-and-reading-the-output","title":"Sending data to external program (and reading the output)","text":"<p>These examples perhaps are not the best use but will illustrate how awk can send data to the standard input of an external program and read the produced output so you can use the data in your script. Awk does not have a function to find the greatest common divisor but python has such function math.gcd and we can use it by sending commands directly to python.</p> <p>gcd1.awk</p> <pre><code>#!/usr/bin/awk -f\nBEGIN{ \n  print \"import math; print(math.gcd(12,88))\" | \"python3\"\n}\n</code></pre> <pre><code>$ ./gcd1.awk\n4\n</code></pre> <p>This will simply send the commands to python and the output will be printed to standard output. We want the result back.</p> <p>gcd2.awk</p> <pre><code>#!/usr/bin/awk -f\nBEGIN{\n  cmd=\"python3\"\n  print \"import math; print(math.gcd(12,88));\" |&amp; cmd\n\n  close(cmd,\"to\")\n\n  cmd |&amp; getline\n  print \"awk:\",$0\n}\n</code></pre> <p>There is a complication, though. Python is an interactive program and expects end of stream in order to preprocess the data or in many situations - to flush the input buffer. The solution to this is to call <code>close(cmd,\"to\")</code> function on line 6, deeply buried in the awk documentation.</p> <p>This last example covers more or less the most complicated situation. Usually one can get away with fewer lines. Note also that we <code>getline</code>-ed only once since we wanted only the first line in the output. This might not be the case and you might need to run <code>while</code> loop to read all lines.</p> <p>Summary of the eight variants of getline, listing which predefined variables are set by each one, and whether the variant is standard or a gawk extension.</p> Variant Effect awk / gawk getline Sets $0, NF, FNR, NR, and RT awk getline var Sets var, FNR, NR, and RT awk getline &lt; file Sets $0, NF, and RT awk getline var &lt; file Sets var and RT awk command | getline Sets $0, NF, and RT awk command | getline var Sets var and RT awk command |&amp; getline Sets $0, NF, and RT gawk command |&amp; getline var Sets var and RT gawk"},{"location":"More_awk/Running_average/","title":"Running/moving/rolling average","text":"<p>Definition on Wikipedia</p> <p>Here is an example from my own experience illustrated with a typical problem. On the figure below I have series of numbers (in this case, the instantaneous temperature from a an MD simulation) with some typical oscillations. By now, you know how to easily calculate an average and select certain region (I would prefer to skip at least 100 steps from the beginning... ). Anyway, there is an easier way to visualize the averages by computing the running average over selected interval. It is a commonly visualized property on stock prices link for example.</p> <p></p> <p>There are countless solutions that you can find on the net. They do the same, but some are more efficient or elegant than others. Some time ago I wrote myself a small code, then improved several times and then I found a \"brilliant\" solution somewhere on the net. Here is the script itself - completely unreadable... The highlighted lines are added for flexibility - to select the column and the range for averaging.</p> <p>run-average.awk</p> <pre><code>#!/usr/bin/awk -f\n{\n  if (!col)  col = 1\n  if (!size) size= 5\n  mod= NR%size; \n  if(NR &lt;= size){count++}\n  else{ sum-= array[mod] };\n  sum+= $(col); array[mod]= $(col);\n  print sum/count\n}\n</code></pre> <p>Here is how the result from the script looks like.</p> <p></p> <p>This plot is generated from the Gnuplot command line  <pre><code>plot [:2000] \"temp.dat\" w l, \\\n             \"&lt; ./run-average.awk size=100 temp.dat\" w l lw 3 lc rgb \"blue\", \\\n             \"&lt; ./run-average.awk size=500 temp.dat\" w l lw 3 lc rgb \"green\"\n</code></pre></p> <p>This easily \"filters out\" the oscillations and showing the averages for the selected size... Even more the not-so-easy to see drift is clearly visible now (green line).</p> <p>Essentially, the program makes an average over selected size range of the previous data at each point. What is particularly smart with this solution is that the average is done by updating the sum from numbers kept in an  array of size <code>size</code>. At every line, the new value is added to the sum and one is subtracted - the one that falls now outside the range for averaging. The position of the element is derived by the reminder computed by the modulo operation <code>mod= NR%size</code> <sup>1</sup>. In principle, the code is only two lines, but the rest is taking care of the initial region where the data is not enough to make an average over the specified <code>size</code>.</p> <p>This is a smart realization of FIFO (first in first out) manipulation of data structure implemented on few lines in this script. Compare this to the code referred in the wikipage.</p> <p>Note</p> <p>I have used to pass different parameters <code>size=100</code> and <code>size=500</code> directly in the gnuplot command instead of a filename (not all programs allow you to make such neat substitutions) but this is one example where awk gets handy - just write you one line script between <code>\"&lt; \"</code>.</p> <p>The value of such programs is that it makes it easy to add or modify small bits, since there is not much of a code anyway. </p> <p>Files</p> <ul> <li>run-average.awk</li> <li>temp.dat</li> </ul> <ol> <li> <p>Gnuawk: Arithmetic Operators \u21a9</p> </li> </ol>"},{"location":"More_awk/User_defined_functions/","title":"User defined functions","text":"<p>Related Awk documentation</p> <p>Usually at this point (looking on how to write own functions) you have a working script (I doubt you want to do this on one line) and you want to wrap your repetitive task in a function call... The User defined function syntax is as follows</p> <pre><code>function name([parameter-list])\n{\n  body-of-function\n\n  [return variable] \n}\n</code></pre> <p>Here is a simple example. It is rather surprising but awk dows not have funtion to return athe absolute value of a number.</p> <pre><code>function abs(x){\n  if (x &gt;= 0) return x\n  else return -x\n}\n</code></pre> <p>or </p> <pre><code>function abs(x)  { return (x &gt;= 0) ? x : -x }\n</code></pre> <p>Here is another example from one of the study cases Gaussian smearing Gaussian funtion. I have used \\(x_0\\) instead of \\(b\\) for the peak center. </p> <pre><code>function gauss(x0,x,c){\n  area=1;\n  if ((x-x0)**2 &lt; 10000) { return  area*exp(-(((x-x0))**2)/(2.*c**2))}\n  else {return 0.0}\n}\n</code></pre> <p>The function returns the value for the Gaussian for a point \\(x\\) away from the center. For large \\((x-x0)^2\\) the <code>exp()</code> was crashing, so I needed to add a condition which makes sure that for these very large numbers the function does not call the <code>exp(...)</code> but returns directly <code>0.0</code> instead. The function is called within a double loop on line 23</p> <pre><code>      data[i]= data[i] + gauss(freq[f],i,FWHM);\n</code></pre> <p>Warning</p> <p>It's entirely fair to say that the awk syntax for local variable definitions is appallingly awful. - Brian Kernighan source</p>"},{"location":"Other/Backreferences/","title":"Backreferences","text":"<p>This is kind of common situation. You know how to do it with sed or perl but now, for whatever reason, you want to do it with awk. Googling around and reading some other sources, can give you the impression that backreference is not implemented in awk...</p> <p>Here is an example how one can \"translate\" sed backreference in awk. </p> <p>sed <pre><code>$ echo \"&gt;seq12/1-100\" | sed -e 's,&gt;seq\\(.*\\)\\/.*,&gt;SEQ-\\1,g'\n&gt;SEQ-12\n</code></pre></p> <p>awk <pre><code>$ echo \"&gt;seq12/1-100\" | awk ' {print gensub( /&gt;seq(.*)\\/.*/ , \"&gt;SEQ-\\\\1\" , \"g\")} '\n&gt;SEQ-12\n</code></pre></p> <p>Note: if you are looking just to rename the entries in a Fasta file, perhaps there is way easier way to do it: Renaming Entries In A Fasta File</p> <p>Here is another example, where I can mention some disadvantages with awk (or perhaps I could not find how to do it properly).</p> <p>filedata</p> <pre><code>/home/ux/user/z156256\n/home/ux/user/z056254\n/home/lx/user/z106253\n/home/ux/user/z150252\n/home/mp/user/z056254\n/home/lx/user/z106253\n</code></pre> <p>sed <pre><code>$ sed -e 's,/home/\\(..\\)/user/\\(z[0-9]\\{6\\}\\),/usr/\\2/\\1,g' filedata\n/usr/z156256/ux\n/usr/z056254/ux\n/usr/z106253/lx\n/usr/z150252/ux\n/usr/z056254/mp\n/usr/z106253/lx\n</code></pre></p> <p>awk <pre><code>$awk '{print gensub(/\\/home\\/(..)\\/user\\/(z[0-9][0-9][0-9][0-9][0-9][0-9])/,\"/usr/\\\\2/\\\\1\",\"g\")}' filedata\n/usr/z156256/ux\n/usr/z056254/ux\n/usr/z106253/lx\n/usr/z150252/ux\n/usr/z056254/mp\n/usr/z106253/lx\n</code></pre></p> <p>... and as a colleague of mine, Matti Hellstr\u00f6m, pointed out - the proper \"awk way\" is:</p> <pre><code>$ awk -F/ '{print \"/usr/\" $5 \"/\" $3}' filedata\n/usr/z156256/ux\n/usr/z056254/ux\n/usr/z106253/lx\n/usr/z150252/ux\n/usr/z056254/mp\n/usr/z106253/lx\n</code></pre>"},{"location":"Other/Binder/","title":"GitHub &amp; Binder","text":"<p>Start a Binder virtual environment for the course with all GitHub files available (click on this link ).  </p> <p>  When the Virtual environment is ready and running, choose \"New/Terminal\" from the drop-down menu on the top-right of the page.</p>"},{"location":"Other/Fixed_size_fields/","title":"Fixed size fields","text":"<p>Your file has fixed length of the data fields...  Here is a good example how to deal with it:</p> <pre><code>$ echo 20140805234656 | awk 'BEGIN { FIELDWIDTHS = \"4 2 2 2 2 2\" } { printf \"%s-%s-%s %s:%s:%s\\n\", $1, $2, $3, $4, $5, $6 }'\n2014-08-05 23:46:56\n</code></pre>"},{"location":"Other/Localization/","title":"Localization problems","text":"<p>Well, here are some annoying problems related to some local settings, for example decimal point/comma. </p> <p>US localization with decimal point <pre><code>$ echo 123.2 |  gawk  '{print; print $1+1}'\n124.2\n</code></pre></p> <p>SE localization with decimal comma  <pre><code>$ echo 123,2 |  gawk  '{print; print $1+1}'\n124\n</code></pre></p> <p>SE localization with decimal comma - FIX <pre><code>$ echo 123,2 | LC_NUMERIC=se_SE.utf-8 gawk --use-lc-numeric '{printf $1+1}'\n124,2\n</code></pre></p>"},{"location":"Other/neat_solution_01/","title":"Neat solution #1","text":"<p>Here is a neat example of using associative arrays again.</p> <p>Let's have a list of e-mails which we collected from different sources and unfortunately might contain duplicate entries. Let's use just some regular text instead of mail addresses... In this example \"aa\" and  \"aaa\" appear 2 times.</p> <p><pre><code>aaa\nbbb\naaa\naa\nccc\naa\n</code></pre> <pre><code>$ awk '!x[$0]++' file.txt\n</code></pre></p> <p>or  <pre><code>$ echo -e \"aaa\\nbbb\\naaa\\naa\\nccc\\naa\" | awk '!x[$0]++'\naaa \nbbb \naa \nccc\n</code></pre> That is really not easy to read/understand - I agree.  Do not worry, it is meant only to demonstrate some features that make awk rather god tool for text parsing and simple data manipulation.</p> <p>Here is what happens. By default, if there is no action defined, awk will execute print command which will print the whole line content. Essentially what we have <code>'!x[$0]++'</code> is a matching criteria. When awk starts, <code>x</code> is empty, so on the first line of the file <code>x[$0]</code> is empty (that is equal to FALSE) then <code>!</code> negates the result and <code>++</code> actually ads 1 to the default 0 for undefined element, so next time when there is a line with the same text, the negated result will be FALSE and the line will not be printed.</p> <p></p>"}]}